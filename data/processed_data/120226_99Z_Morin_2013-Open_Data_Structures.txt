Open Data Structures opel (open paths to enriched learning) Series Editor: Connor Houlihan Open Paths to Enriched Learning (opel) reflects the continued commitment of Athabasca University to removing barriers — including the cost of course materials — that restrict access to university-level study. The opel series offers introductory texts, on a broad array of topics, written especially with undergraduate students in mind. Although the books in the series are designed for course use, they also afford lifelong learners an opportunity to enrich their own knowledge. Like all au Press publications, opel course texts are available for free download at www.aupress.ca, as well as for purchase in both print and digital formats. series titles Open Data Structures: An Introduction Pat Morin PAT MORINOpen Data Structures An Introduction Copyright © 2013 Pat Morin Published by au Press, Athabasca University 1200, 10011-109 Street, Edmonton, ab T5J 3S8 A volume in opel (Open Paths to Enriched Learning) issn 2291-2606 (print) 2291-2614 (digital) Cover and interior design by Marvin Harder, marvinharder.com. Printed and bound in Canada by Marquis Book Printers. Library and Archives Canada Cataloguing in Publication Morin, Pat, 1973 —, author Open data str uctures : an introduction / Pat Morin. (opel (Open paths to enriched learning), issn 2291-2606 ; 1) Includes bibliographical references and index. Issued in print and electronic formats. isbn 978-1-927356-38-8 (pbk.).—isbn 978-1-927356-39-5 (pdf).— isbn 978-1-927356-40-1 (epub) 1. Data structures (Computer science). 2. Computer algorithms. I. Title. II. Series: Open paths to enriched learning ; 1 QA76 . 9.D35M67 2013 005.7 ’3 C 2013-902170-1 We acknowledge the financial support of the Government of Canada through the Canada Book Fund (cbf) for our publishing activities. Assistance provided by the Government of Alberta, Alberta Multimedia Development Fund. This publication is licensed under a Creative Commons license, Attribution-Noncommercial-No Derivative Works 2.5 Canada: see www.creativecommons.org. The text may be reproduced for non-commercial purposes, provided that credit is given to the original author. To obtain permission for uses beyond those outlined in the Creative Commons license, please contact au Press, Athabasca University, at aupress@athabascau.ca. Contents Acknowledgments xi Why This Book? xiii 1 Introduction 1 1.1 The Need for E ﬃciency . . . . . . . . . . . . . . . . . . . . . 2 1.2 Interfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.2.1 The Queue ,Stack , and Deque Interfaces . . . . . . . 5 1.2.2 The List Interface: Linear Sequences . . . . . . . . . 6 1.2.3 The USet Interface: Unordered Sets . . . . . . . . . . 8 1.2.4 The SSet Interface: Sorted Sets . . . . . . . . . . . . 9 1.3 Mathematical Background . . . . . . . . . . . . . . . . . . . 9 1.3.1 Exponentials and Logarithms . . . . . . . . . . . . . 10 1.3.2 Factorials . . . . . . . . . . . . . . . . . . . . . . . . . 11 1.3.3 Asymptotic Notation . . . . . . . . . . . . . . . . . . 12 1.3.4 Randomization and Probability . . . . . . . . . . . . 15 1.4 The Model of Computation . . . . . . . . . . . . . . . . . . . 18 1.5 Correctness, Time Complexity, and Space Complexity . . . 19 1.6 Code Samples . . . . . . . . . . . . . . . . . . . . . . . . . . 22 1.7 List of Data Structures . . . . . . . . . . . . . . . . . . . . . 22 1.8 Discussion and Exercises . . . . . . . . . . . . . . . . . . . . 26 2 Array-Based Lists 29 2.1 ArrayStack : Fast Stack Operations Using an Array . . . . . 30 2.1.1 The Basics . . . . . . . . . . . . . . . . . . . . . . . . 30 2.1.2 Growing and Shrinking . . . . . . . . . . . . . . . . . 33 2.1.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . 35 2.2 FastArrayStack : An Optimized ArrayStack . . . . . . . . . 35 2.3 ArrayQueue : An Array-Based Queue . . . . . . . . . . . . . 36 2.3.1 Summary . . . . . . . . . . . . . . . . . . . . . . . . . 40 2.4 ArrayDeque : Fast Deque Operations Using an Array . . . . 40 2.4.1 Summary . . . . . . . . . . . . . . . . . . . . . . . . . 43 2.5 DualArrayDeque : Building a Deque from Two Stacks . . . . 43 2.5.1 Balancing . . . . . . . . . . . . . . . . . . . . . . . . . 47 2.5.2 Summary . . . . . . . . . . . . . . . . . . . . . . . . . 49 2.6 RootishArrayStack : A Space-E ﬃcient Array Stack . . . . . 49 2.6.1 Analysis of Growing and Shrinking . . . . . . . . . . 54 2.6.2 Space Usage . . . . . . . . . . . . . . . . . . . . . . . 54 2.6.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . 55 2.6.4 Computing Square Roots . . . . . . . . . . . . . . . . 56 2.7 Discussion and Exercises . . . . . . . . . . . . . . . . . . . . 59 3 Linked Lists 63 3.1 SLList : A Singly-Linked List . . . . . . . . . . . . . . . . . 63 3.1.1 Queue Operations . . . . . . . . . . . . . . . . . . . . 65 3.1.2 Summary . . . . . . . . . . . . . . . . . . . . . . . . . 66 3.2 DLList : A Doubly-Linked List . . . . . . . . . . . . . . . . . 67 3.2.1 Adding and Removing . . . . . . . . . . . . . . . . . 69 3.2.2 Summary . . . . . . . . . . . . . . . . . . . . . . . . . 70 3.3 SEList : A Space-E ﬃcient Linked List . . . . . . . . . . . . . 71 3.3.1 Space Requirements . . . . . . . . . . . . . . . . . . 72 3.3.2 Finding Elements . . . . . . . . . . . . . . . . . . . . 73 3.3.3 Adding an Element . . . . . . . . . . . . . . . . . . . 74 3.3.4 Removing an Element . . . . . . . . . . . . . . . . . 77 3.3.5 Amortized Analysis of Spreading and Gathering . . 79 3.3.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . 81 3.4 Discussion and Exercises . . . . . . . . . . . . . . . . . . . . 82 4 Skiplists 87 4.1 The Basic Structure . . . . . . . . . . . . . . . . . . . . . . . 87 4.2 SkiplistSSet : An Eﬃcient SSet . . . . . . . . . . . . . . . 90 4.2.1 Summary . . . . . . . . . . . . . . . . . . . . . . . . . 93 4.3 SkiplistList : An Eﬃcient Random-Access List . . . . . . 93 4.3.1 Summary . . . . . . . . . . . . . . . . . . . . . . . . . 98 4.4 Analysis of Skiplists . . . . . . . . . . . . . . . . . . . . . . . 98 4.5 Discussion and Exercises . . . . . . . . . . . . . . . . . . . . 102 5 Hash Tables 107 5.1 ChainedHashTable : Hashing with Chaining . . . . . . . . . 107 5.1.1 Multiplicative Hashing . . . . . . . . . . . . . . . . . 110 5.1.2 Summary . . . . . . . . . . . . . . . . . . . . . . . . . 114 5.2 LinearHashTable : Linear Probing . . . . . . . . . . . . . . . 114 5.2.1 Analysis of Linear Probing . . . . . . . . . . . . . . . 118 5.2.2 Summary . . . . . . . . . . . . . . . . . . . . . . . . . 121 5.2.3 Tabulation Hashing . . . . . . . . . . . . . . . . . . . 121 5.3 Hash Codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122 5.3.1 Hash Codes for Primitive Data Types . . . . . . . . . 123 5.3.2 Hash Codes for Compound Objects . . . . . . . . . . 123 5.3.3 Hash Codes for Arrays and Strings . . . . . . . . . . 125 5.4 Discussion and Exercises . . . . . . . . . . . . . . . . . . . . 128 6 Binary Trees 133 6.1 BinaryTree : A Basic Binary Tree . . . . . . . . . . . . . . . 135 6.1.1 Recursive Algorithms . . . . . . . . . . . . . . . . . . 136 6.1.2 Traversing Binary Trees . . . . . . . . . . . . . . . . . 136 6.2 BinarySearchTree : An Unbalanced Binary Search Tree . . 140 6.2.1 Searching . . . . . . . . . . . . . . . . . . . . . . . . . 140 6.2.2 Addition . . . . . . . . . . . . . . . . . . . . . . . . . 142 6.2.3 Removal . . . . . . . . . . . . . . . . . . . . . . . . . 144 6.2.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . 146 6.3 Discussion and Exercises . . . . . . . . . . . . . . . . . . . . 147 7 Random Binary Search Trees 153 7.1 Random Binary Search Trees . . . . . . . . . . . . . . . . . . 153 7.1.1 Proof of Lemma 7.1 . . . . . . . . . . . . . . . . . . . 156 7.1.2 Summary . . . . . . . . . . . . . . . . . . . . . . . . . 158 7.2 Treap : A Randomized Binary Search Tree . . . . . . . . . . 159 7.2.1 Summary . . . . . . . . . . . . . . . . . . . . . . . . . 166 7.3 Discussion and Exercises . . . . . . . . . . . . . . . . . . . . 168 8 Scapegoat Trees 173 8.1 ScapegoatTree : A Binary Search Tree with Partial Rebuilding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174 8.1.1 Analysis of Correctness and Running-Time . . . . . 178 8.1.2 Summary . . . . . . . . . . . . . . . . . . . . . . . . . 180 8.2 Discussion and Exercises . . . . . . . . . . . . . . . . . . . . 181 9 Red-Black Trees 185 9.1 2-4 Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186 9.1.1 Adding a Leaf . . . . . . . . . . . . . . . . . . . . . . 187 9.1.2 Removing a Leaf . . . . . . . . . . . . . . . . . . . . 187 9.2 RedBlackTree : A Simulated 2-4 Tree . . . . . . . . . . . . . 190 9.2.1 Red-Black Trees and 2-4 Trees . . . . . . . . . . . . . 190 9.2.2 Left-Leaning Red-Black Trees . . . . . . . . . . . . . 194 9.2.3 Addition . . . . . . . . . . . . . . . . . . . . . . . . . 196 9.2.4 Removal . . . . . . . . . . . . . . . . . . . . . . . . . 199 9.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205 9.4 Discussion and Exercises . . . . . . . . . . . . . . . . . . . . 206 10 Heaps 211 10.1 BinaryHeap : An Implicit Binary Tree . . . . . . . . . . . . . 211 10.1.1 Summary . . . . . . . . . . . . . . . . . . . . . . . . . 217 10.2 MeldableHeap : A Randomized Meldable Heap . . . . . . . 217 10.2.1 Analysis of merge (h1;h2) . . . . . . . . . . . . . . . . 220 10.2.2 Summary . . . . . . . . . . . . . . . . . . . . . . . . . 221 10.3 Discussion and Exercises . . . . . . . . . . . . . . . . . . . . 222 11 Sorting Algorithms 225 11.1 Comparison-Based Sorting . . . . . . . . . . . . . . . . . . . 226 11.1.1 Merge-Sort . . . . . . . . . . . . . . . . . . . . . . . . 226 11.1.2 Quicksort . . . . . . . . . . . . . . . . . . . . . . . . 230 11.1.3 Heap-sort . . . . . . . . . . . . . . . . . . . . . . . . 233 11.1.4 A Lower-Bound for Comparison-Based Sorting . . . 235 11.2 Counting Sort and Radix Sort . . . . . . . . . . . . . . . . . 238 11.2.1 Counting Sort . . . . . . . . . . . . . . . . . . . . . . 239 11.2.2 Radix-Sort . . . . . . . . . . . . . . . . . . . . . . . . 241 11.3 Discussion and Exercises . . . . . . . . . . . . . . . . . . . . 243 12 Graphs 247 12.1 AdjacencyMatrix : Representing a Graph by a Matrix . . . . 249 12.2 AdjacencyLists : A Graph as a Collection of Lists . . . . . . 252 12.3 Graph Traversal . . . . . . . . . . . . . . . . . . . . . . . . . 256 12.3.1 Breadth-First Search . . . . . . . . . . . . . . . . . . 256 12.3.2 Depth-First Search . . . . . . . . . . . . . . . . . . . 258 12.4 Discussion and Exercises . . . . . . . . . . . . . . . . . . . . 261 13 Data Structures for Integers 265 13.1 BinaryTrie : A digital search tree . . . . . . . . . . . . . . . 266 13.2 XFastTrie : Searching in Doubly-Logarithmic Time . . . . . 272 13.3 YFastTrie : A Doubly-Logarithmic Time SSet . . . . . . . . 275 13.4 Discussion and Exercises . . . . . . . . . . . . . . . . . . . . 280 14 External Memory Searching 283 14.1 The Block Store . . . . . . . . . . . . . . . . . . . . . . . . . 285 14.2 B-Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285 14.2.1 Searching . . . . . . . . . . . . . . . . . . . . . . . . . 288 14.2.2 Addition . . . . . . . . . . . . . . . . . . . . . . . . . 290 14.2.3 Removal . . . . . . . . . . . . . . . . . . . . . . . . . 295 14.2.4 Amortized Analysis of B-Trees . . . . . . . . . . . . . 301 14.3 Discussion and Exercises . . . . . . . . . . . . . . . . . . . . 304 Bibliography 309 Index 317 Acknowledgments I am grateful to Nima Hoda, who spent a summer tirelessly proofreading many of the chapters in this book; to the students in the Fall 2011 oﬀering of COMP2402/2002, who put up with the ﬁrst draft of this book and spotted many typographic, grammatical, and factual errors; and to Morgan Tunzelmann at Athabasca University Press, for patiently editing several near-ﬁnal drafts. xi Why This Book? There are plenty of books that teach introductory data structures. Some of them are very good. Most of them cost money, and the vast majority of computer science undergraduate students will shell out at least some cash on a data structures book. Several free data structures books are available online. Some are very good, but most of them are getting old. The majority of these books became free when their authors and/or publishers decided to stop updating them. Updating these books is usually not possible, for two reasons: (1) The copyright belongs to the author and/or publisher, either of whom may not allow it. (2) The source code for these books is often not available. That is, the Word, WordPerfect, FrameMaker, or L ATEX source for the book is not available, and even the version of the software that handles this source may not be available. The goal of this project is to free undergraduate computer science students from having to pay for an introductory data structures book. I have decided to implement this goal by treating this book like an Open Source software project. The L ATEX source, Java source, and build scripts for the book are available to download from the author’s website1and also, more importantly, on a reliable source code management site.2 The source code available there is released under a Creative Commons Attribution license, meaning that anyone is free to share : to copy, distribute and transmit the work; and to remix : to adapt the work, including the right to make commercial use of the work. The only condition on these rights is attribution : you must acknowledge that the derived work contains code and/or text from opendatastructures.org . 1http://opendatastructures.org 2https://github.com/patmorin/ods xiii Why This Book? Anyone can contribute corrections/ﬁxes using the git source-code management system. Anyone can also fork the book’s sources to develop a separate version (for example, in another programming language). My hope is that, by doing things this way, this book will continue to be a useful textbook long after my interest in the project or my pulse, (whichever comes ﬁrst) has waned. xiv Chapter 1 Introduction Every computer science curriculum in the world includes a course on data structures and algorithms. Data structures are that important; they improve our quality of life and even save lives on a regular basis. Many multi-million and several multi-billion dollar companies have been built around data structures. How can this be? If we stop to think about it, we realize that we interact with data structures constantly. • Open a ﬁle: File system data structures are used to locate the parts of that ﬁle on disk so they can be retrieved. This isn’t easy; disks contain hundreds of millions of blocks. The contents of your ﬁle could be stored on any one of them. • Look up a contact on your phone: A data structure is used to look up a phone number in your contact list based on partial information even before you ﬁnish dialing/typing. This isn’t easy; your phone may contain information about a lot of people—everyone you have ever contacted via phone or email—and your phone doesn’t have a very fast processor or a lot of memory. • Log in to your favourite social network: The network servers use your login information to look up your account information. This isn’t easy; the most popular social networks have hundreds of millions of active users. • Do a web search: The search engine uses data structures to ﬁnd the web pages containing your search terms. This isn’t easy; there are §1.1 Introduction over 8.5 billion web pages on the Internet and each page contains a lot of potential search terms. • Phone emergency services (9-1-1): The emergency services network looks up your phone number in a data structure that maps phone numbers to addresses so that police cars, ambulances, or ﬁre trucks can be sent there without delay. This is important; the person making the call may not be able to provide the exact address they are calling from and a delay can mean the di ﬀerence between life or death. 1.1 The Need for E ﬃciency In the next section, we look at the operations supported by the most commonly used data structures. Anyone with a bit of programming experience will see that these operations are not hard to implement correctly. We can store the data in an array or a linked list and each operation can be implemented by iterating over all the elements of the array or list and possibly adding or removing an element. This kind of implementation is easy, but not very e ﬃcient. Does this really matter? Computers are becoming faster and faster. Maybe the obvious implementation is good enough. Let’s do some rough calculations to ﬁnd out. Number of operations: Imagine an application with a moderately-sized data set, say of one million (106), items. It is reasonable, in most applications, to assume that the application will want to look up each item at least once. This means we can expect to do at least one million (106) searches in this data. If each of these 106searches inspects each of the 106items, this gives a total of 106106= 1012(one thousand billion) inspections. Processor speeds: At the time of writing, even a very fast desktop computer can not do more than one billion (109) operations per second.1This 1Computer speeds are at most a few gigahertz (billions of cycles per second), and each operation typically takes a few cycles. The Need for E ﬃciency §1.1 means that this application will take at least 1012=109= 1000 seconds, or roughly 16 minutes and 40 seconds. Sixteen minutes is an eon in computer time, but a person might be willing to put up with it (if he or she were headed out for a co ﬀee break). Bigger data sets: Now consider a company like Google, that indexes over 8.5 billion web pages. By our calculations, doing any kind of query over this data would take at least 8.5 seconds. We already know that this isn’t the case; web searches complete in much less than 8.5 seconds, and they do much more complicated queries than just asking if a particular page is in their list of indexed pages. At the time of writing, Google receives approximately 4 ;500 queries per second, meaning that they would require at least 4 ;5008:5 = 38;250 very fast servers just to keep up. The solution: These examples tell us that the obvious implementations of data structures do not scale well when the number of items, n, in the data structure and the number of operations, m, performed on the data structure are both large. In these cases, the time (measured in, say, machine instructions) is roughly nm. The solution, of course, is to carefully organize data within the data structure so that not every operation requires every data item to be inspected. Although it sounds impossible at ﬁrst, we will see data structures where a search requires looking at only two items on average, independent of the number of items stored in the data structure. In our billion instruction per second computer it takes only 0 :000000002 seconds to search in a data structure containing a billion items (or a trillion, or a quadrillion, or even a quintillion items). We will also see implementations of data structures that keep the items in sorted order, where the number of items inspected during an operation grows very slowly as a function of the number of items in the data structure. For example, we can maintain a sorted set of one billion items while inspecting at most 60 items during any operation. In our billion instruction per second computer, these operations take 0 :00000006 seconds each. The remainder of this chapter brieﬂy reviews some of the main concepts used throughout the rest of the book. Section 1.2 describes the in- §1.2 Introduction terfaces implemented by all of the data structures described in this book and should be considered required reading. The remaining sections discuss: • some mathematical review including exponentials, logarithms, factorials, asymptotic (big-Oh) notation, probability, and randomization; • the model of computation; • correctness, running time, and space; • an overview of the rest of the chapters; and • the sample code and typesetting conventions. A reader with or without a background in these areas can easily skip them now and come back to them later if necessary. 1.2 Interfaces When discussing data structures, it is important to understand the difference between a data structure’s interface and its implementation. An interface describes what a data structure does, while an implementation describes how the data structure does it. Aninterface , sometimes also called an abstract data type , deﬁnes the set of operations supported by a data structure and the semantics, or meaning, of those operations. An interface tells us nothing about how the data structure implements these operations; it only provides a list of supported operations along with speciﬁcations about what types of arguments each operation accepts and the value returned by each operation. A data structure implementation , on the other hand, includes the internal representation of the data structure as well as the deﬁnitions of the algorithms that implement the operations supported by the data structure. Thus, there can be many implementations of a single interface. For example, in Chapter 2, we will see implementations of the List interface using arrays and in Chapter 3 we will see implementations of the List interface using pointer-based data structures. Each implements the same interface, List , but in di ﬀerent ways. Interfaces §1.2 x · · · add (x)/enqueue (x) remove ()/dequeue () Figure 1.1: A FIFO Queue . 1.2.1 The Queue ,Stack , and Deque Interfaces TheQueue interface represents a collection of elements to which we can add elements and remove the next element. More precisely, the operations supported by the Queue interface are •add(x): add the value xto the Queue •remove (): remove the next (previously added) value, y, from the Queue and return y Notice that the remove () operation takes no argument. The Queue ’squeueing discipline decides which element should be removed. There are many possible queueing disciplines, the most common of which include FIFO, priority, and LIFO. AFIFO (ﬁrst-in-ﬁrst-out) Queue , which is illustrated in Figure 1.1, removes items in the same order they were added, much in the same way a queue (or line-up) works when checking out at a cash register in a grocery store. This is the most common kind of Queue so the qualiﬁer FIFO is often omitted. In other texts, the add(x) and remove () operations on a FIFO Queue are often called enqueue (x) and dequeue (), respectively. Apriority Queue , illustrated in Figure 1.2, always removes the smallest element from the Queue , breaking ties arbitrarily. This is similar to the way in which patients are triaged in a hospital emergency room. As patients arrive they are evaluated and then placed in a waiting room. When a doctor becomes available he or she ﬁrst treats the patient with the most life-threatening condition. The remove (x) operation on a priority Queue is usually called deleteMin () in other texts. A very common queueing discipline is the LIFO (last-in-ﬁrst-out) discipline, illustrated in Figure 1.3. In a LIFO Queue , the most recently added element is the next one removed. This is best visualized in terms of a stack of plates; plates are placed on the top of the stack and also §1.2 Introduction 16add(x)remove ()/deleteMin () x6 Figure 1.2: A priority Queue . · · · remove ()/pop ()add (x)/push (x) x Figure 1.3: A stack. removed from the top of the stack. This structure is so common that it gets its own name: Stack . Often, when discussing a Stack , the names ofadd(x) and remove () are changed to push (x) and pop(); this is to avoid confusing the LIFO and FIFO queueing disciplines. ADeque is a generalization of both the FIFO Queue and LIFO Queue (Stack ). A Deque represents a sequence of elements, with a front and a back. Elements can be added at the front of the sequence or the back of the sequence. The names of the Deque operations are self-explanatory: addFirst (x),removeFirst (),addLast (x), and removeLast (). It is worth noting that a Stack can be implemented using only addFirst (x) and removeFirst () while a FIFO Queue can be implemented using addLast (x) andremoveFirst (). 1.2.2 The List Interface: Linear Sequences This book will talk very little about the FIFO Queue ,Stack , orDeque interfaces. This is because these interfaces are subsumed by the List interface. A List , illustrated in Figure 1.4, represents a sequence, x0;:::;xn 1, Interfaces §1.2 e4 5 6 7 n−1 · · · f b k c a0 1 2 3 b c d· · · Figure 1.4: A List represents a sequence indexed by 0 ;1;2;:::;n. In this List a call to get(2) would return the value c. of values. The List interface includes the following operations: 1.size (): return n, the length of the list 2.get(i): return the value xi 3.set(i;x): set the value of xiequal to x 4.add(i;x): add xat position i, displacing xi;:::;xn 1; Setxj+1=xj, for allj2fn 1;:::;ig, increment n, and set xi=x 5.remove (i) remove the value xi, displacing xi+1;:::;xn 1; Setxj=xj+1, for allj2fi;:::;n 2gand decrement n Notice that these operations are easily su ﬃcient to implement the Deque interface: addFirst (x)) add(0;x) removeFirst ()) remove (0) addLast (x)) add(size ();x) removeLast ()) remove (size () 1) Although we will normally not discuss the Stack ,Deque and FIFO Queue interfaces in subsequent chapters, the terms Stack andDeque are sometimes used in the names of data structures that implement the List interface. When this happens, it highlights the fact that these data structures can be used to implement the Stack orDeque interface very e ﬃciently. For example, the ArrayDeque class is an implementation of the List interface that implements all the Deque operations in constant time per operation. §1.2 Introduction 1.2.3 The USet Interface: Unordered Sets TheUSet interface represents an unordered set of unique elements, which mimics a mathematical set. AUSet contains ndistinct elements; no element appears more than once; the elements are in no speciﬁc order. A USet supports the following operations: 1.size (): return the number, n, of elements in the set 2.add(x): add the element xto the set if not already present; Add xto the set provided that there is no element yin the set such that xequals y. Return true ifxwas added to the set and false otherwise. 3.remove (x): remove xfrom the set; Find an element yin the set such that xequals yand remove y. Return y, ornull if no such element exists. 4.find (x): ﬁnd xin the set if it exists; Find an element yin the set such that yequals x. Return y, ornull if no such element exists. These deﬁnitions are a bit fussy about distinguishing x, the element we are removing or ﬁnding, from y, the element we may remove or ﬁnd. This is because xandymight actually be distinct objects that are nevertheless treated as equal.2Such a distinction is useful because it allows for the creation of dictionaries ormaps that map keys onto values. To create a dictionary/map, one forms compound objects called Pair s, each of which contains a keyand a value . Two Pair s are treated as equal if their keys are equal. If we store some pair ( k;v) in a USet and then later call the find (x) method using the pair x= (k;null ) the result will be y= (k;v). In other words, it is possible to recover the value, v, given only the key, k. 2In Java, this is done by overriding the class’s equals (y) and hashCode () methods. Mathematical Background §1.3 1.2.4 The SSet Interface: Sorted Sets The SSet interface represents a sorted set of elements. An SSet stores elements from some total order, so that any two elements xand ycan be compared. In code examples, this will be done with a method called compare (x;y) in which compare (x;y)8>>>><>>>>:<0 if x<y >0 if x>y = 0 if x=y AnSSet supports the size (),add(x), and remove (x) methods with exactly the same semantics as in the USet interface. The di ﬀerence between a USet and an SSet is in the find (x) method: 4.find (x): locate xin the sorted set; Find the smallest element yin the set such that yx. Return yor null if no such element exists. This version of the find (x) operation is sometimes referred to as a successor search . It diﬀers in a fundamental way from USet:find (x) since it returns a meaningful result even when there is no element equal to x in the set. The distinction between the USet andSSet find (x) operations is very important and often missed. The extra functionality provided by an SSet usually comes with a price that includes both a larger running time and a higher implementation complexity. For example, most of the SSet implementations discussed in this book all have find (x) operations with running times that are logarithmic in the size of the set. On the other hand, the implementation of a USet as a ChainedHashTable in Chapter 5 has afind (x) operation that runs in constant expected time. When choosing which of these structures to use, one should always use a USet unless the extra functionality o ﬀered by an SSet is truly needed. 1.3 Mathematical Background In this section, we review some mathematical notations and tools used throughout this book, including logarithms, big-Oh notation, and proba- §1.3 Introduction bility theory. This review will be brief and is not intended as an introduction. Readers who feel they are missing this background are encouraged to read, and do exercises from, the appropriate sections of the very good (and free) textbook on mathematics for computer science [50]. 1.3.1 Exponentials and Logarithms The expression bxdenotes the number braised to the power of x. Ifxis a positive integer, then this is just the value of bmultiplied by itself x 1 times: bx=bbb| {z } Whenxis a negative integer, bx= 1=b x. Whenx= 0,bx= 1. Whenbis n