Michael Biehl The Shallow and the DeepA biased introduction to neural networks and old school machine learning The Shallow and the Deep is a collection of lecture notes that offers an accessible introduction to neural networks and machine learning in general. However, it was clear from the beginning that these notes would not be able to cover this rapidly changing and growing field in its entirety. The focus lies on classical machine learning techniques, with a bias towards classification and regression. Other learning paradigms and many recent developments in, for instance, Deep Learning are not addressed or only briefly touched upon.Biehl argues that having a solid knowledge of the foundations of the field is essential, especially for anyone who wants to explore the world of machine learning with an ambition that goes beyond the application of some software package to some data set . Therefore, The Shallow and the Deep places emphasis on fundamental concepts and theoretical background. This also involves delving into the history and pre-history of neural networks, where the foundations for most of the recent developments were laid.These notes aim to demystify machine learning and neural networks without losing the appreciation for their impressive power and versatility./uni00A0 Michael Biehl is Associate Professor of Computer Science at the Bernoulli Institute for Mathematics, Computer Science and Artificial Intelligence of the University of Groningen, where he joined the Intelligent Systems group in 2003. He also holds an honorary Professorship of Machine Learning at the Center for Systems Modelling and Quantitative Biomedicine of the University of Birmingham, UK. His research focuses on the modelling and theoretical understanding of neural networks and machine learning in general. The development of efficient training algorithms for interpretable, transparent systems is a topic of particular interest. A variety of interdisciplinary collaborations concern practical applications of machine learning in the biomedical domain, in astronomy and other areas. T/h.smcp/e.smcp S/h.smcp/a.smcp/l.smcp/l.smcp/o.smcp/w.smcp /a.smcp/n.smcp/d.smcp /t.smcp/h.smcp/e.smcp D/e.smcp/e.smcp/p.smcp Michael Biehl The Shallow and the Deep The Shallow and the Deep A biased introduction to neural networks and old school machine learning Michael Biehl Published by University of Groningen Press Broerstraat 4 9712 CP Groningen The Netherlands First published in the Netherlands © 2023 Michael Biehl, Bernoulli Institute for Mathematics, Computer Science and Artificial Intelligence, Groningen Comments, corrections and suggestions are welcome, contact: m.biehl@rug.nl Please cite as: Biehl, M. (2023). The Shallow and the Deep: A biased introduction to neural networks and old school machine learning. University of Groningen Press. This book has been published open access thanks to the financial support of the Open Access Textbook Fund of the University of Groningen. Cover design: Bas Ekkers Coverphoto: Michael Biehl Production: LINE UP boek en media bv ISBN (print) 9789403430287 ISBN (ePDF) 9789403430270 DOI https://doi.org/ 10.21827/648c59c1a467e This work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. The full licence terms are available at creativecommons.org/ licenses/ by-nc-sa/4.0/legalcode Preface StopcallingeverythingAI. —MichaelI.Jordan,in[Pre21] Thesubtitleofthese lecturenotesis“Abiasedintroductiontoneura lnetworks andoldschoolmachinelearning”forgoodreasons.Althoughtheaimwasto giveanaccessi bleintroductiontotheﬁeld,ithasbeenclearfromthebeginning thatitwouldnotendupasacomprehensi ve,completeoverview.The focusis onclassicalmachinelearning,manyrecentdevelopmen tscannotbecovered. Personally,Iﬁrstgotintouchwithneura lnetworksinmyearlylifeasa physicist.Atthetime,itwassuﬃci enttoreadahandful ofpapersandperhaps alittlelaterthegoodbook[HKP91]tobeup-to-dateandabletocontributea piecetothebigpuzzl e.AmIexaggeratingandsomewha tnostalgic?Probably. But thesituationhasdeﬁni telychangedalot.Nowadays,anoverwhelming ﬂoodofpublicationsmakesitdiﬃcul ttoﬁlterouttherelevantinformationand keepupwiththedevelopmen ts. Theselectionoftopicsinthese noteshasbeendeterminedtoalargeextent bymyownresearchinterestsandearlyexperiences. Thisisdeﬁni telytruefor theinitialfocusonthesimpleperceptron,thehydrogenatomofneuralnetwork researchasManfredOpperputit[Opp90].Moreover,thebulkofthistextdeals withshallowsystemsforsupervisedlearning,inparticularclassiﬁcation,which reﬂect smymaininterestintheﬁeld. The notesmaybeperceivedasoldschool,certainlybysomededicatedfollowersoffashion[Dav66].Admittedly,thetextdoesnotaddressthemost recentdevelopmen tsine.g.DeepLearninganditsapplications.However,in myhumbleopinionitisinvaluabletohaveasolidbackgroundknowledgeofthe basicsbeforeexploringtheworldofmachinelearningwithanambitionthat goesbeyondtheapplicationofsomesoftwarepackagetosomedataset. Ther efore,theempha sisisonbasicconcept sandtheoreticalbackground, withspeciﬁcaspectsselectedfromapersonalandclearlybiasedviewpoint.In asense, thegoalistode-m ystifymachinelearningandneura lnetworkswithout iii iv losingtheappreciationfortheirfascinatingpowerandversatility.Veryoften, thisinvolvesalookintothehistoryandpre-historyofneura lnetworks,wher e thefounda tionsformostoftherecentdevelopmen tswerelaid. Ihaveaimed atpointingtheinterestedreadertomanyresourcesforfurther explorationofthearea.Ther efore,thelistofreferences inthebibliography, althoughbynomeanscomplete,isslightlymoreextensivethaninitiallyenvisioned. Thestartingpointforthese noteswasthedesiretoprovidemorecomprehensivematerialthanthepresentationslidesintheMSc levelcourseNeuralNetworks(renamed NeuralNetworksandComputationalIntelligencelater)which IhavebeengivingattheUniversityofGroningen.Athorougharcheological investigationofthetextandﬁgureswouldalsorevealtracesofthecoursesTheorieNeuronalerNetzwerkeandUnüberwachtesLernen,thatItaughtwayback when inthePhysicsprogramattheUniversityofWürzbur g. Mywritingactivitywasgreatlyboostedontheoccasionofthewonderful 30thCanaryIslandsWinterSchoolin2018,devotedtoBigDataanalysisin Astronomy,wher eIhadthehonortogiveaseriesoflecturesonsupervised learning,see[MSK 19,Bie19]forcoursematerialsandvideo-reco rdedlectures. LastnotleastIwouldliketoacknowledgeconstructivefeedba ckfromseveral “generations”ofstuden tswho followedthecourseandfrommanycolleagues andcollaborators.Inparticular,IthankElisaOostwalandJanisNordenfora criticalreadingofthemanuscriptandmanysuggestionsforimpro vemen ts. Groningen,June2023 Themysteriousmachinelearningmachine ©CatharinaM.GerigkandElinaL.vandenBrandho f Reproduced withkindpermissionoftheartists. Contents Preface iii 1Fromneuronstonetworks 1 1.1Spikingneuronsandsynapticinteractions.............3 1.2Firingratemodels..........................5 1.2.1Neuralactivityandsynapticinteraction..........5 1.2.2Sigmoidalactivationfunct ions...............6 1.2.3Hebbianlearning.......................8 1.3Networkarchitectures........................9 1.3.1AttractornetworksandtheHopﬁeldmodel........10 1.3.2Feed-fo rwardlayeredneuralnetworks...........12 1.3.3Otherarchitectures......................15 2Learningfromexampledata 17 2.1Learningscena rios..........................17 2.1.1Unsup ervisedlearning....................17 2.1.2Supervisedlearning .....................19 2.1.3Otherlearningscena rios...................22 2.2MachineLearningvs.StatisticalModelling............23 2.2.1Diﬀerences andcommo nalities...............23 2.2.2Anexamplecase:linearregression.............24 2.2.3Conclusion..........................30 3ThePerceptron 31 3.1Historyandliterature........................31 3.2Linearlyseparablefunct ions.....................33 3.3TheRosenblattperceptron.....................36 3.3.1Theperceptronstorageproblem ..............36 3.3.2IterativeHebbiantrainingalgorithms ...........37 3.3.3TheRosenblattperceptronalgorithm ...........39 3.3.4Theperceptronalgorithmasgradientdescen t.......41 3.3.5ThePerceptronConvergence Theo rem...........42 3.3.6Afewremarks........................45 3.4Thecapacityofahyperplane....................46 v vi CONTEN TS 3.4.1Thenumberoflinearlyseparabledichotomies.......46 3.4.2Discussi onoftheresult...................52 3.4.3Timeforapizzaorsomecake................53 3.5Learningalinearlyseparablerule..................55 3.5.1Studen t-teacherscena rio...................55 3.5.2Learninginversionspace..................57 3.5.3Gener alizationbeginswher estorageends .........60 3.5.4Optimalgeneralization....................62 3.6Theperceptronofoptimalstability.................63 3.6.1Thestabilitycriterion....................63 3.6.2TheMinOveralgorithm...................65 3.7Optimalstabilitybyquadraticoptimization............67 3.7.1Optimalstabilityreformulated...............67 3.7.2TheAdaptiveLinearNeuron-Adaline...........68 3.7.3TheAdaptivePerceptronAlgorithm-AdaTron......73 3.7.4Supp ortvectors........................78 3.8Inhom.lin.sep.funct ionsrevisited.................80 3.9Someremarks.............................81 4Beyondlinearseparability 83 4.1Perceptronwitherrors........................85 4.1.1Minimalnumberoferrors..................85 4.1.2Softmarginclassiﬁer.....................87 4.2Layerednetworksofperceptron-likeunits.............90 4.2.1Committeeandparitymachines ..............91 4.2.2Theparitymachine:auniversalclassiﬁer .........92 4.2.3Thecapacityofmachines ..................95 4.3Supp ortVectorMachines ......................97 4.3.1Non-lineartransformationtohigherdimensi on......98 4.3.2Largemarginclassiﬁer....................99 4.3.3Thekerneltrick.......................100 4.3.4Afewremarks........................104 5Feed-forwardnetworksforregressionandclassiﬁcation 107 5.1Feed-fo rwardnetworksasnon-linearfunct ionapproximators...107 5.1.1Architectureandinput-outputrelation...........108 5.1.2Universalapproximators...................109 5.2Gradientbasedtrainingoffeed-fo rwardnets............114 5.2.1Comput ingthegradient:BackpropagationofError....116 5.2.2Batchgradientdescen t....................116 5.2.3Stochasticgradientdescen t.................119 5.2.4Practicalaspectsandmodiﬁcations.............122 5.3Objectivefunctions..........................123 5.3.1Costfunct ionsforregression................124 5.3.2Costfunct ionsforclassiﬁcation...............125 5.4Activationfunct ions.........................127 CONTEN TS vii 5.4.1Sigmoidalandrelatedfunct ions...............127 5.4.2One-sidedandunbounded activationfunct ions......128 5.4.3Exponentialandnormalizedactivations..........130 5.4.4Remark:universalfunct ionapproximation.........131 5.5Speciﬁcarchitectures.........................131 5.5.1Popularshallownetworks..................132 5.5.2Deepandconvolutionalneuralnetworks..........135 6Distance-basedclassiﬁers 141 6.1Prototype-basedclassiﬁers ......................143 6.1.1NearestNeighborandNearestPrototypeClassiﬁers ...143 6.1.2LearningVectorQuantization................144 6.1.3LVQtrainingalgorithms ...................145 6.2Distancemeasures andrelevancelearning.............148 6.2.1LVQbeyondEuclideandistance ..............148 6.2.2Adaptivedistances inrelevancelearning..........149 6.3Concludingremarks.........................153 7Modelevaluationandregularization 155 7.1Biasandvariance,over-andunder ﬁtting .............155 7.1.1Decompositionoftheerror.................156 7.1.2Thebias-variancedilemma .................158 7.1.3Beyondtheclassicalbias-variancetrade-oﬀ(?)......161 7.2Controllingthenetworkcomplexity.................163 7.2.1Earlystopping........................163 7.2.2Weightdecayandrelatedconcept s.............164 7.2.3Constructivealgorithms ...................167 7.2.4Pruning............................167 7.2.5Weight-sharing........................169 7.2.6Dropout............................170 7.3Cross-validationandrelatedmethods................171 7.3.1n-foldcross-validationandrelatedschemes ........172 7.3.2Modelandparameterselection...............175 7.4Performancemeasures ........................175 7.4.1Measures forregression...................176 7.4.2Measures forclassiﬁcation..................176 7.4.3ReceiverOperatingCharacteristics.............177 7.4.4Theareaunder theROCcurve...............180 7.4.5Alternativemeasures fortwo-classproblems ........181 7.4.6Multi-classproblems .....................182 7.4.7Averagesofclass-wi sequalitymeasures..........183 7.5Interpretablesystems.........................185 viii CONTEN TS 8Preprocessingandunsupervisedlearning 187 8.1Normalizationandtransformations.................188 8.1.1Coordinate-wisetransformations..............189 8.1.2Normalization.........................191 8.2Dimensi onalityreduct ion......................192 8.2.1Low-dimensi onalembedding.................194 8.2.2Multi-dimensi onalScaling..................194 8.2.3NeighborhoodEmbedding..................195 8.2.4Featureselection.......................196 8.3PCA andrelatedmethods......................197 8.3.1PrincipalComponentAnalysis...............198 8.3.2PCA byHebbianlearning..................200 8.3.3Indep enden tComponentAnalysis.............203 8.4ClusteringandVectorQuantization................204 8.4.1Basicclusteringmethods..................205 8.4.2CompetitivelearningforVectorQuantization.......206 8.4.3Practicalissues andextensionsofVQ...........208 8.5Densityestimation..........................211 8.5.1Parametricdensi tyestimation................211 8.5.2GaussianMixtureModels..................212 8.6Missingvaluesandimput ationtechniques .............215 8.6.1Approacheswithoutexplicitimput ation..........216 8.6.2Imput ationbasedonavailabledata.............216 8.7Over-andunder sampling,augmentation..............218 8.7.1Weightedcostfunct ions...................218 8.7.2Undersampling........................218 8.7.3Oversampling.........................219 8.7.4Dataaugmentation......................220 Concludingquote 223 AOptimiz ation 225 A.1Multi-dimensi onalTaylorexpansion................225 A.2Localextrema andsaddlepoints..................227 A.2.1Necessa ryandsuﬃci entconditions.............227 A.2.2Example:unsolvablesystemsoflinearequations.....228 A.3Constrainedoptimization......................230 A.3.1Equalityconstraints.....................230 A.3.2Example:under -determinedlinearequations.......231 A.3.3Inequalityconstraints....................232 A.3.4TheWolfeDualforconvexproblems ............234 A.4Gradientbasedoptimization.....................235 A.4.1Gradientanddirectionalderivative.............235 A.4.2Gradientdescen t.......................236 A.4.3Thegradientunder coordinatetransformations......238 A.5Variantsofgradientdescen t.....................239 CONTEN TS ix A.5.1Coordinatedescen t......................239 A.5.2Constrainedproblemsandprojectedgradients......240 A.5.3Stochasticgradientdescen t.................240 A.6Examplecalculationofagradient..................243 Listofﬁgures 246 Listofalgorithms 247 Abbrev.andacronyms 248 Bibliography 250 1.Abandontheideathatyouareevergoingtoﬁnish. —JohnSteinbeck,SixWritingTips x CONTEN TS Chapter1 Fromneuronstonetworks Realityisoverratedanyway. —Unknown Tounder standandexplainthebrain’sfascinatingcapabilities1remainsone ofthegreatestscientiﬁcchallengesever.Thisisparticularlytrueforitsplasticity,i.e.theabilitytolearnfromexperience, toadapttoandtosurvivein ever-changingenvironmen ts. Ultimately,theperformanceofthebrainmustrelyonitshardware(orwetware[LB89])andemer gesfromthecooperativebehaviorofitsmany,relatively simple,yethighlyinterconnect edbuildingblocks:theneurons.The human cortex,forinstance, comprisesanestimatednumberof1012neuronsandeach individualcellcanbeconnect edtothousandsofothers. InthisintroductiontoNeuralNetworksandComputationalIntelligencewe willstudyartiﬁcialneuralnetworksandrelatedsystems, designedforthepurposeofadaptiveinformationprocessing.The degreetowhichthese systems relatetotheirbiologicalcounterpartsis,generallyspeaking,quitelimited.However,theirdevelopmen twasgreatlyinspiredbykeyaspectsofbiologicalneurons andnetworks.Ther efore,itisuseful tobeawareoftheconcept ualconnect ions betweenartiﬁcialandbiologicalsystems, atleastonabasiclevel. Quiteoften,technicalsolutionsareinspiredbynaturalsystemswithoutcopyingalltheirpropertiesindetail.Duetobiologicalconstraints,nature(i.e.evolution)mighthavefoundhighlycomplexsolutionstocertainproblemsthatcould bedealtwithinasimplerfashioninatechnicalrealization.Asomewha toverused exampleinthiscontextistheconstructionofeﬃcientaircraft,whichbyno meansrequirestheuseofmovingwingsinordertoimitatebirdﬂightfaithfully. Ofcourse,itisunclearaprioriwhichofthedetailsareessen tialandwhich onescanbeleftoutinartiﬁcialsystems. Obviously,thisalsodepends onthe 1(including thecapabilit yofbeing fascinated) 2 1.FROMNEURONSTONETW ORKS speciﬁctaskandcontext.Consequently,theinteractionbetweentheneurosciences andmachinelearningresearchcontinuestoplayanimportantrolefor thefurtherdevelopmen tofboth. Inthisintroductorytextwewillconsiderlearningsystems, whichdrawon onlythemostbasicmechanisms. Ther efore,thischapterisonlymeantasavery briefoverview,whichshouldallowtorelatesomeoftheconcept sinartiﬁcial neura lcomput ationtotheirbiologicalbackground. Thereadershouldbeaware thatthepresentationiscertainlyover-simplifyingandprobablynotquiteupto-dateinallaspects. Recomme ndedtextbooksandothersources Inmostofthisintroductorychapter,detailedcitationsconcerningspeciﬁctopics willnotbeprovided. Instead,thefollowinglistpointsthereadertoselected textbooks,reviewsorlecturenotes.They rangefrombriefandsuperﬁcialto verycomprehensi veanddetailedreviewsofthebiologicalbackground. Thesame istrueforthediscussi onofthediﬀerentconcept uallevelsonwhichbiological systems canbemodelled.References pointtothefullcitationinformationin thebibliography.Notethattheselectioniscertainlyincompleteandbiasedby personalpreferences. ◦K.Guer ney’s(NeuralNetworks)givesaverybasicoverviewandprovides aglossaryofbiologicalorbiologicallyinspiredterms[Gue9 7]. ◦TheﬁrstsectionsofNeuralNetworksandLearningMachinesbyS.Haykin covertherelevanttopicsinslightlygreaterdepth[Hay09]. ◦TheclassicaltextbookNeuralNetworks:AnIntroductiontotheTheoryof NeuralComputationbyJ.A.Hertz,A.KroghandR.G.Palmerdiscusses theinspirationfrombiologicalneuro nsandnetworksintheﬁrstchapters. ItalsoprovidesathoroughanalysisoftheHopﬁeldmodelfromastatistical physicsperspective[HKP91]. ◦H.HornerandR.KühngiveabriefgeneralintroductioninNeuralNetworks[HK98],includingabasicdiscussi onofthebiologicalbackground. ◦Modelsofbiologicalneuro ns,theirbio-chemi stryandbio-physicsarein thefocusofC.Koch’scomprehensi vemonographontheBiophysicsof computation[Koc98].Itdiscusses thediﬀerentmodellingapproachesand relatesthem toexperimentaldataobtainedfromrealworldneurons. ◦T.Kohonenhasintroduced importantprototype-basedlearningschemes. AnentirechapterofhisseminalworkSelf-OrganizingMapsisdevotedto theJustiﬁcationofNeuralModeling[Koh97]. ◦H.Ritter,T.MartinetzandK.Schultengiveanoverviewandalsodiscuss someaspectsoftheorganizationofthebrainintermsofmapsintheir monographNeuralComputationandSelf-OrganizingMaps[RMS92]. ◦M.vanRossum’ slecturenotesonNeuralComputationprovideanoverview ofbiologicalinformationprocessingandmodelsofneuralactivity,synaptic interactionandplasticity.Moreover,modellingapproachesarediscussed insomedetail[Ros16]. 1.1.SPIK INGNEURONSANDSYNAPTIC INTERACTIO NS 3 1.1Spikingneuronsandsynapticinteractions The physiologyandfunct ionalityofthebiologicalsystems ishighlycomplex, alreadyonthesingleneuro nlevel.Sophisticatedmodellingframeworkshave beendevelopedthattakeintoaccounttherelevantelectro-chemi calprocesses ingreatdetailinordertorepresen tthebiologyasfaithfullyaspossible.This includes thefamousHodgkin-Huxleymodelandvariantsthereof. They descr ibethestateofcellcompartmentsintermsofanelectrostatic potential,whichisduetovaryingionconcentrationsonbothsidesofthecell mem brane.Anumberofionchannelsandpumpscontroltheconcentrations and,thus,governthemem branepotential.TheoriginalHodgkin-Huxleymodel descr ibesitstemporalevolutionintermsoffourcoupledordinarydiﬀerential equations,theparametersofwhichcanbeﬁttedtoexperimentaldatameasured inrealworldneurons. Whenev erthemem branepotentialreachesathresholdvalue,forinstance triggeredbytheinject ionofanexternalcurrent,ashort,localizedelectrical pulseisgenerated.The termactionpotentialorthemoresloppyspikewillbe used synonymously.Theneuro nissaidtoﬁrewhen aspikeisgenerated. Theactionpotentialdischargesthemem branelocallyandpropagatesalong themem brane.AsillustratedinFigure1.1(leftpanel),astronglyelongated extensionisattachedtothesoma,theso-calledaxon.Fromapurelytechnical pointofview,itservesasacablealongwhichactionpotentialscantravel. Ofcourse,theactualelectro-chemi calprocesses aresigniﬁcantlydiﬀerent fromtheﬂowofelectronsinaconventionalcoppercable,forinstance. Infact, actionpotentialsjump betweenshortgapsinthemyelinsheath,aninsulating layeraround theaxon.Bymeansofsaltatoryconduction,actionpotentials sprea dalongtheaxonicbranchesoftheﬁringneuronandeventuallyreachthe pointswher ethebranchesconnect tothedendr itesofotherneuro ns. Such aconnect ion,termed synapse,isshownschema ticallyinFig.1.1(rightpanel). Uponarrivalofaspike,so-calledneuro-transmittersarereleasedintothesynapticcleft,i.e.thegapbetweenpre-synapticaxonbranchandthepost-synaptic dendr ite.Thetransmittersarereceivedonthepost-synapticsidebysubst ance speciﬁcrecept ors.Thus,inthesynapse,theactionpotentialisnottransfer red directlythroughaphysicalcontactpoint,butchemi cally.2Theeﬀect thatanarrivingspikehasonthepost-synapticneurondepends onthedetailedproperties ofthesynapse: ◦Ifthesynapseisoftheexcitatorytype,thepost-synapticmem branepotentialincrea sesuponarrivalofthepre-synapticspike, ◦When aspikearrivesataninhibitorysynapse, thepost-synapticmembranepotentialdecreases. Bothexcitatoryandinhibitorysynapsescanhavevaryingstrengths,asreﬂect ed 2Notethat alsoso-called gapjunctions existwhichcanfunction asbi-dir ectional electrical synaps es,seee.g.[CL04] forfurther information andreferences. 4 1.FROMNEURONSTONETW ORKS Figure1.1:Schema ticillustrationofneuro ns(pyramidalcells)andtheirconnections.Left:Pre-synapticandpost-synapticneuro nswithsoma,dendr itic tree,axon,andaxonicbranches.Right:Thesynapticcleftwithvesiclesreleasingneuro -transmittersandcorrespondingrecept orsonthepost-synapticside. Redrawnafter[Kat66]. inthemagnitudeofthechangethataspikeimposesonthepost-synapticmembranepotential. Consequently,themem branepotentialofaparticularcellwillvaryovertime, dependingontheactualactivitiesoftheneuro nsitreceivesspikesfromthrough excitatoryandinhibitorysynapses. When thethresholdforspikegenerationis reached, theneuronﬁresitselfand,thus,inﬂuences thepotentialandactivityof allitspost-synapticneighbors.Allinall,asetofinterconnect edneuronsforms acomplexdynamicalsystemofthresholdunitswhichinﬂuence eachother’s activitythroughgenerationandsynaptictransmissionofactionpotentials. The originofaverysuccessful approachtothemodellingofneuro nalactivitydatesbacktoLouisLapicquein1907.Intheframeworkoftheso-called Integrate-and-F ire(IaF)model,electro-chemi caldetailsaccountedforinthe Hodgkin-Huxleytypeofmodelsareomitted(andwereprobablynotknownat thetime).The mem braneissimplyrepresen tedbyitsconduct anceandohmic resistance. Allchargetransportpheno mena arecombinedinoneeﬀect iveelectriccurrent,whichsumma rizestheindividualcontributionsofchangingion concentrationsaswellasleakcurrentsthroughthemem brane.Similarly,the preciseformofspikesanddetailsoftheirgenerationandtransportareignored. Instead,theﬁringismodelledasanall-or-nothingthresholdprocess, whichresultsinaninstantaneousdischarge.Aspikeisrepresen tedbyastructureless Diracdeltafunct ionwhichdeﬁnes thetimepointoftheevent.Despiteitssimplicitycomparedtomorerealisticelectro-chemi calmodels,theIaFmodelcan beﬁttedtophysiologicaldataandyieldsafairlyrealisticdescr iptionofneuronal activity. 1.2.FIRINGRATEMODELS 5 time [ms] Figure1.2:Left(upper):Schema ticillustrationofanactionpotential,i.e. ashortpulseonmV-andms-scale.Left(lower):Spikestravelalongtheaxon throughsaltatoryconduct ionviagapsintheinsulatingmyelinsheath.Right: Schema ticillustrationofhowmeanﬁringratesarederivedfromatemporal spikepattern. 1.2Firin gratemodels Inanotherstepofabstraction,thedescr iptionofneuralactivityissimpliﬁed bytakingintoaccountonlythemeanﬁringrate,e.g.obtainedastheaverage numberofspikesperunittime; theconcept isillustratedinFig.1.2(right panel). The implicitassumpt ionisthatmostoftheinformationinneuralprocessingiscontainedinthemeanactivityandfrequency ofspikesoftheneuro ns. Hence, theprecisetimingofindividualactionpotentialsiscompletelydisregarded. Whiletheroleofindividualspiketimingappearstobethetopicof ongoingdebateintheneurosciences3,thesimpliﬁcationclearlyfacilitateseﬃcientsimulationsofverylargenetworksofneuro nsandcanbeseen asthebasis ofvirtuallyallartiﬁcialneura lnetworksandlearningsystemsconsidered inthis text. 1.2.1 Neuralactivityandsynapticinteraction The ﬁringratepictureallowsforasimplemathema ticaldescr iptionofneural activityandsynapticinteraction.ConsiderthemeanactivitySiofneuro ni, whichreceivesinput fromasetJofneuronswithj∕=i.Takingintoaccountthe factthattheﬁringrateofabiologicalneuroncannotexceed acertainmaximum duetophysiologicalandbio-chemi calconstraints,wecanlimitSitoarangeof values0≤Siwher etheupperlimit1isgiveninarbitraryunits.The resting stateSi=0obviouslycorrespondstotheabsence ofanyspikegeneration. Theactivityofneuroniisgivenasa(non-linear)responseofincomingspikes, whichare-however-alsorepresen tedonlybythemeanactivitiesSj:in Si=h(xi)withxi=󰁛 j∈JwijSj. (1.1) 3See, e.g., http://r omain brette.fr /categor y/blog/r ate-v s-timing/ forfurther references. 6 1.FROMNEURONSTONETW ORKS Here,thequantitieswij∈Rrepresen tthestrengthofthesynapseconnect ing oneneuronj∈Jwithneuro ni.Positivewij>0increa setheso-calledlocal potentialxiifneuro njisactive(Sj>0),whilewij<0contributenegative termstotheweightedsum. Notethatrealworldchemi calsynapses arestrictly uni-directional:evenifconnect ionswijandwjiexistforagivenpairofneuro ns, theywouldbephysiologicallyseparate,indep enden tentities. 1.2.2 Sigmoid alactivationfunctions Avarietyofdiﬀerentactivationfunct ionsh(x)havebeenempl oyedinartiﬁcial neura lnetworks.Afewspeciﬁctypesoffunct ionswillbeintroduced inalater chapter.Herewerestrictthediscussi ontothebynowclassicalsigmoidalactivationwhicharguablycapturesimportantcharacteristicsofbiologicalsystems. Itisplausibletoassume thefollowingmathema ticalpropertiesoftheactivationfunct ionh(x)ofagivenneuro n(subscri ptiomitted)withlocalpotential xasinEq.(1.1): lim x→−∞h(x) =0 (restingstate,absence ofspikegeneration) h′(x)≥0 (monotonicincrea seoftheexcitation) lim x→+∞h(x) =1 (maximumpossibleﬁringrate), (1.2) whichtakesintoaccountthelimitationsofindividualneuralactivitydiscussed intheprevioussection. Variousactivationortransfer funct ionshavebeensuggestedandconsidered intheliterature.Inthecontextoffeed-fo rwardneuralnetworks,wewilldiscuss severaloptionsinSec. 5.4.Averyimportantclassofplausibleactivationsis givenbyso-calledsigmoidalfunct ions,oneprominent4examplebeing h(x)=1 1+tanh󰀅 γ(x−θ)󰀆󰀖 (1.3) whichclearlysatisﬁes theconditionsgivenabove.The twoimportantparametersarethethresholdθ,whichlocalizesthesteepestincrea seofactivity,and thegainparameterγ,whichquantiﬁestheslope.Itisimportanttonotethat θdoesnotdirectlycorrespondtothepreviouslydiscussed thresholdoftheallor-nothinggenerationofindividualspikes:itmarksthecharacteristicvalueof hatwhichtheactivationfunct ioniscentered. 4Itspopular ityispartlyduetothefactthat therelation tanh′=1−tanh2facilitates a veryeﬃcien tcomputation ofthederivative,seealsoChapter 5. 1.2.FIRINGRATEMODELS 7 θxi=󰁓 jwijSj Si θxi=󰁓 jwijSj Figure1.3:Schema ticillustrationofsymmet rizedactivationfunct ions.Left: Asigmoidaltransfer funct ionwithgainγandthresholdθinthesymmet rized represen tation,cf.Eq.(1.6).Right:ThebinaryMcCul lochPittsactivationas obtainedinthelimitγ→∞. Symme trizedrepresentationofactivity Wewillfrequentlyconsiderasymmet rizeddescr iptionofneuralactivityinterms ofmodiﬁedactivationfunct ions: lim x→−∞g(x) =−1 (restingstate,absence ofspikegeneration) g′(x)≥0 (monotonicincrea seoftheexcitation) lim x→+∞g(x) =1 (maximumpossibleﬁringrate). (1.4) AnexampleactivationanalogoustoEq.(1.3)is g(x)=tanh󰀅 γ(x−θ)󰀄 . (1.5) Atﬁrstsight,thisappearstobejustanalternativeassignmen tofavalueS=−1 totherestingstate. Notethatintheoriginaldescr iptionwith0<Sj<1,aquiescen tneuro n doesnotinﬂuence itspostsynapticneuro nsexplicitly.However,keepingthe formoftheactivationas Si=g(xi)withxi=󰁛 j∈JwijSj (1.6) impliesthattheabsence ofactivity(Sj=−1)inneuronjcannowincrea sethe ﬁringrateofneuroniifconnect edthroughaninhibitorysynapsewij<0.This andothermathema ticalsubtletiesareclearlybiologicallyimplausiblewhichis duetothesomewha tartiﬁcialintroductionof–inasense –negativeandpositive activitieswhicharetreatedinasymmet rizedfashion. However,aswedonotaimatdescr ibingbiologicalreality,theabovediscussed symmet rizationcanbejustiﬁed. Infact,itsimpliﬁesthemathema tical andcomput ationaltreatment,andhascontributedto,forinstance,thefruitful popularizationofneura lnetworksinthestatisticalphysicscommunityinthe 1980sand1990s. 8 1.FROMNEURONSTONETW ORKS McCullochPittsneurons Quitefrequently,anevenmoredrasticmodiﬁcationisconsidered: forinﬁnite gainγ→∞thesigmoidalactivationbecomesastepfunct ion,seeFig.1.3(right panel)foranillustration.Eq.(1.5)forinstanceyieldsinthislimit g(x)=sign(x−θ)=󰀝 +1 ifx≥θ −1 ifx<θ.(1.7) Inthissymmet rizedversionofabinaryactivationfunct ion,onlytwopossible statesareconsidered: eitherthemodelneuronistotallyquiescen t(S=−1)or itﬁresatmaximumfrequency ,whichisrepresen tedbyS=+1. The extreme abstractiontobinaryactivationstateswithouttheﬂexibility ofagradedresponsewasﬁrstdiscussed byMcCul lochandPittsin1943,who originallydenotedthequiescen tstatebyS=0.The persistingpopularityof thismodelisduetoitssimplicityaswellasitssimilaritytoBooleanconcept s inconventionalcomp