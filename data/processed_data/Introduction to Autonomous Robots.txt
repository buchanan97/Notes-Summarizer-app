Nikolaus Correll, Bradley Hayes, Christoffer Heckman, and Alessandro Roncone Introduction to Autonomous Robots: Mechanisms, Sensors, Actuators, and Algorithms v3.0, December 1, 2021 Copyright in this monograph has been licensed exclusively to The MIT Press, http://mitpress.mit.edu , which will be releasing the final version to the public in 2022. All inquiries regarding rights should be addressed to The MIT Press, Rights and Permissions Department. Source code of this book is licensed under a Creative Commons Attribution-NonCommercialNoDerivatives 4.0 International (CC BY-NC-ND 4.0). You are free to share, i.e., copy, distribute and transmit sources under the following conditions: you must attribute the work to its main author, you may not use this work for commercial purposes, and if you remix or modify this work you may not distribute the modified material. For more information, please consult https://creativecommons.org/licenses/by-nc-nd/4.0/ . For Arthur, Tatiana, Benedict and Silvester David Leonardo and Lily future robot users Contents 1. Introduction 19 1.1. Intelligence and embodiment . . . . . . . . . . . . . . . . . . 20 1.2. A roboticists’ problem . . . . . . . . . . . . . . . . . . . . . . 21 1.3. Ratslife: an example of autonomous mobile robotics . . . . . 22 1.4. Autonomous mobile robots: some core challenges . . . . . . . 24 1.5. Autonomous manipulation: some core challenges . . . . . . . 24 I. Mechanisms 29 2. Locomotion, manipulation and their representations 33 2.1. Locomotion and manipulation examples . . . . . . . . . . . . 33 2.2. Static and dynamic stability . . . . . . . . . . . . . . . . . . . 36 2.3. Degrees of freedom . . . . . . . . . . . . . . . . . . . . . . . . 37 2.4. Coordinate Systems and Frames of Reference . . . . . . . . . 42 2.4.1. Matrix notation . . . . . . . . . . . . . . . . . . . . . 43 2.4.2. Mapping from one frame to another . . . . . . . . . . 47 2.4.3. Concatenation of Transformations . . . . . . . . . . . 48 2.4.4. Other representations for orientation . . . . . . . . . . 48 3. Kinematics 55 3.1. Forward Kinematics . . . . . . . . . . . . . . . . . . . . . . . 56 3.1.1. Forward Kinematics of a simple robot arm . . . . . . 57 3.1.2. The Denavit-Hartenberg notation . . . . . . . . . . . . 59 3.2. Inverse Kinematics . . . . . . . . . . . . . . . . . . . . . . . . 62 3.2.1. Solvability . . . . . . . . . . . . . . . . . . . . . . . . . 63 3.2.2. Inverse Kinematics of a Simple Manipulator Arm . . . 63 Contents 3.3. Differential Kinematics . . . . . . . . . . . . . . . . . . . . . . 66 3.3.1. Forward Differential Kinematics . . . . . . . . . . . . 66 3.3.2. Forward Kinematics of a Differential Wheeled Robot . 67 3.3.3. Forward kinematics of Car-like steering . . . . . . . . 74 3.4. Inverse Differential Kinematics . . . . . . . . . . . . . . . . . 75 3.4.1. Inverse Kinematics of Mobile Robots . . . . . . . . . . 77 3.4.2. Feedback Control for Mobile Robots . . . . . . . . . . 80 3.4.3. Under-actuation and Over-actuation . . . . . . . . . . 80 4. Forces 87 4.1. Statics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88 4.2. Kineto-Statics Duality . . . . . . . . . . . . . . . . . . . . . . 90 4.3. Manipulability . . . . . . . . . . . . . . . . . . . . . . . . . . 91 4.3.1. Manipulability Ellipsoid in Velocity space . . . . . . . 91 4.3.2. Manipulability Ellipsoid in Force space . . . . . . . . . 92 4.3.3. Manipulability Considerations . . . . . . . . . . . . . . 94 5. Grasping 97 5.1. The theory of grasping . . . . . . . . . . . . . . . . . . . . . . 97 5.1.1. Friction . . . . . . . . . . . . . . . . . . . . . . . . . . 98 5.1.2. Multiple contacts and deformation . . . . . . . . . . . 100 5.1.3. Suction . . . . . . . . . . . . . . . . . . . . . . . . . . 101 5.2. Simple grasping mechanisms . . . . . . . . . . . . . . . . . . . 102 5.2.1. 1-DoF scissor-like gripper . . . . . . . . . . . . . . . . 102 5.2.2. Parallel jaw . . . . . . . . . . . . . . . . . . . . . . . . 103 5.2.3. 4-bar linkage parallel gripper . . . . . . . . . . . . . . 105 5.2.4. Multi-fingered hands . . . . . . . . . . . . . . . . . . . 105 II. Sensing and actuation 109 6. Actuators 113 6.1. Electric motors . . . . . . . . . . . . . . . . . . . . . . . . . . 113 6.1.1. AC and DC motors . . . . . . . . . . . . . . . . . . . . 114 6.1.2. Stepper motor . . . . . . . . . . . . . . . . . . . . . . 116 6.1.3. Brushless DC motor . . . . . . . . . . . . . . . . . . . 117 6.1.4. Servo motor . . . . . . . . . . . . . . . . . . . . . . . . 117 6.1.5. Motor controllers . . . . . . . . . . . . . . . . . . . . . 118 Contents 6.2. Hydraulic and pneumatic actuators . . . . . . . . . . . . . . . 119 6.2.1. Hydraulic actuators . . . . . . . . . . . . . . . . . . . 119 6.2.2. Pneumatic actuators and soft robotics . . . . . . . . . 119 6.3. Safety considerations . . . . . . . . . . . . . . . . . . . . . . . 120 7. Sensors 125 7.1. Terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127 7.1.1. Proprioception vs. Exteroception . . . . . . . . . . . . 128 7.2. Sensors that measure the robot’s joint configuration . . . . . 129 7.3. Sensors that measure ego-motion . . . . . . . . . . . . . . . . 130 7.3.1. Accelerometers . . . . . . . . . . . . . . . . . . . . . . 130 7.3.2. Gyroscopes . . . . . . . . . . . . . . . . . . . . . . . . 130 7.4. Measuring force . . . . . . . . . . . . . . . . . . . . . . . . . . 132 7.4.1. Measuring pressure or touch . . . . . . . . . . . . . . . 133 7.5. Sensors to measure distance . . . . . . . . . . . . . . . . . . . 135 7.5.1. Reflection . . . . . . . . . . . . . . . . . . . . . . . . . 135 7.5.2. Phase shift . . . . . . . . . . . . . . . . . . . . . . . . 135 7.5.3. Time-of-flight . . . . . . . . . . . . . . . . . . . . . . . 137 7.6. Sensors to sense global pose . . . . . . . . . . . . . . . . . . . 138 III. Computation 143 8. Vision 147 8.1. Images as two-dimensional signals . . . . . . . . . . . . . . . 147 8.2. From signals to information . . . . . . . . . . . . . . . . . . . 149 8.3. Basic image operations . . . . . . . . . . . . . . . . . . . . . . 152 8.3.1. Threshold-based operations . . . . . . . . . . . . . . . 152 8.3.2. Convolution-based filters . . . . . . . . . . . . . . . . . 152 8.3.3. Morphological Operations . . . . . . . . . . . . . . . . 155 8.4. Extracting Structure from Vision . . . . . . . . . . . . . . . . 156 8.5. Computer Vision and Machine Learning . . . . . . . . . . . . 159 9. Feature extraction 163 9.1. Feature detection as an information-reduction problem . . . . 163 9.2. Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164 9.3. Line recognition . . . . . . . . . . . . . . . . . . . . . . . . . . 165 9.3.1. Line fitting using least squares . . . . . . . . . . . . . 166 Contents 9.3.2. Split-and-merge algorithm . . . . . . . . . . . . . . . . 167 9.3.3. RANSAC: Random Sample and Consensus . . . . . . 168 9.3.4. The Hough transform . . . . . . . . . . . . . . . . . . 169 9.4. Scale-invariant feature transforms . . . . . . . . . . . . . . . . 170 9.4.1. Overview . . . . . . . . . . . . . . . . . . . . . . . . . 170 9.4.2. Object Recognition using scale-invariant features . . . 172 9.5. Feature detection and machine learning . . . . . . . . . . . . 173 10.Artificial Neural Networks 177 10.1. The simple Perceptron . . . . . . . . . . . . . . . . . . . . . . 178 10.1.1. Geometric interpretation of the simple perceptron . . 179 10.1.2. Training the simple perceptron . . . . . . . . . . . . . 180 10.2. Activation Functions . . . . . . . . . . . . . . . . . . . . . . . 181 10.3. From the simple perceptron to Multi-layer neural networks . 183 10.3.1. Formal description of Artificial Neural Networks . . . 184 10.3.2. Training a multi-layer neural network . . . . . . . . . 185 10.4. From single outputs to higher dimensional data . . . . . . . . 186 10.5. Objective functions and optimization . . . . . . . . . . . . . . 188 10.5.1. Loss functions for regression tasks . . . . . . . . . . . 188 10.5.2. Loss functions for classification tasks . . . . . . . . . . 189 10.5.3. Binary and Categorical cross-entropy . . . . . . . . . . 190 10.6. Convolutional Neural Networks . . . . . . . . . . . . . . . . . 191 10.6.1. From convolutions to 2D neural networks . . . . . . . 193 10.6.2. Padding and striding . . . . . . . . . . . . . . . . . . . 193 10.6.3. Pooling . . . . . . . . . . . . . . . . . . . . . . . . . . 194 10.6.4. Flattening . . . . . . . . . . . . . . . . . . . . . . . . . 195 10.6.5. A sample CNN . . . . . . . . . . . . . . . . . . . . . . 195 10.6.6. Convolutional Networks beyond 2D image data . . . . 195 10.7. Recurrent Neural Networks . . . . . . . . . . . . . . . . . . . 196 11.Task execution 201 11.1. Reactive control . . . . . . . . . . . . . . . . . . . . . . . . . 201 11.1.1. Limitations of reactive control . . . . . . . . . . . . . 204 11.2. Finite State Machines . . . . . . . . . . . . . . . . . . . . . . 204 11.2.1. Implementation . . . . . . . . . . . . . . . . . . . . . . 207 11.3. Hierarchical Finite State Machines . . . . . . . . . . . . . . . 208 11.3.1. Implementation . . . . . . . . . . . . . . . . . . . . . . 209 Contents 11.4. Behavior Trees . . . . . . . . . . . . . . . . . . . . . . . . . . 209 11.4.1. Node Definition and Status . . . . . . . . . . . . . . . 210 11.4.2. Node Types . . . . . . . . . . . . . . . . . . . . . . . . 211 11.4.3. Behavior Tree Execution . . . . . . . . . . . . . . . . . 212 11.4.4. Implementation . . . . . . . . . . . . . . . . . . . . . . 213 11.5. Mission Planning . . . . . . . . . . . . . . . . . . . . . . . . . 214 11.5.1. The General Problem Solver and STRIPS . . . . . . . 214 12.Mapping 221 12.1. Map representations . . . . . . . . . . . . . . . . . . . . . . . 223 12.2. Iterative Closest Point for Sparse Mapping . . . . . . . . . . . 224 12.3. Octomap: dense mapping of voxels . . . . . . . . . . . . . . . 227 12.4. RGB-D mapping: dense mapping of surfaces . . . . . . . . . 228 13.Path Planning 233 13.1. The configuration space . . . . . . . . . . . . . . . . . . . . . 234 13.2. Graph-based planning algorithms . . . . . . . . . . . . . . . . 234 13.2.1. Dijkstra’s algorithm . . . . . . . . . . . . . . . . . . . 235 13.2.2. A* . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237 13.3. Sampling-based path planning . . . . . . . . . . . . . . . . . . 238 13.3.1. Rapidly Exploring Random Trees . . . . . . . . . . . . 239 13.4. Planning at different length scales . . . . . . . . . . . . . . . 243 13.5. Coverage path planning . . . . . . . . . . . . . . . . . . . . . 245 13.6. Summary and Outlook . . . . . . . . . . . . . . . . . . . . . . 245 14.Manipulation 251 14.1. Non-Prehensile Manipulation . . . . . . . . . . . . . . . . . . 252 14.2. Choosing the right grasp . . . . . . . . . . . . . . . . . . . . . 252 14.2.1. Finding good grasps for simple grippers . . . . . . . . 253 14.2.2. Finding good grasps for multi-fingered hands . . . . . 256 14.3. Pick and place . . . . . . . . . . . . . . . . . . . . . . . . . . 257 14.4. Peg-in-hole problems . . . . . . . . . . . . . . . . . . . . . . . 258 IV. Uncertainty 265 15.Uncertainty and Error Propagation 269 15.1. Uncertainty in Robotics as Random Variable . . . . . . . . . 270 Contents 15.2. Error Propagation . . . . . . . . . . . . . . . . . . . . . . . . 270 15.2.1. Example: Line Fitting . . . . . . . . . . . . . . . . . . 273 15.2.2. Example: Odometry . . . . . . . . . . . . . . . . . . . 274 15.3. Optimal Sensor Fusion . . . . . . . . . . . . . . . . . . . . . . 276 15.3.1. The Kalman Filter . . . . . . . . . . . . . . . . . . . . 277 15.4. Take-home lessons . . . . . . . . . . . . . . . . . . . . . . . . 278 16.Localization 281 16.1. Motivating Example . . . . . . . . . . . . . . . . . . . . . . . 282 16.2. Markov Localization . . . . . . . . . . . . . . . . . . . . . . . 283 16.2.1. Perception Update . . . . . . . . . . . . . . . . . . . . 283 16.2.2. Action Update . . . . . . . . . . . . . . . . . . . . . . 285 16.2.3. Example: Markov Localization on a Topological Map 286 16.3. The Bayes Filter . . . . . . . . . . . . . . . . . . . . . . . . . 289 16.3.1. Example: Bayes filter on a grid . . . . . . . . . . . . . 291 16.4. Particle Filter . . . . . . . . . . . . . . . . . . . . . . . . . . . 293 16.5. Extended Kalman Filter . . . . . . . . . . . . . . . . . . . . . 296 16.5.1. Odometry using the Kalman Filter . . . . . . . . . . . 297 16.6. Summary: Probabilistic Map based localization . . . . . . . . 299 17.Simultaneous Localization and Mapping 303 17.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304 17.1.1. Landmarks . . . . . . . . . . . . . . . . . . . . . . . . 304 17.1.2. Special Case I: one landmark . . . . . . . . . . . . . . 304 17.1.3. Special Case II: two landmarks . . . . . . . . . . . . . 305 17.2. The Covariance Matrix . . . . . . . . . . . . . . . . . . . . . . 306 17.3. EKF SLAM . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307 17.3.1. Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . 307 17.3.2. Multiple Sensors . . . . . . . . . . . . . . . . . . . . . 309 17.4. Graph-based SLAM . . . . . . . . . . . . . . . . . . . . . . . 310 17.4.1. SLAM as a Maximum-Likelihood Estimation Problem 311 17.4.2. Numerical Techniques for Graph-based SLAM . . . . 314 V. Appendices 319 A. Trigonometry 323 A.1. Inverse trigonometry . . . . . . . . . . . . . . . . . . . . . . . 324 Contents A.2. Trigonometric identities . . . . . . . . . . . . . . . . . . . . . 324 B. Linear Algebra 327 B.1. Dot product . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327 B.2. Cross product . . . . . . . . . . . . . . . . . . . . . . . . . . . 327 B.3. Matrix product . . . . . . . . . . . . . . . . . . . . . . . . . . 328 B.4. Matrix inversion . . . . . . . . . . . . . . . . . . . . . . . . . 328 B.5. Principal Component Analysis . . . . . . . . . . . . . . . . . 329 C. Statistics 333 C.1. Random Variables and Probability Distributions . . . . . . . 333 C.1.1. The Normal Distribution . . . . . . . . . . . . . . . . 334 C.1.2. Normal distribution in two dimensions . . . . . . . . . 335 C.2. Conditional Probabilities and Bayes Rule . . . . . . . . . . . 335 C.3. Sum of two random processes . . . . . . . . . . . . . . . . . . 336 C.4. Linear Combinations of Independent Gaussian Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336 C.5. Testing Statistical Significance . . . . . . . . . . . . . . . . . 337 C.5.1. Null Hypothesis on Distributions . . . . . . . . . . . . 337 C.5.2. Testing whether two distributions are independent . . 338 C.5.3. Statistical Significance of True-False Tests . . . . . . . 339 C.5.4. Summary . . . . . . . . . . . . . . . . . . . . . . . . . 340 D. Backpropagation 343 D.1. Backward propagation of error . . . . . . . . . . . . . . . . . 345 D.2. Backpropagation algorithm . . . . . . . . . . . . . . . . . . . 347 E. How to write a research paper 349 E.1. Original . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349 E.2. Hypothesis: Or, what do we learn from this work? . . . . . . 351 E.3. Survey and Tutorial . . . . . . . . . . . . . . . . . . . . . . . 352 E.4. Writing it up! . . . . . . . . . . . . . . . . . . . . . . . . . . . 352 F. Sample curricula 355 F.1. An introduction to autonomous mobile robots . . . . . . . . . 355 F.1.1. Overview . . . . . . . . . . . . . . . . . . . . . . . . . 355 F.1.2. Content . . . . . . . . . . . . . . . . . . . . . . . . . . 356 F.1.3. Implementation suggestions . . . . . . . . . . . . . . . 357 Contents F.2. An introduction to robotic manipulation . . . . . . . . . . . . 358 F.2.1. Overview . . . . . . . . . . . . . . . . . . . . . . . . . 358 F.2.2. Content . . . . . . . . . . . . . . . . . . . . . . . . . . 359 F.2.3. Implementation suggestions . . . . . . . . . . . . . . . 359 F.3. An introduction to robotic systems . . . . . . . . . . . . . . . 360 F.3.1. Overview . . . . . . . . . . . . . . . . . . . . . . . . . 360 F.3.2. Content . . . . . . . . . . . . . . . . . . . . . . . . . . 361 F.3.3. Implementation suggestions . . . . . . . . . . . . . . . 361 F.4. Class debates . . . . . . . . . . . . . . . . . . . . . . . . . . . 361 Preface This book provides an algorithmic perspective to autonomous robotics to students with a sophomore-level of linear algebra and probability theory. Robotics is an emerging field at the intersection of mechanical engineering, electrical engineering, and computer science. With computers becoming more powerful, making robots smart is getting more and more into the focus of attention and robotics research most challenging frontier. While there is a large number of textbooks on the mechanics and dynamics of robots available to sophomore-level undergraduates, books that provide a broad algorithmic perspective are mostly limited to the graduate level. This book has therefore been developed not to create “yet another textbook, but better than the others”, but to allow us to teach robotics to the 3rd and 4th year undergraduates at the Department of Computer Science at the University of Colorado. Although falling under the umbrella of “Artificial Intelligence”, standard AI techniques are not sufficient to tackle problems that involve uncertainty, such as a robot’s interaction in the real world. This book uses simple trigonometry to develop the kinematic equations of manipulators and mobile robots, then introduces path planning, sensing, and lastly uncertainty. The robot localization problem is introduced by formally defining error propagation, which leads to Markov localization, Particle filtering and finally the Extended Kalman Filter, and Simultaneous Localization and Mapping. Instead of focusing on state-of-the-art solutions to a particular sub-problem, emphasis of the book is on a progressive step-by-step development concepts through recurrent examples that capture the essence of a problem. The described solutions might not necessarily be the best, however they are easy to comprehend and widely used in the community. For example, odometry and line-fitting are used to explain forward kinematics and least-squares solutions, respectively, and later serve as motivating examples for error prop- Contents agation and the Kalman filter in a localization context. Notably, the book is explicitly robot-agnostic, reflecting the timeliness of fundamental concepts. Rather, a series of possible project-based curricula are described in an Appendix and available online, ranging from a maze-solving competition that can be realized with most camera-equipped differential-wheel robots to manipulation experiments with a robotic arm, all of which can be entirely conducted in simulation to teach most of the core concepts. After multiple years of development and distribution mainly via Github, this new edition of the book has been co-authored by my colleagues in the Computer Science department, Bradley Hayes, Christoffer Heckman, and Alessandro Roncone, each having thaught multiple iterations of our “Introduction to Robotics” and “Advanced Robotics” courses as well as special topics courses that pertain to their sub-fields of robotics research. They are adding not only tremendous technical depth, but also years of experience on how certain subjects should be taught to remain engaging and exciting. This book is released under a Creative Commons CC BY-NC-ND 4.0 International license, which allows anyone to copy and share its source code. However, neither the compiled version nor the code shall be used to create derivatives for commercial purposes. We have chosen this format as it seems to maintain the best trade-off between a freely available textbook resource that others may contribute to and maintaining a consistent curriculum that others can refer to. We are incredibly grateful to MIT Press and our editor Elizabeth Swayne to support this forward-looking model. Writing this book would not have been possible without the excellent work of others before us, most notably “Introduction to Robotics: Mechanics and Control” by John Craig and “Introduction to Autonomous Mobile Robots” by Roland Siegwart, Illah Nourbakhsh and Davide Scaramuzza, and innumerable other books and websites from which I learned and borrowed examples and notation. We are also grateful to Brian Amberg, Aaron Becker, Bachir El-Kadir, James Grime, Michael Sambol, Cyrill Stachniss, Subh83, Ethan Tiran-Thompson who made lecture video snippets and animations available online, and which are referenced throughout the book using QR codes. I would like to acknowledge Mike Miles and Harel Biggie, graduate students in the authors’ shared laboratory at the University of Colorado Boulder, for their careful reading and contributions. Finally, I would also like to Contents acknowledge Github users AlWiVo, beardicus, mguida22, aokeson, as1ndu, apnorton, JohnAllen, jmodares, countsoduku, choffmann, and chrstphrdlz for their pull requests and comments as well as Haluk Bayram. Your interest and motivation in this project has been one of our biggest rewards. Nikolaus Correll Boulder, Colorado, December 1, 2021 Chapter 1 Introduction Robotics celebrated its 60th birthday in 2021, dating back to the first commercial robot in 1961 (the Unimate). In a “Tonight Show” at the time, this robot did amazing things: it opened a bottle of beer, poured it, put a golf ball into a hole, and even conducted an orchestra. This robot did all of the things we expect a good robot to do: it was dexterous, accurate, and even creative. Since this robot’s appearance on the Tonight show, more than 60 years have passed—so how incredible must the capabilities of today’s robots be and what must they be able to do? Interestingly, we only recently developed the techniques to autnomously do all of the things demonstrated by the Unimate. Unimate indeed did what was shown on TV, but all of its motions were preprogrammed and the environment was carefully staged. Only the advent of cheap and powerful sensors and the surge in computation capabilities have recently enabled robots to detect objects by themselves, plan motions to reach for them, and ultimately grasp and manipulate. Yet, robotics is still far away from doing these tasks with human-like performance. This book introduces you to the computational fundamentals behind the design and control of autonomous robots. Robots are considered to be autonomous when they make decisions in response to their environment (rather than simply following a pre-programmed set of motions). They achieve this using a multitude of modern techniques ranging from signal processing, control theory, artificial intelligence, and more. These techniques are tightly intertwined with the mechanics, the sensors, and the actuators of the robot. Designing a robot therefore requires a deep understanding of both algorithms 1. Introduction Figure 1.1. A wind-up toy that does not fall off the table using purely mechanical control. A fly-wheel that turns orthogonal to the robot’s motion induces a right turn as soon as it hits the ground once the front caster wheel goes off the edge. and its interfaces to the physical world. The goals of this introductory chapter are to introduce the kind of problems roboticists deal with and how they solve it. 1.1. Intelligence and embodiment Our notion of “intelligent behavior” is strongly biased by our understanding of the brain and how computers work: intelligence is located in our heads. In fact, however, a lot of behavior that looks intelligent can be achieved by very simple mechanisms. For example, mechanical wind-up toys can avoid falling off an edge simply by using a fly-wheel that rotates at a right angle to their direction of motion and a caster wheel. Once the caster wheel loses contact with the ground—that is, when the robot has reached the edge—the fly-wheel kicks in and pulls the robot to the right (Figure 1.1). A robot vacuum cleaner might solve the same problem very differently: it employs infrared sensors that are pointed downwards to detect edges such as those found on stairs and issues a command to make an avoiding turn in response. Given that on-board electronics is needed, this is a much more efficient, albeit more complex, approach. Even though the above examples provide different approaches to implement intelligent behaviors, similar trade-offs exist for robotic planning. For example, ants can find the shortest path between their nest and a food source by simply choosing the trail that already has more pheromones (the chemicals ants communicate with) on it. As shorter paths have ants not only moving faster towards the food, but also returning faster, their pheromone trails build up quicker (Figure 1.2). But ants are not stuck to this solution. 1.2. A roboticists’ problem Figure 1.2. Ants finding the shortest path from their nest (bottom) to a food source (top). From left to right: The ants initially have equal preference for the left and the right branch, both going back and forth. As ants return faster on the shorter branch there will be more pheromones present on the short branch once a new ant arrives from the nest. Every now and then, ants give the longer path another shot, eventually finding new food sources. What looks like intelligent behavior at the swarm level, is essentially achieved by a pheromone sensor that occasionally fails. A modern industrial robot would solve the problem completely differently: it would first acquire some representation of the environment in the form of a map populated with obstacles, and then plan a path using an algorithm. Which solution to achieve a certain desired behavior is best depends on the resources that are available to the designer. We will now study a more elaborate problem for which many, more or less efficient, solutions exist. 1.2. A roboticists’ problem Imagine the following scenario. You are a robot in a maze-like environment such as a cluttered warehouse, hospital, or office building. There is a chest full of gold coins hidden somewhere inside. Unfortunately, you don’t have a map of the maze. In case you find the chest, you may only take a couple of coins at a time, and bring them to the exit door where your car is parked. Think about a strategy that will allow you to harvest as many coins in the shortest time as possible. Think about the cognitive and perception capabilities you would use. Now discuss alternative strategies: if you could not use these capabilities, what would you do? I.e., what if you were blind, or had no memory of the past? These are exactly the same problems a robot has. A robot is a mobile 1. Introduction machine that may reason about its environment with sensors and computation. Current robots are far from possessing the capabilities humans have, therefore it is worth considering what strategies youwould employ to solve a problem if youwere to lack some important perception or computational capabilities. Before we move forward to discuss potential strategies for robots with impeded sensory systems, let’s rely on a little bit on what we know from studying algorithms and briefly consider a particular strategy. You will need to explore the maze without entering any branch twice. You can use a technique known as depth-first search to do this, but will need to be able to not only map the environment, but also localize in it, e.g., by recognizing places and dead-reckoning on the map. Once you find the gold, you would need to plan the shortest path back to the exit, which you can then use to go back and forth until all the gold is harvested. 1.3. Ratslife: an example of autonomous mobile robotics Ratslife is a miniature robot maze competition developed by Olivier Michel from Cyberbotics S.A., which exemplifies a broad range of topics covered in this book. The Ratslife environment can easily be created from LEGO bricks, cardboard or wood and the game can be played with any two mobile robots, preferably ones with the ability to identify markers in the environment. These include simple differential-wheel educational platforms with onboard cameras or even a smartphone driven robot. Figure 1.3 shows an example environment that can be constructed from craft materials and illustrates some practical aspects of mobile robots for competitions. In Ratslife, two miniature robots compete on searching for four “feeders” that are hidden in a maze. Once a robot reaches a feeder, it receives “energy” to go on for another 60 seconds, and the feeder becomes temporarily unavailable. After a short while, the feeder becomes available again. The feeders can be either controlled by a referee who also takes care of time-keeping or constructed as part of a simple curriculum on electronics or mechatronics. It should be clear by now how youmight solve these tasks using your abilities, and you may have also thought about some fall-back strategies in case a sensor or two of yours were unavailable. Here are some possible algorithms for a robot, ordered by increasingly sophisticated capabilities the robot might wield: 1.3. Ratslife: an example of autonomous mobile robotics Figure 1.3. A simple maze made from cardboard, wood, or Lego bricks with one or more charging stations. Locations in the maze are marked with unique markers that can be recognized by a simple robot. •Imagine you have a robot that can only drive (actuation) and bounce off a wall. The resulting random walk will eventually let the robot reach a feeder. As the allowed time to do so is limited, it is likely that the robot’s energy will soon deplete. •Now imagine a robot that has a sensor that gives it the ability to estimate its distance from a wall. This could be a whisker, an infrared distance sensor, an ultrasound distance sensor, or a laser range finder. The robot could now use this sensor to keep following a wall to its right. Using this strategy for solving the maze, it will eventually explore the