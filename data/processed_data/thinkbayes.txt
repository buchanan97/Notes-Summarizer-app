Think Ba y es Ba y esian Statistics Made Simple V ersion 1.0.9 Think Ba y es Ba y esian Statistics Made Simple V ersion 1.0.9 Allen B. Do wney Green T ea Press Needham, Massac h usetts Cop yrigh t Â© 2012 Allen B. Do wney . Green T ea Press 9 W ash burn A v e Needham MA 02492 P ermission is gran ted to cop y , distribute, and/or mo dify this do cumen t under the terms of the Creativ e Commons A ttribution-NonCommercial-ShareAlik e 4.0 In ternational License, whic h is a v ailable at http://creativecommons. org/licenses/by- nc- sa/4.0/ . The LA T E X source for this b o ok is a v ailable from http://greenteapress.com/ thinkbayes . Preface 0.1 My theory , whic h is mine The premise of this b o ok, and the other b o oks in the Think X series, is that if y ou kno w ho w to program, y ou can use that skill to learn other topics. Most b o oks on Ba y esian statistics use mathematical notation and presen t ideas in terms of mathematical concepts lik e calculus. This b o ok uses Python co de instead of math, and discrete appro ximations instead of con tin uous mathematics. As a result, what w ould b e an in tegral in a math b o ok b ecomes a summation, and most op erations on probabilit y distributions are simple lo ops. I think this presen tation is easier to understand, at least for p eople with programming skills. It is also more general, b ecause when w e mak e mo deling decisions, w e can c ho ose the most appropriate mo del without w orrying to o m uc h ab out whether the mo del lends itself to con v en tional analysis. Also, it pro vides a smo oth dev elopmen t path from simple examples to realw orld problems. Chapter 3 is a go o d example. It starts with a simple example in v olving dice, one of the staples of basic probabilit y . F rom there it pro ceeds in small steps to the lo comotiv e problem, whic h I b orro w ed from Mosteller's Fifty Chal lenging Pr oblems in Pr ob ability with Solutions , and from there to the German tank problem, a famously successful application of Ba y esian metho ds during W orld W ar I I. 0.2 Mo deling and appro ximation Most c hapters in this b o ok are motiv ated b y a real-w orld problem, so they in v olv e some degree of mo deling. Before w e can apply Ba y esian metho ds (or an y other analysis), w e ha v e to mak e decisions ab out whic h parts of the realw orld system to include in the mo del and whic h details w e can abstract a w a y . vi Chapter 0. Preface F or example, in Chapter 7, the motiv ating problem is to predict the winner of a ho c k ey game. I mo del goal-scoring as a P oisson pro cess, whic h implies that a goal is equally lik ely at an y p oin t in the game. That is not exactly true, but it is probably a go o d enough mo del for most purp oses. In Chapter 12 the motiv ating problem is in terpreting SA T scores (the SA T is a standardized test used for college admissions in the United States). I start with a simple mo del that assumes that all SA T questions are equally di cult, but in fact the designers of the SA T delib erately include some questions that are relativ ely easy and some that are relativ ely hard. I presen t a second mo del that accoun ts for this asp ect of the design, and sho w that it do esn't ha v e a big eect on the results after all. I think it is imp ortan t to include mo deling as an explicit part of problem solving b ecause it reminds us to think ab out mo deling errors (that is, errors due to simpli cations and assumptions of the mo del). Man y of the metho ds in this b o ok are based on discrete distributions, whic h mak es some p eople w orry ab out n umerical errors. But for real-w orld problems, n umerical errors are almost alw a ys smaller than mo deling errors. F urthermore, the discrete approac h often allo ws b etter mo deling decisions, and I w ould rather ha v e an appro ximate solution to a go o d mo del than an exact solution to a bad mo del. On the other hand, con tin uous metho ds sometimes yield p erformance adv an tagesfor example b y replacing a linear- or quadratic-time computation with a constan t-time solution. So I recommend a general pro cess with these steps: 1. While y ou are exploring a problem, start with simple mo dels and implemen t them in co de that is clear, readable, and demonstrably correct. F o cus y our atten tion on go o d mo deling decisions, not optimization. 2. Once y ou ha v e a simple mo del w orking, iden tify the biggest sources of error. Y ou migh t need to increase the n um b er of v alues in a discrete appro ximation, or increase the n um b er of iterations in a Mon te Carlo sim ulation, or add details to the mo del. 3. If the p erformance of y our solution is go o d enough for y our application, y ou migh t not ha v e to do an y optimization. But if y ou do, there are t w o approac hes to consider. Y ou can review y our co de and lo ok for optimizations; for example, if y ou cac he previously computed results y ou migh t b e able to a v oid redundan t computation. Or y ou can lo ok for analytic metho ds that yield computational shortcuts. 0.3. W orking with the co de vii One b ene t of this pro cess is that Steps 1 and 2 tend to b e fast, so y ou can explore sev eral alternativ e mo dels b efore in v esting hea vily in an y of them. Another b ene t is that if y ou get to Step 3, y ou will b e starting with a reference implemen tation that is lik ely to b e correct, whic h y ou can use for regression testing (that is, c hec king that the optimized co de yields the same results, at least appro ximately). 0.3 W orking with the co de The co de and sound samples used in this b o ok are a v ailable from https:// github.com/AllenDowney/ThinkBayes . Git is a v ersion con trol system that allo ws y ou to k eep trac k of the les that mak e up a pro ject. A collection of les under Git's con trol is called a rep ository. GitHub is a hosting service that pro vides storage for Git rep ositories and a con v enien t w eb in terface. The GitHub homepage for m y rep ository pro vides sev eral w a ys to w ork with the co de: Âˆ Y ou can create a cop y of m y rep ository on GitHub b y pressing the F o rk button. If y ou don't already ha v e a GitHub accoun t, y ou'll need to create one. After forking, y ou'll ha v e y our o wn rep ository on GitHub that y ou can use to k eep trac k of co de y ou write while w orking on this b o ok. Then y ou can clone the rep o, whic h means that y ou cop y the les to y our computer. Âˆ Or y ou could clone m y rep ository . Y ou don't need a GitHub accoun t to do this, but y ou w on't b e able to write y our c hanges bac k to GitHub. Âˆ If y ou don't w an t to use Git at all, y ou can do wnload the les in a Zip le using the button in the lo w er-righ t corner of the GitHub page. The co de for the rst edition of the b o ok w orks with Python 2. If y ou are using Python 3, y ou migh t w an t to use the up dated co de in https://github. com/AllenDowney/ThinkBayes2 instead. I dev elop ed this b o ok using Anaconda from Con tin uum Analytics, whic h is a free Python distribution that includes all the pac k ages y ou'll need to run the co de (and lots more). I found Anaconda easy to install. By default it do es a user-lev el installation, not system-lev el, so y ou don't need administrativ e privileges. Y ou can do wnload Anaconda from http://continuum.io/downloads . If y ou don't w an t to use Anaconda, y ou will need the follo wing pac k ages: viii Chapter 0. Preface Âˆ NumPy for basic n umerical computation, http://www.numpy.org/ ; Âˆ SciPy for scien ti c computation, http://www.scipy.org/ ; Âˆ matplotlib for visualization, http://matplotlib.org/ . Although these are commonly used pac k ages, they are not included with all Python installations, and they can b e hard to install in some en vironmen ts. If y ou ha v e trouble installing them, I recommend using Anaconda or one of the other Python distributions that include these pac k ages. Man y of the examples in this b o ok use classes and functions de ned in thinkbayes.py . Some of them also use thinkplot.py , whic h pro vides wrapp ers for some of the functions in pyplot , whic h is part of matplotlib . 0.4 Co de st yle Exp erienced Python programmers will notice that the co de in this b o ok do es not comply with PEP 8, whic h is the most common st yle guide for Python ( http://www.python.org/dev/peps/pep- 0008/ ). Sp eci cally , PEP 8 calls for lo w ercase function names with underscores b et w een w ords, like_this . In this b o ok and the accompan ying co de, function and metho d names b egin with a capital letter and use camel case, LikeThis . I brok e this rule b ecause I dev elop ed some of the co de while I w as a Visiting Scien tist at Go ogle, so I follo w ed the Go ogle st yle guide, whic h deviates from PEP 8 in a few places. Once I got used to Go ogle st yle, I found that I lik ed it. And at this p oin t, it w ould b e to o m uc h trouble to c hange. Also on the topic of st yle, I write Ba y es's theorem with an s after the ap ostrophe, whic h is preferred in some st yle guides and deprecated in others. I don't ha v e a strong preference. I had to c ho ose one, and this is the one I c hose. And nally one t yp ographical note: throughout the b o ok, I use PMF and CDF for the mathematical concept of a probabilit y mass function or cum ulativ e distribution function, and Pmf and Cdf to refer to the Python ob jects I use to represen t them. 0.5 Prerequisites There are sev eral excellen t mo dules for doing Ba y esian statistics in Python, including pymc and Op enBUGS. I c hose not to use them for this b o ok b ecause 0.5. Prerequisites ix y ou need a fair amoun t of bac kground kno wledge to get started with these mo dules, and I w an t to k eep the prerequisites minimal. If y ou kno w Python and a little bit ab out probabilit y , y ou are ready to start this b o ok. Chapter 1 is ab out probabilit y and Ba y es's theorem; it has no co de. Chapter 2 in tro duces Pmf , a thinly disguised Python dictionary I use to represen t a probabilit y mass function (PMF). Then Chapter 3 in tro duces Suite , a kind of Pmf that pro vides a framew ork for doing Ba y esian up dates. In some of the later c hapters, I use analytic distributions including the Gaussian (normal) distribution, the exp onen tial and P oisson distributions, and the b eta distribution. In Chapter 15 I break out the less-common Diric hlet distribution, but I explain it as I go along. If y ou are not familiar with these distributions, y ou can read ab out them on Wikip edia. Y ou could also read the companion to this b o ok, Think Stats , or an in tro ductory statistics b o ok (although I'm afraid most of them tak e a mathematical approac h that is not particularly helpful for practical purp oses). Con tributor List If y ou ha v e a suggestion or correction, please send email to downey@al lendowney.c om . If I mak e a c hange based on y our feedbac k, I will add y ou to the con tributor list (unless y ou ask to b e omitted). If y ou include at least part of the sen tence the error app ears in, that mak es it easy for me to searc h. P age and section n um b ers are ne, to o, but not as easy to w ork with. Thanks! Âˆ First, I ha v e to ac kno wledge Da vid MacKa y's excellen t b o ok, Information The ory, Infer enc e, and L e arning A lgorithms , whic h is where I rst came to understand Ba y esian metho ds. With his p ermission, I use sev eral problems from his b o ok as examples. Âˆ This b o ok also b ene ted from m y in teractions with Sanjo y Maha jan, esp ecially in fall 2012, when I audited his class on Ba y esian Inference at Olin College. Âˆ I wrote parts of this b o ok during pro ject nigh ts with the Boston Python User Group, so I w ould lik e to thank them for their compan y and pizza. Âˆ Olivier Yiptong sen t sev eral helpful suggestions. Âˆ Y uriy P asic hn yk found sev eral errors. Âˆ Kristopher Ov erholt sen t a long list of corrections and suggestions. Âˆ Max Hailp erin suggested a clari cation in Chapter 1. x Chapter 0. Preface Âˆ Markus Dobler p oin ted out that dra wing co okies from a b o wl with replacemen t is an unrealistic scenario. Âˆ In spring 2013, studen ts in m y class, Computational Ba y esian Statistics, made man y helpful corrections and suggestions: Kai Austin, Claire Barnes, Kari Bender, Rac hel Bo y , Kat Mendoza, Arjun Iy er, Ben Kro op, Nathan Lin tz, Kyle McConnaugha y , Alec Radford, Brendan Ritter, and Ev an Simpson. Âˆ Greg Marra and Matt Aasted help ed me clarify the discussion of The Pric e is R ight problem. Âˆ Marcus Ogren p oin ted out that the original statemen t of the lo comotiv e problem w as am biguous. Âˆ Jasmine K wit yn and Dan F auxsmith at O'Reilly Media pro ofread the b o ok and found man y opp ortunities for impro v emen t. Âˆ Linda P escatore found a t yp o and made some helpful suggestions. Âˆ T omasz MiÂ¡sk o sen t man y excellen t corrections and suggestions. Other p eople who sp otted t yp os and small errors include T om P ollard, P aul A. Giannaros, Jonathan Edw ards, George Purkins, Rob ert Marcus, Ram Lim bu, James La wry , Ben Kahle, Jerey La w, and Alv aro Sanc hez. Contents Preface v 0.1 My theory , whic h is mine . . . . . . . . . . . . . . . . . . . . v 0.2 Mo deling and appro ximation . . . . . . . . . . . . . . . . . . v 0.3 W orking with the co de . . . . . . . . . . . . . . . . . . . . . vii 0.4 Co de st yle . . . . . . . . . . . . . . . . . . . . . . . . . . . . viii 0.5 Prerequisites . . . . . . . . . . . . . . . . . . . . . . . . . . . viii 1 Ba y es's Theorem 1 1.1 Conditional probabilit y . . . . . . . . . . . . . . . . . . . . . 1 1.2 Conjoin t probabilit y . . . . . . . . . . . . . . . . . . . . . . . 2 1.3 The co okie problem . . . . . . . . . . . . . . . . . . . . . . . 3 1.4 Ba y es's theorem . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.5 The diac hronic in terpretation . . . . . . . . . . . . . . . . . 5 1.6 The M&M problem . . . . . . . . . . . . . . . . . . . . . . . 6 1.7 The Mon t y Hall problem . . . . . . . . . . . . . . . . . . . . 8 1.8 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 2 Computational Statistics 11 2.1 Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 2.2 The co okie problem . . . . . . . . . . . . . . . . . . . . . . . 12 2.3 The Ba y esian framew ork . . . . . . . . . . . . . . . . . . . . 13 xii Con ten ts 2.4 The Mon t y Hall problem . . . . . . . . . . . . . . . . . . . . 15 2.5 Encapsulating the framew ork . . . . . . . . . . . . . . . . . . 16 2.6 The M&M problem . . . . . . . . . . . . . . . . . . . . . . . 17 2.7 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 2.8 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 3 Estimation 21 3.1 The dice problem . . . . . . . . . . . . . . . . . . . . . . . . 21 3.2 The lo comotiv e problem . . . . . . . . . . . . . . . . . . . . 22 3.3 What ab out that prior? . . . . . . . . . . . . . . . . . . . . . 25 3.4 An alternativ e prior . . . . . . . . . . . . . . . . . . . . . . . 25 3.5 Credible in terv als . . . . . . . . . . . . . . . . . . . . . . . . 27 3.6 Cum ulativ e distribution functions . . . . . . . . . . . . . . . 28 3.7 The German tank problem . . . . . . . . . . . . . . . . . . . 29 3.8 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 3.9 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 4 More Estimation 33 4.1 The Euro problem . . . . . . . . . . . . . . . . . . . . . . . . 33 4.2 Summarizing the p osterior . . . . . . . . . . . . . . . . . . . 35 4.3 Sw amping the priors . . . . . . . . . . . . . . . . . . . . . . 37 4.4 Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . 37 4.5 The b eta distribution . . . . . . . . . . . . . . . . . . . . . . 39 4.6 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 4.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 Con ten ts xiii 5 Odds and A ddends 43 5.1 Odds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 5.2 The o dds form of Ba y es's theorem . . . . . . . . . . . . . . . 44 5.3 Oliv er's blo o d . . . . . . . . . . . . . . . . . . . . . . . . . . 45 5.4 A ddends . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 5.5 Maxima . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 5.6 Mixtures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 5.7 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 6 Decision Analysis 55 6.1 The Pric e is R ight problem . . . . . . . . . . . . . . . . . . . 55 6.2 The prior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 6.3 Probabilit y densit y functions . . . . . . . . . . . . . . . . . . 57 6.4 Represen ting PDF s . . . . . . . . . . . . . . . . . . . . . . . 57 6.5 Mo deling the con testan ts . . . . . . . . . . . . . . . . . . . . 60 6.6 Lik eliho o d . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 6.7 Up date . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 6.8 Optimal bidding . . . . . . . . . . . . . . . . . . . . . . . . . 64 6.9 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 7 Prediction 69 7.1 The Boston Bruins problem . . . . . . . . . . . . . . . . . . 69 7.2 P oisson pro cesses . . . . . . . . . . . . . . . . . . . . . . . . 70 7.3 The p osteriors . . . . . . . . . . . . . . . . . . . . . . . . . . 71 7.4 The distribution of goals . . . . . . . . . . . . . . . . . . . . 72 7.5 The probabilit y of winning . . . . . . . . . . . . . . . . . . . 74 7.6 Sudden death . . . . . . . . . . . . . . . . . . . . . . . . . . 75 7.7 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76 7.8 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77 xiv Con ten ts 8 Observ er Bias 79 8.1 The Red Line problem . . . . . . . . . . . . . . . . . . . . . 79 8.2 The mo del . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79 8.3 W ait times . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81 8.4 Predicting w ait times . . . . . . . . . . . . . . . . . . . . . . 84 8.5 Estimating the arriv al rate . . . . . . . . . . . . . . . . . . . 86 8.6 Incorp orating uncertain t y . . . . . . . . . . . . . . . . . . . . 89 8.7 Decision analysis . . . . . . . . . . . . . . . . . . . . . . . . 90 8.8 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92 8.9 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93 9 T w o Dimensions 95 9.1 P ain tball . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95 9.2 The suite . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96 9.3 T rigonometry . . . . . . . . . . . . . . . . . . . . . . . . . . 97 9.4 Lik eliho o d . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99 9.5 Join t distributions . . . . . . . . . . . . . . . . . . . . . . . . 100 9.6 Conditional distributions . . . . . . . . . . . . . . . . . . . . 100 9.7 Credible in terv als . . . . . . . . . . . . . . . . . . . . . . . . 102 9.8 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104 9.9 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105 10 Appro ximate Ba y esian Computation 107 10.1 The V ariabilit y Hyp othesis . . . . . . . . . . . . . . . . . . . 107 10.2 Mean and standard deviation . . . . . . . . . . . . . . . . . 108 10.3 Up date . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110 10.4 The p osterior distribution of CV . . . . . . . . . . . . . . . . 111 10.5 Under o w . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111 Con ten ts xv 10.6 Log-lik eliho o d . . . . . . . . . . . . . . . . . . . . . . . . . . 113 10.7 A little optimization . . . . . . . . . . . . . . . . . . . . . . 114 10.8 ABC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116 10.9 Robust estimation . . . . . . . . . . . . . . . . . . . . . . . . 117 10.10 Who is more v ariable? . . . . . . . . . . . . . . . . . . . . . 120 10.11 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121 10.12 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122 11 Hyp othesis T esting 123 11.1 Bac k to the Euro problem . . . . . . . . . . . . . . . . . . . 123 11.2 Making a fair comparison . . . . . . . . . . . . . . . . . . . . 124 11.3 The triangle prior . . . . . . . . . . . . . . . . . . . . . . . . 126 11.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127 11.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127 12 Evidence 129 12.1 In terpreting SA T scores . . . . . . . . . . . . . . . . . . . . . 129 12.2 The scale . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130 12.3 The prior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130 12.4 P osterior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132 12.5 A b etter mo del . . . . . . . . . . . . . . . . . . . . . . . . . 134 12.6 Calibration . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136 12.7 P osterior distribution of e cacy . . . . . . . . . . . . . . . . 137 12.8 Predictiv e distribution . . . . . . . . . . . . . . . . . . . . . 139 12.9 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140 xvi Con ten ts 13 Sim ulation 143 13.1 The Kidney T umor problem . . . . . . . . . . . . . . . . . . 143 13.2 A simple mo del . . . . . . . . . . . . . . . . . . . . . . . . . 144 13.3 A more general mo del . . . . . . . . . . . . . . . . . . . . . . 146 13.4 Implemen tation . . . . . . . . . . . . . . . . . . . . . . . . . 148 13.5 Cac hing the join t distribution . . . . . . . . . . . . . . . . . 149 13.6 Conditional distributions . . . . . . . . . . . . . . . . . . . . 150 13.7 Serial Correlation . . . . . . . . . . . . . . . . . . . . . . . . 151 13.8 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155 14 A Hierarc hical Mo del 157 14.1 The Geiger coun ter problem . . . . . . . . . . . . . . . . . . 157 14.2 Start simple . . . . . . . . . . . . . . . . . . . . . . . . . . . 158 14.3 Mak e it hierarc hical . . . . . . . . . . . . . . . . . . . . . . . 159 14.4 A little optimization . . . . . . . . . . . . . . . . . . . . . . 160 14.5 Extracting the p osteriors . . . . . . . . . . . . . . . . . . . . 161 14.6 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162 14.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163 15 Dealing with Dimensions 165 15.1 Belly button bacteria . . . . . . . . . . . . . . . . . . . . . . 165 15.2 Lions and tigers and b ears . . . . . . . . . . . . . . . . . . . 166 15.3 The hierarc hical v ersion . . . . . . . . . . . . . . . . . . . . . 168 15.4 Random sampling . . . . . . . . . . . . . . . . . . . . . . . . 170 15.5 Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . 172 15.6 Collapsing the hierarc h y . . . . . . . . . . . . . . . . . . . . 173 15.7 One more problem . . . . . . . . . . . . . . . . . . . . . . . 175 15.8 W e're not done y et . . . . . . . . . . . . . . . . . . . . . . . 177 Con ten ts xvii 15.9 The b elly button data . . . . . . . . . . . . . . . . . . . . . 178 15.10 Predictiv e distributions . . . . . . . . . . . . . . . . . . . . . 181 15.11 Join t p osterior . . . . . . . . . . . . . . . . . . . . . . . . . . 185 15.12 Co v erage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186 15.13 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188 xviii Con ten ts Chapter 1 Bayes's Theorem 1.1 Conditional probabilit y The fundamen tal idea b ehind all Ba y esian statistics is Ba y es's theorem, whic h is surprisingly easy to deriv e, pro vided that y ou understand conditional probabilit y . So w e'll start with probabilit y , then conditional probabilit y , then Ba y es's theorem, and on to Ba y esian statistics. A probabilit y is a n um b er b et w een 0 and 1 (including b oth) that represen ts a degree of b elief in a fact or prediction. The v alue 1 represen ts certain t y that a fact is true, or that a prediction will come true. The v alue 0 represen ts certain t y that the fact is false. In termediate v alues represen t degrees of certain t y . The v alue 0.5, often written as 50%, means that a predicted outcome is as lik ely to happ en as not. F or example, the probabilit y that a tossed coin lands face up is v ery close to 50%. A conditional probabilit y is a probabilit y based on some bac kground information. F or example, I w an t to kno w the probabilit y that I will ha v e a heart attac k in the next y ear. A ccording to the CDC, Ev ery y ear ab out 785,000 Americans ha v e a rst coronary attac k. ( http://www.cdc.gov/heartdisease/ facts.htm ) The U.S. p opulation is ab out 311 million, so the probabilit y that a randomly c hosen American will ha v e a heart attac k in the next y ear is roughly 0.3%. But I am not a randomly c hosen American. Epidemiologists ha v e iden ti ed man y factors that aect the risk of heart attac ks; dep ending on those factors, m y risk migh t b e higher or lo w er than a v erage. 2 Chapter 1. Ba y es's Theorem I am male, 45 y ears old, and I ha v e b orderline high c holesterol. Those factors increase m y c hances. Ho w ev er, I ha v e lo w blo o d pressure and I don't smok e, and those factors decrease m y c hances. Plugging ev erything in to the online calculator at http://cvdrisk.nhlbi. nih.gov/calculator.asp , I nd that m y risk of a heart attac k in the next y ear is ab out 0.2%, less than the national a v erage. That v alue is a conditional probabilit y , b ecause it is based on a n um b er of factors that mak e up m y condition. The usual notation for conditional probabilit y is p(AjB) , whic h is the probabilit y ofA giv en thatB is true. In this example, A represen ts the prediction that I will ha v e a heart attac k in the next y ear, and B is the set of conditions I listed. 1.2 Conjoin t probabilit y Conjoin t probabilit y is a fancy w a y to sa y the probabilit y that t w o things are true. I write p(AandB) to mean the probabilit y that A andB are b oth true. If y ou learned ab out probabilit y in the con text of coin tosses and dice, y ou migh t ha v e learned the follo wing form ula: p(AandB) = p(A) p(B) W ARNING: not alw a ys true F or example, if I toss t w o coins, and A means the rst coin lands face up, and B means the second coin lands face up, then p(A) = p(B) = 0:5 , and sure enough, p(AandB) = p(A) p(B) = 0:25 . But this form ula only w orks b ecause in this case A andB are indep enden t; that is, kno wing the outcome of the rst ev en t do es not c hange the probabilit y of the second. Or, more formally , p(BjA) =p(B) . Here is a dieren t example where the ev en ts are not indep enden t. Supp ose thatA means that it rains to da y and B means that it rains tomorro w. If I kno w that it rained to da y , it is more lik ely that it will rain tomorro w, so p(BjA)>p(B) . In general, the probabilit y of a conjunction is p(AandB) = p(A) p(BjA) for an yA andB . So if the c hance of rain on an y giv en da y is 0.5, the c hance of rain on t w o consecutiv e da ys is not 0.25, but probably a bit higher. 1.3. The co okie problem 3 1.3 The co okie problem W e'll get to Ba y es's theorem so on, but I w an t to motiv ate it with an example called the co okie problem.1Supp ose there are t w o b o wls of co okies. Bo wl 1 con tains 30 v anilla co okies and 10 c ho colate co okies. Bo wl 2 con tains 20 of eac h. No w supp ose y ou c ho ose one of the b o wls at random and, without lo oking, select a co okie at random. The co okie is v anilla. What is the probabilit y that it came from Bo wl 1? This is a conditional probabilit y; w e w an t p( Bo wl 1j v anilla ) , but it is not ob vious ho w to compute it. If I ask ed a dieren t questionthe probabilit y of a v anilla co okie giv en Bo wl 1it w ould b e easy: p( v anillaj Bo wl 1 ) = 3=4 Sadly , p(AjB) is not the same as p(BjA) , but there is a w a y to get from one to the other: Ba y es's theorem. 1.4 Ba y es's theorem A t this p oin t w e ha v e ev erything w e need to deriv e Ba y es's theorem. W e'll start with the observ ation that conjunction is comm utativ e; that is p(AandB) = p(BandA) for an y ev en ts A andB . Next, w e write the probabilit y of a conjunction: p(AandB) = p(A) p(BjA) Since w e ha v e not said an ything ab out what A andB mean, they are in terc hangeable. In terc hanging them yields p(BandA) = p(B) p(AjB) That's all w e need. Pulling those pieces together, w e get p(B) p(AjB) = p(A) p(BjA) 1Based on an example from http://en.wikipedia.org/wiki/Bayes' _theorem that is no longer there. 4 Chapter 1. Ba y es's Theorem Whic h means there are t w o w a ys to compute the conjunction. If y ou ha v e p(A) , y ou m ultiply b y the conditional probabilit y p(BjA) . Or y ou can do it the other w a y around; if y ou kno w p(B) , y ou m ultiply b y p(AjB) . Either w a y y ou should get the same thing. Finally w e can divide through b y p(B) : p(AjB) =p(A) p(BjA) p(B) And that's Ba y es's theorem! It migh t not lo ok lik e m uc h, but it turns out to b e surprisingly p o w erful. F or example, w e can use it to solv e the co okie problem. I'll write B1 for the h yp othesis that the co okie came from Bo wl 1 and V for the v anilla co okie. Plugging in Ba y es's theorem w e get p(B1jV) =p(B1) p(VjB1) p(V) The term on the left is what w e w an t: the probabilit y of Bo wl 1, giv en that w e c hose a v anilla co okie. The terms on the righ t are: Âˆp(B1) : This is the probabilit y that w e c hose Bo wl 1, unconditioned b y what kind of co okie w e got. Since the problem sa ys w e c hose a b o wl at random, w e can assume p(B1) = 1=2 . Âˆp(VjB1) : This is the probabilit y of getting a v anilla co okie from Bo wl 1, whic h is 3/4. Âˆp(V) : This is the probabilit y of dra wing a v anilla co okie from either b o wl. Since w e had an equal c hance of c ho osing either b o wl and the b o wls con tain the sa