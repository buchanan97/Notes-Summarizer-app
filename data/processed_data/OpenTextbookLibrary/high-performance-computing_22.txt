you would like it to be. Caches can be or ganized in one of several ways: direct mapped, fully associative, and set associative. Direct-Mapped Cache Direct mapping, as shown in link , is the simplest algorithm for deciding how memory maps onto the cache. Say , for example, that your computer has a -KB cache. In a direct mapped scheme, memory location maps into cache location , as do memory locations K, K, K, etc. In other words, memory maps onto the cache size. Another way to think about it is to imagine a metal spring with a chalk line marked down the side. Every time around the spring, you encounter the chalk line at the same place modulo the circumference of the spring. If the spring is very long, the chalk line crosses many coils, the analog being a lar ge memory with many locations mapping into the same cache line. Problems occur when alternating runtime memory references in a directmapped cache point to the same cache line. Each reference causes a cache miss and replaces the entry just replaced, causing a lot of overhead. The popular word for this is thrashing . When there is lots of thrashing, a cache can be more of a liability than an asset because each cache miss requires that a cache line be refilled an operation that moves more data than merely satisfying the reference directly from main memory . It is easy to construct a pathological case that causes thrashing in a -KB direct-mapped cache: Many memory addresses map to the same cache line REAL A(1024), B(1024) COMMON STUFF A,B DO I ,1024 A(I) A(I) B(I) END DO END The arrays