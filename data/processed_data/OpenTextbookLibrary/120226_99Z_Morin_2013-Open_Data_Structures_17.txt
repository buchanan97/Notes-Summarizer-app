use asymptotic notation within strings of equations. A particularly strange example of this occurs when we write statements like T(n) 2logn O( ): Again, this would be more correctly written as T(n) 2logn some member of O( ) : The expression O( ) also brings up another issue. Since there is no variable in this expression, it may not be clear which variable is getting arbitrarily large. Without context, there is no way to tell. In the example above, since the only variable in the rest of the equation is n, we can assume that this should be read as T(n) 2logn O(f(n)), wheref(n) . Big-Oh notation is not new or unique to computer science. It was used by the number theorist Paul Bachmann as early as 1894, and is immensely useful for describing the running times of computer algorithms. Consider the following piece of code: Introduction Simple void snippet() for (int i ; i n; i ) a i i; One execution of this method involves assignment ( inti ), n comparisons ( i n), nincrements ( i ), narray o set calculations ( a i ), and nindirect assignments ( a i i). So we could write this running time as T(n) a b(n ) cn dn en; wherea,b,c,d, andeare constants that depend on the machine running the code and represent the time to perform assignments, comparisons, increment operations, array o set calculations, and indirect assignments, respectively. However, if this expression represents the running time of two lines of code, then clearly this kind of analysis will not be tractable to complicated code or algorithms. Using big-Oh notation, the running time can be simpli ed to T(n) O(n):