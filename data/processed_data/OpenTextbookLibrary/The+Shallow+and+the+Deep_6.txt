ofactivity(Sj )inneuronjcannowincrea sethe ringrateofneuroniifconnect edthroughaninhibitorysynapsewij .This andothermathema ticalsubtletiesareclearlybiologicallyimplausiblewhichis duetothesomewha tarti cialintroductionof inasense negativeandpositive activitieswhicharetreatedinasymmet rizedfashion. However,aswedonotaimatdescr ibingbiologicalreality,theabovediscussed symmet rizationcanbejusti ed. Infact,itsimpli esthemathema tical andcomput ationaltreatment,andhascontributedto,forinstance,thefruitful popularizationofneura lnetworksinthestatisticalphysicscommunityinthe 1980sand1990s. .FROMNEURONSTONETW ORKS Mc Culloch Pittsneurons Quitefrequently,anevenmoredrasticmodi cationisconsidered: forin nite gain thesigmoidalactivationbecomesastepfunct ion,see Fig. (right panel)foranillustration.Eq.( )forinstanceyieldsinthislimit g(x) sign(x ) ifx ifx .( ) Inthissymmet rizedversionofabinaryactivationfunct ion,onlytwopossible statesareconsidered: eitherthemodelneuronistotallyquiescen t(S )or it resatmaximumfrequency ,whichisrepresen tedby S . The extreme abstractiontobinaryactivationstateswithoutthe exibility ofagradedresponsewas rstdiscussed by Mc Cul lochand Pittsin1943,who originallydenotedthequiescen tstateby S .The persistingpopularityof thismodelisduetoitssimplicityaswellasitssimilarityto Booleanconcept s inconventionalcomput ing.Inthefollowing,wewillfrequentlyresorttobinary modelneuronsinthesymmet rizedversion( ).Infact,theso-calledperceptron,asdiscussed in Chapter3,canbeinterpretedasasingle Mc Cul loch Pitts unitwhichisconnect edto Ninputneurons. Hebbianlearning Probablythemostintriguingpropertyofbiologicalneura lnetworksistheirabilitytolearn.Insteadofrealizingonlypre-wiredfunct ionalities,brainsadaptto theirenvironmen tor-inhigherlevelterms-theycanlearnfromexperience. Manypotentialformsofplasticityandmemo ryrepresen tationhavebeendiscussed intheliterature,includingthechemi calstorageofinformationorlearning throughneuro genesi s,i.e.thegrowthofnewneuro ns. Averypopularandplausibleparadigmoflearningissynapticplasticity.A keymechanism,Hebbian Learning,isnamed afterpsychologist Donald Hebb, who published hiswork The Organizationof Behaviorin1949 Heb49 .The originalhypothesiswasformulatedintermsofapairofneurons,whichareconnectedthroughanexcitatorysynapse: Whenanaxonofcell Aisnearenoughtoexcitecell Bandrepeatedly orpersistently takespartin ringit,somegrowthprocessormetabolicchange takesplaceinoneorbothcellssuchthat A se ciency,asoneofthecells ring B,isincreased. Thisisknownas Hebb slawandsometimes rephrasedas Neuronsthat re together,wiretogether. Hebbian Learningresultsinamemo rye ect which favorsthesimultaneousactivityofneuro ns Aand Binthefuture.Hence, it constitutesaformoflearningthroughsynapticplasticity. The questiontowhichextentthe Hebbianparadigmre ect sthebiological realityoflearningissubjectofon-goingdebate.Alternativeorcomplemen ting mechanismshavebeensuggested,see SVG forarecentexample.Inthe .NETW ORKARCHITECTU RES contextofarti cialneuralnetworks,Hebbiansynapticplasticityprovidesavery plausiblebasisfortherepresen tationoflearninginthemodels. Inthemathema ticalframeworkof ringratemodelspresentedintheprevioussection,wecanexpress Hebbian Learningquiteelegantly,assumi ngthatthe synapticchangeissimplyproportionaltothepre-andpost-synapticactivity: w AB SASB. ( ) Hence, thechange w ABofaparticularsynapsew ABdepends onlyonlocally availableinformation:theactivitiesofthepre-synaptic(SB)andthepostsynapticneuron(SA).For SA,SB 0thisisquiteclosetotheactual Hebbian hypothesis. The symmet rizationwith SA,B 1addssomebiologicallyimplausibleaspectstothepicture.Forinstance, anexcitatorysynapseconnect ing A and Bwouldalsobestrengthened accordingto Eq.( )ifbothneuronsare quiescen tatthesametime,sinceinthiscase SASB .Similarly,highactivityin Aandlowactivityin B(orviceversa)with SASB 0wouldweakenan excitatoryorstrengthenaninhibitorysynapse. In Hebb soriginalformulation, however,onlythepresence ofsimultaneousactivityshouldtriggerchangesof theinvolvedsynapse. Moreover,themathema ticalformalismin( )facilitates thepossibilitythatanindividualexcitatorysynapsecanbecomeinhibitoryor viceversa,whichisalsoquestionablefromthebiologicalpointofview. Manylearningparadigmsinarti cialneuralnetworksandotheradaptive systemscanbeinterpretedas Hebbian Learninginthesense oftheabovediscussi on.Examplescanbefoundinavarietyofcontexts,includingsupervised andunsup ervisedlearning,see Sec.2forworkingde ni tionsofthese terms. Notethattheactualinterpretationoftheterm Hebbian Learningvariesa lotintheliterature.Occasionally,itisempl oyedonlyinthecontextofunsupervisedlearning,sincefeedba ckfromtheenvironmen tisquitegenerallyassumed toconstitutenon-localinformation.Here,wefollowthewide-spr ead,rather relaxeduseofthetermforlearningprocesses whichdependonthestatesofthe pre-andpost-synapticunitsasin Eq.( ). Frequently,learningcanbeseen astheoptimizationofsuitablecostswhich areinterpretedasafunct ionofthenetworkparameters,i.e.thesynaptic strengthsorweights.Aswewillsee,inmanycasesnumer icaloptimization procedur es,whichareforinstancebasedongradientdescen t,leadtoupdate rulesfortheweightsthatresem ble Hebbian Learningtoalargeextent. Networkarchitectures Intheprevioussectionwehaveconsidered typesofmodelneuronswhichretain certainaspectsoftheirbiologicalcounterpartsandallowforamathema tical formulationofneura lactivity,synapticinteractions,andlearning.Thisenables ustoconstructnetworksfrom,forinstance,sigmoidalor Mc Cul