in quotes b ecause in this example, the probabilities are not normalized; that is, they do not add up to . So they are not true probabilities. But in this example the w ord coun ts are prop ortional to the probabilities. So after w e coun t all the w ords, w e can compute probabilities b y dividing through b y the total n um b er of w ords. Pmf pro vides a metho d, Normalize , that do es exactly that: pmf.Normalize() Once y ou ha v e a Pmf ob ject, y ou can ask for the probabilit y asso ciated with an y v alue: print pmf.Prob( ' the ' ) And that w ould prin t the frequency of the w ord the as a fraction of the w ords in the list. Pmf uses a Python dictionary to store the v alues and their probabilities, so the v alues in the Pmf can b e an y hashable t yp e. The probabilities can b e an y n umerical t yp e, but they are usually oating-p oin t n um b ers (t yp e float ). The co okie problem In the con text of Ba y es's theorem, it is natural to use a Pmf to map from eac h h yp othesis to its probabilit y . In the co okie problem, the h yp otheses are B1 and B2 . In Python, I represen t them with strings: pmf Pmf() pmf.Set( ' Bowl ' , ) pmf.Set( ' Bowl ' , ) This distribution, whic h con tains the priors for eac h h yp othesis,