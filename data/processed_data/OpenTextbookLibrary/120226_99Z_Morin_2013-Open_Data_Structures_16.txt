an individual computer. When we talk about the running time of an operation we are referring to the number of computer instructions performed during the operation. Even for simple code, this quantity can be di cult to compute exactly. Therefore, instead of analyzing running times exactly, we will use the so-called big-Oh notation : For a function f(n), O(f(n)) denotes a set of functions, O(f(n)) ( g(n) : there exists c , andn0such that g(n) c f(n) for alln n0) : Thinking graphically, this set consists of the functions g(n) wherec f(n) starts to dominate g(n) whennis su ciently large. We generally use asymptotic notation to simplify functions. For example, in place of nlogn 8n we can write O(nlogn). This is proven as follows: 5nlogn 8n 5nlogn 8n 5nlogn 8nlogn forn (so that log n ) 13nlogn : This demonstrates that the function f(n) 5nlogn 8n is in the set O(nlogn) using the constants c andn0 . Mathematical Background A number of useful shortcuts can be applied when using asymptotic notation. First: O(nc1) O(nc2); for anyc1 c2. Second: For any constants a;b;c , O(a) O(logn) O(nb) O(cn): These inclusion relations can be multiplied by any positive value, and they still hold. For example, multiplying by nyields: O(n) O(nlogn) O(n1 b) O(ncn): Continuing in a long and distinguished tradition, we will abuse this notation by writing things like f1(n) O(f(n)) when what we really mean isf1(n) O(f(n)). We will also make statements like the running time of this operation is O(f(n)) when this statement should be the running time of this operation is a member of O(f(n)). These shortcuts are mainly to avoid awkward language and to make it easier to