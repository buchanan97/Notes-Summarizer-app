OCR will approach this by being taught how to classify characters. It will be supplied with a training set, which will be a reasonable sample of letters and the correct classifications of what they are. Training sets are typically large. For example, the well-known and widely used MNIST set, which is a collection of hand-written examples of the digits through , with correct identification labels, has around , entries (Le Cun, Cortes, and Burges 1998). The overall OCR technique is an optical one, so it is the features of the sample letters that can be detected optically that will be the input (e.g. size, shape, color, grid arrangement of component dots or pixels, etc.). Then the program will attempt to correlate combinations (i.e. vectors) of these with the correct classification e.g. that a particular sample token character is an a . More than likely, the program will make many mistakes initially. But either the programmers, or the program itself, will tune various parameters (e.g. weights on the components of the vectors) to improve the classification until it reaches an acceptable level of performance. There is an interesting point to be made here about what are known in statistics as 'omitted variables'. As mentioned, the ML program will start by considering optical input from size, color, pixels etc. But it will then learn which variables of these to include and which to omit. The machine here has an advantage over a human statistician as it has the sheer computing power to run through the alternative possibilities in a reasonable time. The training set needs to be adequate for the task. For example, if the letter j does not appear in the