(for now at least). For example, May 2023, from (Open AI 2022c), you can pay a month and have good API access to GPT- . From an educational point of view, we can take a machete and cleave out and discard pretty much all of machine learning prior to Foundation Models and start our studies at that point. Andrej Karpathy writes: the whole setting of training a neural network from scratch on some target task (like digit recognition) is quickly becoming outdated due to finetuning, especially with the emergence of foundation models like GPT. These foundation models are trained by only a few institutions with substantial computing resources, and most applications are achieved via lightweight finetuning of part of the network, prompt engineering, or an optional step of data or model distillation into smaller, special-purpose inference networks. I think we should expect this trend to be very much alive, and indeed, intensify. In its most extreme extrapolation, you will not want to train any neural networks at all. (Karpathy 2023b) Digitization and Transcription Digital computers work with electronic digits, surprise. They work with the digits 0s and . But, unfortunately, at least some of the information resources that the ML algorithms have the potential to address are not, or were not, in digital form. For example, Shakespeare s only surviving playscript The Booke of Sir Thomas Moore was not (British Library 2020). So, digitization of those resources not born digital is an important precursor to wide-ranging ML in librarianship. The 2002 Google Books Project, or Google Project Ocean, was a very early attempt to address digitization. Its approach was to use OCR scanning on the physical resources. It was not