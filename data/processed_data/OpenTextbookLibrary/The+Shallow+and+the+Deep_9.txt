) g kvk k Figure1. :Feed-fo rwardneuralnetworks. Left:Amultilayeredarchitecture withvaryinglayer-sizeandasingleoutputunit.Right:Aconvergentfeedforwardnetworkwithalayerofinput neuro ns,onehidden layer,andasingle outputunit. quantitieswkjdenotetheweightsconnect ingthej-thinput componenttothekthhidden unitwithk , , K,wher e Kisthetotalnumberofhidden units (intheexample K ).These weightscanbegroupedinvectorswk RN, whilethehidden-t o-outputconnect ionsaredenotedbyvk R. Altogether,thearchitectureandconnect ivity,theactivationfunct ionandits parameters(gain,thresholdetc.),andthesetofallweightsdeterminetheactual input outputfunct ion RN S( ) Rparameterizedbythefeed-fo rward network.Again,theextensiontoseveraloutputunits,i.e.multi-dimensi onal funct ionvalues,isconcept uallystraightforward. Withoutgoingintodetailsyet,wenotethatwecontrolthefunct ionthatis actuallyimplemen tedbysettingtheweightsandotherfreeparametersinthe network.Iftheirdeterminationisguidedbyasetofexampledatarepresen ting atargetfunct ion,thetermlearningisused forthisadaptationor ttingprocess. Tobemoreprecise,thissituationconstitutesanexampleofsupervisedlearning asdiscussed inthenextsection. Tosumma rize,afeed-fo rwardneura lnetworkrepresentsanadaptiveparameterizationofa,ingeneralnon-linear,funct ionaldependence. Under rathermild conditions,feed-fo rwardnetworkswithsuitable,continuousactivationfunct ions areuniversalapproximators.Looselyspeaking,thismeansthatanetworkcan approximateany non-ma licious , continuousfunct iontoarbitraryprecision, providedthenetworkcomprisesasu ci entlylarge(problemdependen t)numberofhidden unitsinasuitablearchitecture,see Chapter5.Thisproperty clearlymotivatestheuseoffeed-fo rwardnetsinquitegeneralregressiontasks. Iftheresponseofthenetworkisdiscretized,forinstanceduetoathresholdingoperationthatyields S( ) , , C , ( ) thesystemperformstheassignmen tofallpossibleinputs tooneof Ccategories Following theauthor spersonal preference, layerednetworksaredrawnfromtop(input) tobottom (output). Alternativ eorientations canbeachievedbyrotating thepage. .NETW ORKARCHITECTU RES orclasses. Hence, thefeed-fo rwardnetworkconstitutesaclassi erwhichcanbe adaptedtoexampledatabychoiceofweightsandotherfreeparameters. Thesimplestfeed-fo rwardclassi er,theso-calledperceptron,willserveasa veryimportantexamplesysteminthefollowing.The perceptronisde ned as alinearthresholdclassi erwithresponse S( ) sign N j 1wj j ( ) toanypossibleinput R N,correspondingtoanassignmenttooneoftwo classesrepresen tedas S .Comparisonwith Eq.( )showsthatitcan beinterpretedasasingle Mc Cul loch Pittsneuro nwhichreceivesinput from N real-valuedunits. Theperceptronwillbediscussed indetailin Chapter3asabasicarchitecture thatprovidesvaluableinsightsintothefunda mentalsofmachinelearning. Otherarchitectures Apartfromtheclear-cut,fullyconnect edattractorneura lnetworksandstrictly feed-fo rwardlayerednets,alargevarietyofnetworktypeshavebeenconsidered anddesigned, oftenwithspeci capplicationdomainsinmind,see Chapter5. Combinationsoffeed-fo rwardstructureswith,forinstance, layersofhighly interconnect edunitsareempl oyedinthecontextof Reservoir Computing,see e.g. SVC07,LJ09 foroverviewsandreferences. Recently,theuseoffeed-fo rwardarchitectureshasre-gainedsigni cantpopularityinthecontextof Deep Learning GBC1 ,Hue19 .Speci cdesignsand architecturesof Deep Neural Networksincludinge.g.so-calledconvolutionalor poolinglayerswillbediscussed verybrie yin Chapter5. Theframeworkofprototype-basedlearningisintroduced in Chapter6.Systemslike Learning Vector Quantizationcanalsobeinterpretedaslayerednetworkswithspeci c,distance-ba sedactivationfunct ionsinthehidden units(the prototypes)andawinner-takes-allorsoftmaxoutputlayerforclassi cationor regression,respectively.Prototype-basedsystemsinunsup ervisedlearningwill bediscussed brie yin Chapter6. .FROMNEURONSTONETW ORKS Chapter2 Learningfromexampledata Youliveandlearn.Atanyrate,youlive. Douglas Adams Di erentformsofmachinelearningwerealreadybrie ypresentedin Sec. . Inthefollowingsection,wefocusonthemostclear-cut scena rios:supervised learningandunsup ervisedlearning.Inaddition,wewillbrie ydiscuss the interestingandfruitfulrelationsofmachinelearningwithstatisticalmodelling. Learningscenarios Themainchaptersofthese notesdealwithsupervisedlearning,withempha sis onclassi cationandregression.Severaloftheintroduced concept sandmethods canhoweverbetransfer redtounsup ervisedsettings.Thisisalsothecasefor theprototype-basedmethodsdiscussed in Chapter6. Unsupervisedlearning Unsup ervisedlearningisanumbrellatermcomprisingvariousmethodsforthe analysisofunlabeleddata.Suchdatasetsdonotcontainlabelinformationassociatedwithsomepre-de ned targetasitwouldbethecaseinclassi cationor regression.Moreover,there isnodirectfeedb ackavailablefromtheenvironmen t orateacherthatwouldfacilitatetheevaluationofthesystem sperformance. A comparisonofitsresponsewithagivengroundtruthorapproximaterepresentationthereo fisnotavailableorpossible. Formoreaboutthebackground ofunsup ervisedlearning,thereaderis referredtotheliterature,e.g. Hay09,Bis95a,Bis06 .Anintroduction,speci calgorithms andapplicationscanalsobefound inlecturenotesby Dalya Baron Bar19 .Hereweonlybrie