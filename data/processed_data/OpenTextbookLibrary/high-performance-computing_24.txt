line is the one that gets overwritten with new data. The assumption is that if the data hasn t been used in quite a while, it is least likely to be used in the future. Fully associative caches have superior utilization when compared to direct mapped caches. It s difficult to find real-world examples of programs that will cause thrashing in a fully associative cache. The expense of fully associative caches is very high, in terms of size, price, and speed. The associative caches that do exist tend to be small. Set-Associative Cache Now imagine that you have two direct mapped caches sitting side by side in a single cache unit as shown in link . Each memory location corresponds to a particular cache line in each of the two direct-mapped caches. The one you choose to replace during a cache miss is subject to a decision about whose line was used last the same way the decision was made in a fully associative cache except that now there are only two choices. This is called a set-associative cache . Set-associative caches generally come in two and four separate banks of cache. These are called two-way and four-way set associative caches, respectively . Of course, there are benefits and drawbacks to each type of cache. A set-associative cache is more immune to cache thrashing than a direct-mapped cache of the same size, because for each mapping of a memory address into a cache line, there are two or more choices where it can go. The beauty of a direct-mapped cache, however , is that it s easy to implement and, if made lar ge enough, will perform roughly as well