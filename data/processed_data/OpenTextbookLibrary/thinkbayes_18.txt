Ba y es's theorem is called the diac hronic in terpretation . Diac hronic means that something is happ ening o v er time; in this case the probabilit y of the h yp otheses c hanges, o v er time, as w e see new data. Rewriting Ba y es's theorem with H and D yields: p(Hj D) p(H) p(Dj H) p(D) In this in terpretation, eac h term has a name: p(H) is the probabilit y of the h yp othesis b efore w e see the data, called the prior probabilit y , or just prior . p(Hj D) is what w e w an t to compute, the probabilit y of the h yp othesis after w e see the data, called the p osterior . p(Dj H) is the probabilit y of the data under the h yp othesis, called the lik eliho o d . p(D) is the probabilit y of the data under an y h yp othesis, called the normalizing constan t . Sometimes w e can compute the prior based on bac kground information. F or example, the co okie problem sp eci es that w e c ho ose a b o wl at random with equal probabilit y . In other cases the prior is sub jectiv e; that is, reasonable p eople migh t disagree, either b ecause they use di eren t bac kground information or b ecause they in terpret the same information di eren tly . The lik eliho o d is usually the easiest part to compute. In the co okie problem, if w e kno w whic h b o wl the co