understood. SSL really found its place in Natural Language Processing (NLP), and some examples from NLP may make the technique clearer. The foundation models BERT and GPT- both have the pre-training task of predicting the next word, or previous word, from a sequence of words in a passage of English (or other natural languages) (Devlin et al. 2019; Brown et al. 2020). The way they do it is to scan vast amounts of English text e.g. trillions of word tokens such as the entire of the Internet (including Wikipedia, Reddit linked sources, all freely available digitized novels, etc.). Apparently GPT- was pre-trained on terabytes of data. This is about the same size as one quarter of the holdings of the Library of Congress. This pre-training provides the various transition probabilities from prefix, or suffix, words or sequences, to the current target word, and, essentially, the solution. Now, the data itself, the English text, is not labeled, so this is not supervised learning. But the scanning of the text can produce a pseudo-label for the missing 'gap word'. For example, the label for 'The cat sat on the ?label? ' can be produced merely by looking through a vast amount of real-life text. Of course, there does not have to be a single one-word answer to this. 'mat', 'table', 'floor', etc. might all be possible answers. But then there will be probabilities associated with the possible answers, and the wider context will provide guidance. Self-supervised learning has an obvious home with natural language. But it also can be used with images. There is a context to which patches of colors or pixels are close to other patches. Further, foundation models like