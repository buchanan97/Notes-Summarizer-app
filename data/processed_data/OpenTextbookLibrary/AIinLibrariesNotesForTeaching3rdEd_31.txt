be ranked, perhaps by a human judge; then the rankings could be used as a reward structure, and a reinforcement learning system introduced improve the system at translating. Sometimes, in this context, the reward structure is called the 'reward model'. Typically, reinforcement learning is very compute intensive e.g. for chess, the ML system may need to play hundreds of thousands of games. There are many algorithms to produce reinforcement learning, but few, if any, are efficient in really large settings. Supposedly, one of the technologies that enabled some of the uses of Foundation Models, such as Chat GPT, was the invention of Proximal Policy Optimization (Open AI 2017). Proximal Policy Optimization is a reinforcement learning algorithm. Reinforcement Learning from Human Feedback (RLHF) Modern Foundation Models or Large Language Models often use Reinforcement Learning in a very specific way. The training of these takes place in two stages: the initial training to produce a plain vanilla base model, then Reinforcement Learning from Human Feedback (RLHF) is used to yield the desired product. A textual base model might be produced by self-supervised training on most of the text on the Internet. This might take months to do, and the result might be able to write presentable English. But the model might at that point lack some desirable qualities (such as answering questions given by prompts) and might possess some undesirable properties (such as lying, giving poor medical advice, revealing private information). The model will then be tuned using RLHF. A jury of perhaps people will be assembled and given maybe , samples of pairs of answers from the model. Each member of the jury will evaluate the answers, saying which of a