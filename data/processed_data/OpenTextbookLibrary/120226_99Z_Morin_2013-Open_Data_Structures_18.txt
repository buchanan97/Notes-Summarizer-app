Not only is this more compact, but it also gives nearly as much information. The fact that the running time depends on the constants a,b,c,d, andein the above example means that, in general, it will not be possible to compare two running times to know which is faster without knowing the values of these constants. Even if we make the e ort to determine these constants (say, through timing tests), then our conclusion will only be valid for the machine we run our tests on. Big-Oh notation allows us to reason at a much higher level, making it possible to analyze more complicated functions. If two algorithms have Mathematical Background the same big-Oh running time, then we won t know which is faster, and there may not be a clear winner. One may be faster on one machine, and the other may be faster on a di erent machine. However, if the two algorithms have demonstrably di erent big-Oh running times, then we can be certain that the one with the smaller running time will be faster for large enough values of n. An example of how big-Oh notation allows us to compare two di erent functions is shown in Figure , which compares the rate of grown off1(n) nversusf2(n) nlogn. It might be that f1(n) is the running time of a complicated linear time algorithm while f2(n) is the running time of a considerably simpler algorithm based on the divide-andconquer paradigm. This illustrates that, although f1(n) is greater than f2(n) for small values of n, the opposite is true for large values of n. Eventuallyf1(n) wins out, by an increasingly wide margin. Analysis using big-Oh notation told us that