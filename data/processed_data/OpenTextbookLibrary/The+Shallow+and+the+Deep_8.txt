)restores thepatternnearlyperfectlyand S(t)approaches forlarget.Theretrievalof astoredpatternfromanoisyinitialstateisillustratedin Fig. (rightpanel). Notethatthetwo-dimensi onalarrangemen tofneuronsservespurelyillustrative purposes.Sinceeveryneuronisconnect edtoeveryotherneuron,neighborhood relationsdonotde ne alow-dimensi onaltopologyinthe Hop eldmodel. Successful retrievalofastoredpatternisonlypossibleiftheinitialdeviation of S( )from isnottoolarge.Moreover,onlyalimitednumberofpatterns canbestoredandretrievedsuccessful ly.Forrandompatternswithzero mean activities j ,thestatisticalphysicsbasedtheoryofthe Hop eldmodel (validinthelimit N )showsthat P r Nmustbesatis ed. The value r .14markstheso-calledcapacitylimitofthe Hop eldmodel5. Notethattheweightmatrixconstruction( )canalsobeinterpretedas Hebbian Learning:Startingfromatabularasastateofthesynapticstrengths withzeroweights,asingletermoftheform i jisadded foreachactivity pattern,represen tingtheneuronsthatareconnect edbysynapsewij(andwji). Hence, Eq.( )canbewrittenasaniteration wij( ) ,wij( ) wij( ) P i j, ( ) wher etheincremen talchangeofwijdepends onlyonlocallyavailableinformationandisoftheform pre-synaptic post-synapticactivity. The Hop eldmodelservesasaprototypicalexampleofhighlyconnect ed neura lnetworks.Potentialapplicationsincludepatternrecognitionandimage processingtasks.Perhapsmoreimportantly,themodelhasprovidedmany theoreticalandconcept ualinsightsintoneura lcomput ationandcontinuesto doso. Moregeneralrecurrentneura lnetworksareappliedinvariousdomainsthat requiresomesortoftemporalorsequence-ba sedinformationprocessing.This includes, amongothers, robotics,speechorhandwr itingrecognition. Feed-forwardlayered neuralnetworks Thro ughoutthisreader, wewillmainlydealwithanotherclear-cut network architecture:layeredfeed-fo rwardnetworks.Inthese systems, neuro nsarearrangedinlayersandinformationisprocesse dinawell-de ned direction. This critical value isoftenreferredtoas cintheliterature,butitshould notbeconfused with thestoragecapacit yoffeed-f orwardnetworksin,e.g., Sec. .NETW ORKARCHITECTU RES The leftpanelof Fig. .5showsaschema ticillustrationofafeed-fo rward architecture.Aspeci c,singlelayerofunits(thetoplayerintheillustration) represen tsexternalinput tothesystemintermsofneuralactivity.Inthe biologicalcontext,onemightthinkofthephotorecept orsintheretinaorother senso ryneuro nswhichcanbeactivatedbyexternalstimuli. Thestateoftheneuronsinallotherlayersofthenetworkisdeterminedvia synapticinteractions,andactivationsoftheform S(k) i g jw(k) ij S(k ) j . ( ) Here,theactivity S(k) iofneuroniinlayerkisdeterminedfromtheweightedsum ofactivitiesinthepreviouslayer(k )only:informationcontainedintheinput isprocessed layerbylayer.Ultimately,thelastlayerinthestructure(bottom layerintheillustration)represen tsthenetwork soutput,i.e.itsresponsetothe input orstimulusinthe rstlayer.Theillustrationdisplaysonlyasingleoutput unit,buttheextensiontoalayerofseveraloutputsisstraightforward. The essen tialpropertyofthefeed-fo rwardnetworkisthedirectedinformationprocessing:neuronsreceiveonlyinput fromunitsinthepreviouslayer.As aconsequence, thenetworkcanbeinterpretedastheparameterizationofaninput outputrelation,i.e.amathema ticalfunct ionthatmapsthevectorofinput activationstoasingleorseveraloutputvalues.Thisinterpretationstillholdsif nodesreceiveinput fromseveralpreviouslayers,orinotherwords:connect ions may skip layers.Forthesakeofclarityandsimplicity,wewillnotconsider thisoptioninthefollowing. The feed-fo rwardpropertyandinterpretationasasimpleinput outputrelationislostassoonasanyformoffeedba ckispresent:inter-layersynapses or backwardconnect ionsfeedinginformationintoprevious( higher ) layersintroduce feedbackloops,makingitnecessa rytodescr ibethesystemintermsofits fulldynamics. Neuronsthatdonotcommunicatedirectlywiththeenvironmen t,i.e.all unitsthatareneitherinput noroutputnodes,aretermed hidden units(nodes, neuro ns)whichformhidden layersinthefeed-fo rwardarchitecture. The rightpanelof Fig. .5displaysamoreconcret eexample.The networkcomprisesonelayerofhidden units,here withactivities k R,and asingleoutput S.The responseofthesystemtoaninput con guration ( 1, 2, , N) RNisgivenas S( ) g K k 1vk k g K k 1vkg N j 1wkj j . ( ) Hereweassume, forsimplicity,thatallhidden andoutputnodesempl oythe sameactivationfunct iong( ).Obviously,thisrestrictioncanberelaxedby de ni nglayer-speci corevenindividualactivationfunct ions.In Eq.( )the .FROMNEURONSTONETW ORKS S( ) j R j R wkj R k g jwkj j vk R S(