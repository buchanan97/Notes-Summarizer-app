a digital surrogate, there are cryptographic techniques to preserve text surrogates 'forever'. They could be encrypted, hashed, and put into a suitable public blockchain using content-based indexing. Alternatively, one digital copy could be placed in the Billion Year Archive through the good graces of the Arch Mission Foundation (Spivak and Slavin 2023). Free Books! Once text is in digital form the marginal cost of duplication is near zero. Of course, there are considerations of licensing and intellectual property. Natural Language Processing Natural Language Processing (NLP) has always used 0s and 1s and computers. But it has absolutely flourished with ML, and especially with Large Language Models (which we will discuss later). As a selection of possible procedures or techniques, there is: the entire field of digital humanities, text summarization, text mining, question answering, information extraction, text categorization, sentiment analysis, plagiarism detection, author and genre recognition, word sense disambiguation, and lexical and ontological acquisition, and text analysis for social applications such as blogs and social networks. So, for example, given two physical texts by unknown authors and the question 'are these texts written by the same author?', digitization and NLP can provide the answer. Processing by Computer Software Much data either exists or was initially recorded on a physical medium, such as paper or cards. This data really needs to be digitized in order to be processed by statistics or data science. This processing would then amount to data processing, or data mining, or text data mining (TDM). Vast continents of knowledge or information would be opened up. Data and the Need for Good Data ML aims to learn, and what it is going to learn from is data. Primarily this