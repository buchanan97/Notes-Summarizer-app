eld. The notesmaybeperceivedasoldschool,certainlybysomededicatedfollowersoffashion Dav66 .Admittedly,thetextdoesnotaddressthemost recentdevelopmen tsine.g.Deep Learninganditsapplications.However,in myhumbleopinionitisinvaluabletohaveasolidbackgroundknowledgeofthe basicsbeforeexploringtheworldofmachinelearningwithanambitionthat goesbeyondtheapplicationofsomesoftwarepackagetosomedataset. Ther efore,theempha sisisonbasicconcept sandtheoreticalbackground, withspeci caspectsselectedfromapersonalandclearlybiasedviewpoint.In asense, thegoalistode-m ystifymachinelearningandneura lnetworkswithout iii iv losingtheappreciationfortheirfascinatingpowerandversatility.Veryoften, thisinvolvesalookintothehistoryandpre-historyofneura lnetworks,wher e thefounda tionsformostoftherecentdevelopmen tswerelaid. Ihaveaimed atpointingtheinterestedreadertomanyresourcesforfurther explorationofthearea.Ther efore,thelistofreferences inthebibliography, althoughbynomeanscomplete,isslightlymoreextensivethaninitiallyenvisioned. Thestartingpointforthese noteswasthedesiretoprovidemorecomprehensivematerialthanthepresentationslidesinthe MSc levelcourse Neural Networks(renamed Neural Networksand Computational Intelligencelater)which Ihavebeengivingatthe Universityof Groningen.Athorougharcheological investigationofthetextand gureswouldalsorevealtracesofthecourses Theorie Neuronaler Netzwerkeand Un berwachtes Lernen,that Itaughtwayback when inthe Physicsprogramatthe Universityof W rzbur g. Mywritingactivitywasgreatlyboostedontheoccasionofthewonderful 30th Canary Islands Winter Schoolin2018,devotedto Big Dataanalysisin Astronomy,wher e Ihadthehonortogiveaseriesoflecturesonsupervised learning,see MSK ,Bie19 forcoursematerialsandvideo-reco rdedlectures. Lastnotleast Iwouldliketoacknowledgeconstructivefeedba ckfromseveral generations ofstuden tswho followedthecourseandfrommanycolleagues andcollaborators.Inparticular,Ithank Elisa Oostwaland Janis Nordenfora criticalreadingofthemanuscriptandmanysuggestionsforimpro vemen ts. Groningen,June2023 Themysteriousmachinelearningmachine Catharina M.Gerigkand Elina L.vanden Brandho f Reproduced withkindpermissionoftheartists. Contents Preface iii Fromneuronstonetworks Spikingneuronsandsynapticinteractions Firingratemodels Neuralactivityandsynapticinteraction Sigmoidalactivationfunct ions Hebbianlearning Networkarchitectures Attractornetworksandthe Hop eldmodel Feed-fo rwardlayeredneuralnetworks Otherarchitectures Learningfromexampledata Learningscena rios Unsup ervisedlearning Supervisedlearning Otherlearningscena rios Machine Learningvs.Statistical Modelling Di erences andcommo nalities Anexamplecase:linearregression Conclusion The Perceptron Historyandliterature Linearlyseparablefunct ions The Rosenblattperceptron Theperceptronstorageproblem Iterative Hebbiantrainingalgorithms The Rosenblattperceptronalgorithm Theperceptronalgorithmasgradientdescen t The Perceptron Convergence Theo rem Afewremarks Thecapacityofahyperplane v vi CONTEN TS Thenumberoflinearlyseparabledichotomies Discussi onoftheresult Timeforapizzaorsomecake Learningalinearlyseparablerule Studen t-teacherscena rio Learninginversionspace Gener alizationbeginswher estorageends Optimalgeneralization Theperceptronofoptimalstability Thestabilitycriterion The Min Overalgorithm Optimalstabilitybyquadraticoptimization Optimalstabilityreformulated The Adaptive Linear Neuron-Adaline The Adaptive Perceptron Algorithm-Ada Tron Supp ortvectors Inhom.lin.sep.funct ionsrevisited Someremarks Beyondlinearseparability Perceptronwitherrors Minimalnumberoferrors Softmarginclassi er Layerednetworksofperceptron-likeunits Committeeandparitymachines Theparitymachine:auniversalclassi er Thecapacityofmachines Supp ort Vector Machines Non-lineartransformationtohigherdimensi on Largemarginclassi er Thekerneltrick Afewremarks Feed-forwardnetworksforregressionandclassi cation Feed-fo rwardnetworksasnon-linearfunct ionapproximators Architectureandinput-outputrelation Universalapproximators Gradientbasedtrainingoffeed-fo rwardnets Comput ingthegradient:Backpropagationof Error Batchgradientdescen t Stochasticgradientdescen t Practicalaspectsandmodi cations Objectivefunctions Costfunct ionsforregression Costfunct ionsforclassi cation Activationfunct ions CONTEN TS vii Sigmoidalandrelatedfunct ions One-sidedandunbounded activationfunct ions Exponentialandnormalizedactivations Remark:universalfunct ionapproximation Speci carchitectures Popularshallownetworks Deepandconvolutionalneuralnetworks Distance-basedclassi ers Prototype-basedclassi ers Nearest Neighborand Nearest Prototype Classi ers Learning Vector Quantization LVQtrainingalgorithms Distancemeasures andrelevancelearning LVQbeyond Euclideandistance Adaptivedistances inrelevancelearning