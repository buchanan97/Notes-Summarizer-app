GPT- , GPT- , etc., are becoming multi-modal. Multi-modal means that they can work with different modes such as with text, images, audio, video, etc.. The earlier technique for this was to use text as a stepping-stone. The model would be pre-trained, using SSL on text, then fine-tuned, perhaps with some prompts or labeling, to work on images. Nowadays, many Foundation Models can work with different modes natively, without using text as an intermediary step. SSL offers freedom. Getting good, labeled data at scale is difficult, if not near impossible. It is a barrier or bottleneck. But with SSL, it is not needed. Reinforcement Reinforcement learning is familiar to us in daily life. It involves exploration of an environment by trial-and-error, and, as part of this, having what are called 'delayed rewards' (Sutton and Barto 2018). The rewards provide feedback as to how well the trial-and-error is working. Imagine a student backpacker having temporary employment picking apples in an orchard. She gets paid for each apple she picks (but for each apple she picks there will be one less apple to pick on the tree that she picked it from). Also, she gets a bonus for each basket of apples that she picks, especially if she fills the basket faster than other pickers. Bigger apples will fill a basket quicker, but there will be fewer of them in the basket. We will assume here that she is trying to maximize her pay, i.e. her rewards. Quite what her best picking strategy might be is a bit of a question. She presumably will have to change trees from time to time, but changing trees is not actually picking. It amounts to