michael biehl shallow deepa biased introduction neural network old school machine learning shallow deep collection lecture note offer accessible introduction neural network machine learning general clear beginning note able cover rapidly changing growing field entirety focus lie classical machine learning technique bias classification regression learning paradigm recent development instance deep learning addressed briefly touched argues having solid knowledge foundation field essential especially want explore world machine learning ambition go application software package data set shallow deep place emphasis fundamental concept theoretical background involves delving history neural network foundation recent development note aim demystify machine learning neural network losing appreciation impressive power michael biehl associate professor computer science bernoulli institute mathematics computer science artificial intelligence university groningen joined intelligent system group hold honorary professorship machine learning center system modelling quantitative biomedicine university birmingham uk research focus modelling theoretical understanding neural network machine learning general development efficient training algorithm interpretable transparent system topic particular variety interdisciplinary collaboration concern practical application machine learning biomedical domain astronomy area michael biehlthe shallow deepthe shallow deep biased introduction neural network old school machine learning michael biehlpublished university groningen press broerstraat cp groningen netherlands published netherlands michael biehl bernoulli institute mathematics computer science artificial intelligence groningen comment correction suggestion welcome contact cite biehl shallow deep biased introduction neural network old school machine learning university groningen press book published open access thanks financial support open access textbook fund university groningen cover design ba ekkers coverphoto michael biehl production line boek en medium bv isbn print isbn epdf doi http work licensed creative common international license licence term available stopcallingeverythingai thesubtitleofthese lecturenotesis abiasedintroductiontoneura lnetworks andoldschoolmachinelearning giveanaccessi bleintroductiontotheﬁeld ithasbeenclearfromthebeginning thatitwouldnotendupasacomprehensi ve focusis onclassicalmachinelearning manyrecentdevelopmen tscannotbecovered personally iﬁrstgotintouchwithneura lnetworksinmyearlylifeasa itwassuﬃci enttoreadahandful ofpapersandperhaps alittlelaterthegoodbook piecetothebigpuzzl tnostalgic probably thesituationhasdeﬁni anoverwhelming ﬂoodofpublicationsmakesitdiﬃcul ttoﬁlterouttherelevantinformationand keepupwiththedevelopmen t theselectionoftopicsinthese noteshasbeendeterminedtoalargeextent bymyownresearchinterestsandearlyexperiences thisisdeﬁni telytruefor theinitialfocusonthesimpleperceptron thehydrogenatomofneuralnetwork researchasmanfredopperputit thebulkofthistextdeals withshallowsystemsforsupervisedlearning inparticularclassiﬁcation reﬂect smymaininterestintheﬁeld notesmaybeperceivedasoldschool lowersoffashion thetextdoesnotaddressthemost recentdevelopmen myhumbleopinionitisinvaluabletohaveasolidbackgroundknowledgeofthe basicsbeforeexploringtheworldofmachinelearningwithanambitionthat goesbeyondtheapplicationofsomesoftwarepackagetosomedataset ther efore theempha sisisonbasicconcept sandtheoreticalbackground asense ystifymachinelearningandneura lnetworkswithout iiiiv lnetworks wher e thefounda tionsformostoftherecentdevelopmen tswerelaid ihaveaimed atpointingtheinterestedreadertomanyresourcesforfurther efore thelistofreferences inthebibliography althoughbynomeanscomplete sioned thestartingpointforthese sivematerialthanthepresentationslidesinthemsc work renamed neuralnetworksandcomputationalintelligencelater orieneuronalernetzwerkeandunüberwachteslernen thatitaughtwayback inthephysicsprogramattheuniversityofwürzbur mywritingactivitywasgreatlyboostedontheoccasionofthewonderful devotedtobigdataanalysisin astronomy wher eihadthehonortogiveaseriesoflecturesonsupervised learning msk rdedlectures lastnotleastiwouldliketoacknowledgeconstructivefeedba ckfromseveral generation ofstuden tswho followedthecourseandfrommanycolleagues ithankelisaoostwalandjanisnordenfora criticalreadingofthemanuscriptandmanysuggestionsforimpro vemen t groningen themysteriousmachinelearningmachine f reproduced preface iii ion rwardlayeredneuralnetworks rio ervisedlearning rio andcommo nalities linearregression ion t theo rem vvi conten t onoftheresult rio alizationbeginswher estorageends ortvectors ionsrevisited auniversalclassiﬁer ortvectormachines ionapproximators rwardnets ingthegradient backpropagationoferror t t ionsforregression ionsforclassiﬁcation ion t vii ion activationfunct ion universalfunct ionapproximation andrelevancelearning inrelevancelearning ﬁtting s forregression forclassiﬁcation theroccurve sequalitymeasures conten t onalityreduct ion onalembedding onalscaling andrelatedmethods byhebbianlearning enden tcomponentanalysis andextensionsofvq tyestimation ationtechniques ation ationbasedonavailabledata sampling augmentation ion concludingquote aoptimiz ation onaltaylorexpansion andsaddlepoints ryandsuﬃci entconditions unsolvablesystemsoflinearequations t coordinatetransformations t t ix t t listofﬁgures listofalgorithms bibliography sixwritingtipsx conten fromneuronstonetworks realityisoverratedanyway tounder standandexplainthebrain ticity toadapttoandtosurvivein t ultimately theperformanceofthebrainmustrelyonitshardware ware andemer gesfromthecooperativebehaviorofitsmany relatively simple yethighlyinterconnect edbuildingblocks human cortex forinstance individualcellcanbeconnect edtothousandsofothers inthisintroductiontoneuralnetworksandcomputationalintelligencewe willstudyartiﬁcialneuralnetworksandrelatedsystems degreetowhichthese system relatetotheirbiologicalcounterpartsis generallyspeaking theirdevelopmen twasgreatlyinspiredbykeyaspectsofbiologicalneurons efore itisuseful tobeawareoftheconcept ualconnect ion betweenartiﬁcialandbiologicalsystems atleastonabasiclevel quiteoften nature tion mighthavefoundhighlycomplexsolutionstocertainproblemsthatcould used exampleinthiscontextistheconstructionofeﬃcientaircraft whichbyno meansrequirestheuseofmovingwingsinordertoimitatebirdﬂightfaithfully ofcourse itisunclearaprioriwhichofthedetailsareessen tialandwhich onescanbeleftoutinartiﬁcialsystems obviously thisalsodepends onthe including thecapabilit yofbeing fascinated orks science andmachinelearningresearchcontinuestoplayanimportantrolefor thefurtherdevelopmen tofboth inthisintroductorytextwewillconsiderlearningsystems whichdrawon onlythemostbasicmechanisms ther efore thischapterisonlymeantasavery briefoverview whichshouldallowtorelatesomeoftheconcept sinartiﬁcial neura lcomput ationtotheirbiologicalbackground thereadershouldbeaware recomme ndedtextbooksandothersources inmostofthisintroductorychapter detailedcitationsconcerningspeciﬁctopics willnotbeprovided instead thefollowinglistpointsthereadertoselected textbook rangefrombriefandsuperﬁcialto verycomprehensi veanddetailedreviewsofthebiologicalbackground thesame istrueforthediscussi onofthediﬀerentconcept uallevelsonwhichbiological system pointtothefullcitationinformationin personalpreferences ney s neuralnetworks givesaverybasicoverviewandprovides aglossaryofbiologicalorbiologicallyinspiredterms covertherelevanttopicsinslightlygreaterdepth anintroductiontothetheoryof theinspirationfrombiologicalneuro nsandnetworksintheﬁrstchapters italsoprovidesathoroughanalysisofthehopﬁeldmodelfromastatistical physicsperspective work includingabasicdiscussi onofthebiologicalbackground n scomprehensi vemonographonthebiophysicsof computation thediﬀerentmodellingapproachesand relatesthem toexperimentaldataobtainedfromrealworldneurons thejustiﬁcationofneuralmodeling someaspectsoftheorganizationofthebrainintermsofmapsintheir slecturenotesonneuralcomputationprovideanoverview ofbiologicalinformationprocessingandmodelsofneuralactivity synaptic modellingapproachesarediscussed insomedetail ingneuronsandsynaptic interactio n physiologyandfunct ionalityofthebiologicalsystems ishighlycomplex alreadyonthesingleneuro calprocesses ingreatdetailinordertorepresen includes descr ibethestateofcellcompartmentsintermsofanelectrostatic potential whichisduetovaryingionconcentrationsonbothsidesofthecell mem governthemem descr ibesitstemporalevolutionintermsoffourcoupledordinarydiﬀerential equation theparametersofwhichcanbeﬁttedtoexperimentaldatameasured inrealworldneurons whenev erthemem branepotentialreachesathresholdvalue forinstance triggeredbytheinject ionofanexternalcurrent ashort localizedelectrical termactionpotentialorthemoresloppyspikewillbe used nissaidtoﬁrewhen aspikeisgenerated theactionpotentialdischargesthemem branelocallyandpropagatesalong themem leftpanel astronglyelongated extensionisattachedtothesoma pointofview itservesasacablealongwhichactionpotentialscantravel ofcourse calprocesses aresigniﬁcantlydiﬀerent fromtheﬂowofelectronsinaconventionalcoppercable forinstance infact actionpotentialsjump betweenshortgapsinthemyelinsheath aninsulating layeraround actionpotentials sprea dalongtheaxonicbranchesoftheﬁringneuronandeventuallyreachthe pointswher ethebranchesconnect tothedendr itesofotherneuro n aconnect ion termed synapse isshownschema rightpanel uponarrivalofaspike ticcleft dendr ance speciﬁcrecept inthesynapse theactionpotentialisnottransfer red directlythroughaphysicalcontactpoint butchemi onthedetailedproperties ofthesynapse tentialincrea aspikearrivesataninhibitorysynapse branepotentialdecreases bothexcitatoryandinhibitorysynapsescanhavevaryingstrengths asreﬂect ed gapjunctions existwhichcanfunction ectional electrical synaps e forfurther information orks schema ticillustrationofneuro n pyramidalcells nswithsoma dendr itic tree axon ingneuro redrawnafter branepotential consequently themem branepotentialofaparticularcellwillvaryovertime dependingontheactualactivitiesoftheneuro nsitreceivesspikesfromthrough excitatoryandinhibitorysynapses thethresholdforspikegenerationis reached theneuronﬁresitselfand inﬂuences thepotentialandactivityof asetofinterconnect edneuronsforms acomplexdynamicalsystemofthresholdunitswhichinﬂuence eachother s activitythroughgenerationandsynaptictransmissionofactionpotentials originofaverysuccessful approachtothemodellingofneuro ire iaf model caldetailsaccountedforinthe andwereprobablynotknownat thetime mem braneissimplyrepresen tedbyitsconduct anceandohmic resistance allchargetransportpheno mena arecombinedinoneeﬀect triccurrent whichsumma rizestheindividualcontributionsofchangingion concentrationsaswellasleakcurrentsthroughthemem preciseformofspikesanddetailsoftheirgenerationandtransportareignored instead tedbyastructureless diracdeltafunct ionwhichdeﬁnes calmodels theiafmodelcan beﬁttedtophysiologicaldataandyieldsafairlyrealisticdescr iptionofneuronal time m left upper schema ticillustrationofanactionpotential lower spikestravelalongtheaxon throughsaltatoryconduct schema ticillustrationofhowmeanﬁringratesarederivedfromatemporal spikepattern gratemodels inanotherstepofabstraction thedescr iptionofneuralactivityissimpliﬁed bytakingintoaccountonlythemeanﬁringrate numberofspikesperunittime theconcept right panel implicitassumpt ingiscontainedinthemeanactivityandfrequency ofspikesoftheneuro n garded whiletheroleofindividualspiketimingappearstobethetopicof cientsimulationsofverylargenetworksofneuro nsandcanbeseen asthebasis ofvirtuallyallartiﬁcialneura lnetworksandlearningsystemsconsidered inthis text neuralactivityandsynapticinteraction ﬁringratepictureallowsforasimplemathema ticaldescr iptionofneural ni whichreceivesinput factthattheﬁringrateofabiologicalneuroncannotexceed acertainmaximum calconstraints wecanlimitsitoarangeof resting ofanyspikegeneration theactivityofneuroniisgivenasa responseofincomingspikes tedonlybythemeanactivitiessj xi http omain forfurther orks tthestrengthofthesynapseconnect ing potentialxiifneuro njisactive sj whilewij termstotheweightedsum notethatrealworldchemi calsynapses arestrictly evenifconnect ionswijandwjiexistforagivenpairofneuro n theywouldbephysiologicallyseparate indep enden tentities sigmoid alactivationfunctions avarietyofdiﬀerentactivationfunct ionsh x havebeenempl oyedinartiﬁcial neura ionswillbeintroduced inalater vationwhicharguablycapturesimportantcharacteristicsofbiologicalsystems itisplausibletoassume thefollowingmathema vationfunct ionh x ofagivenneuro n subscri ptiomitted withlocalpotential xasineq lim x restingstate absence ofspikegeneration x monotonicincrea seoftheexcitation lim x maximumpossibleﬁringrate whichtakesintoaccountthelimitationsofindividualneuralactivitydiscussed intheprevioussection variousactivationortransfer funct ionshavebeensuggestedandconsidered rwardneuralnetworks wewilldiscuss severaloptionsinsec ion h x γ whichclearlysatisﬁes etersarethethresholdθ whichlocalizesthesteepestincrea seofactivity thegainparameterγ θdoesnotdirectlycorrespondtothepreviouslydiscussed itmarksthecharacteristicvalueof hatwhichtheactivationfunct ioniscentered ityispartlyduetothefactthat therelation veryeﬃcien tcomputation ofthederivative seealsochapter siγ jwijsj si jwijsj schema ticillustrationofsymmet rizedactivationfunct asigmoidaltransfer funct ionwithgainγandthresholdθinthesymmet rized represen tation thebinarymccul lochpittsactivationas symme trizedrepresentationofactivity wewillfrequentlyconsiderasymmet rizeddescr iptionofneuralactivityinterms ofmodiﬁedactivationfunct ion lim x restingstate absence ofspikegeneration x monotonicincrea seoftheexcitation lim x maximumpossibleﬁringrate anexampleactivationanalogoustoeq g x γ atﬁrstsight thisappearstobejustanalternativeassignmen totherestingstate notethatintheoriginaldescr sj aquiescen tneuro n doesnotinﬂuence itspostsynapticneuro keepingthe formoftheactivationas xi impliesthattheabsence ofactivity inneuronjcannowincrea sethe ﬁringrateofneuroniifconnect edthroughaninhibitorysynapsewij andothermathema ticalsubtletiesareclearlybiologicallyimplausiblewhichis duetothesomewha activitieswhicharetreatedinasymmet rizedfashion aswedonotaimatdescr ibingbiologicalreality cussed symmet rizationcanbejustiﬁed infact itsimpliﬁesthemathema tical andcomput ationaltreatment andhascontributedto forinstance thefruitful popularizationofneura lnetworksinthestatisticalphysicscommunityinthe orks mccullochpittsneurons quitefrequently anevenmoredrasticmodiﬁcationisconsidered forinﬁnite ion right panel forinstanceyieldsinthislimit g x ifx θ inthissymmet rizedversionofabinaryactivationfunct ion onlytwopossible statesareconsidered eitherthemodelneuronistotallyquiescen t itﬁresatmaximumfrequency whichisrepresen extreme abstractiontobinaryactivationstateswithouttheﬂexibility ofagradedresponsewasﬁrstdiscussed bymccul originallydenotedthequiescen persistingpopularityof thismodelisduetoitssimplicityaswellasitssimilaritytobooleanconcept s inconventionalcomput wewillfrequentlyresorttobinary modelneuronsinthesymmet rizedversion tron asdiscussed canbeinterpretedasasinglemccul lochpitts unitwhichisconnect edtoninputneurons hebbianlearning probablythemostintriguingpropertyofbiologicalneura ionalities brainsadaptto theirenvironmen manypotentialformsofplasticityandmemo ryrepresen cussed intheliterature includingthechemi calstorageofinformationorlearning throughneuro genesi s n keymechanism hebbianlearning isnamed afterpsychologistdonaldhebb published originalhypothesiswasformulatedintermsofapairofneurons nectedthroughanexcitatorysynapse whenanaxonofcellaisnearenoughtoexcitecellbandrepeatedly orpersistently takespartinﬁringit somegrowthprocessormetabolicchange takesplaceinoneorbothcellssuchthata seﬃciency asoneofthecellsﬁring b thisisknownashebb slawandsometimes rephrasedas neuronsthatﬁre hebbianlearningresultsinamemo ryeﬀect favorsthesimultaneousactivityofneuro constitutesaformoflearningthroughsynapticplasticity questiontowhichextentthehebbianparadigmreﬂect sthebiological ting mechanismshavebeensuggested orkarchitectu re contextofartiﬁcialneuralnetworks hebbiansynapticplasticityprovidesavery plausiblebasisfortherepresen tationoflearninginthemodels inthemathema oussection wecanexpresshebbianlearningquiteelegantly assumi ngthatthe onlyonlocally availableinformation sb synapticneuron sa sb hypothesis symmet sa b anexcitatorysynapseconnect inga andbwouldalsobestrengthened accordingtoeq ifbothneuronsare quiescen tatthesametime sinceinthiscasesasb ityinaandlowactivityinb orviceversa withsasb excitatoryorstrengthenaninhibitorysynapse inhebb soriginalformulation onlythepresence ofsimultaneousactivityshouldtriggerchangesof theinvolvedsynapse themathema ticalformalismin facilitates thepossibilitythatanindividualexcitatorysynapsecanbecomeinhibitoryor viceversa whichisalsoquestionablefromthebiologicalpointofview manylearningparadigmsinartiﬁcialneuralnetworksandotheradaptive systemscanbeinterpretedashebbianlearninginthesense cussi includingsupervised andunsup ervisedlearning tionsofthese term notethattheactualinterpretationofthetermhebbianlearningvariesa itisempl visedlearning sincefeedba ckfromtheenvironmen tisquitegenerallyassumed ead relaxeduseofthetermforlearningprocesses whichdependonthestatesofthe frequently learningcanbeseen astheoptimizationofsuitablecostswhich areinterpretedasafunct ionofthenetworkparameters inmanycasesnumer icaloptimization procedur e whichareforinstancebasedongradientdescen t leadtoupdate rulesfortheweightsthatresem blehebbianlearningtoalargeextent intheprevioussectionwehaveconsidered typesofmodelneuronswhichretain certainaspectsoftheirbiologicalcounterpartsandallowforamathema tical formulationofneura lactivity synapticinteractions ustoconstructnetworksfrom forinstance sigmoidalormccul ron concerningthesynapticconnect orks inthefollowing turesareintroduced anddiscussed namelyfullyconnect edrecurrentnetworks possibilitiesformodiﬁcationsofthese network aswellasforhybridandintermedi morespeciﬁcarchitectureswillbeintroduced brieﬂylater attractornetworksandthehopﬁeldmodel networkswithveryhighorunstructuredconnect ivityformdynamicalsystems ofneuro nswhichinﬂuence leftpanel theactivityofaparticularneurondepends etimestepstoneobtainsanupdate oftheform si t wher ethesum euronireceivesinput canbeinterpretedasanupdateofall unitscouldbevisitedinadeterministicor randomizedsequentialorder wewillnotdiscuss thesubtle yetimportant diﬀerences betweenparallelandsequentialdynamicshere andreferthereader totheliterature fromaninitialconﬁgurations vidualactivitiess sn thedynamicsgeneratesa sequence ofstatess t whichcanbeconsidered thesystem sresponsetothe termrecurrentnetworkshasbeencoinedforthistypeof dynamicalsystem oneofthemostextreme exampleofarecurrentarchitectureis thefullyconnect hopﬁeldnetworkcomprisesnneuro nsofthemccul lochpittstypewhichare fullyconnect n whiletheexclusionofexplicit theassumpt ionofsymmet ric anotherseriousdeviationfrombiologicalreality thedynamicsofthebinaryunitsisgivenby si t johnhopﬁeld realizedthatthecorrespondingrandomsequentialupdate canbeseen orkarchitectu re s s recurrentneura withpartialconnect right patternretrieval storing pixel rightmostﬁguredisplaysthesystemshortlybeforeperfectretrievalisachieved governedbyanenergyfunct ionoftheform h s t jwijsi t sj t themathema calphysics forbackgroundandfurtherreferences ther e havebeenconsidered inavarietyofscientiﬁccontextsrangingfromtheformationofbinaryalloys toabstractmodelsofsegregationinthesocialsciences positiveweightswij thetotalenergyofthe constantpositive whilerandomlydrawn interactionsareempl oyedtomodeldisordered magneticmaterials glass intheactualhopﬁeldmodel synapticweightswijareconstructed orlearnedinordertofacilitateaspeciﬁcformofinformationprocessing fromagivensetofuncorrelated ξµ p withξµ aweightmatrixisconstructedaccordingto iξµ wher cordingtoeq wecaninterprettheweightsasempi orks vedversionsoftheweightmatrixforcorrelatedpatternsare allperceptrontrainingalgorithms discussed later couldbeapplied perneuro n inthehopﬁeldnetworkaswell dressablememo ry ifthesystemispreparedinaninitialstates si thedynamicscanretrievetheoriginalξµfromcorruptedornoisy thetemporalevolutionunder theupdates restores thepatternnearlyperfectlyands t rightpanel onalarrangemen tofneuronsservespurelyillustrative edtoeveryotherneuron neighborhood relationsdonotdeﬁne onaltopologyinthehopﬁeldmodel successful retrievalofastoredpatternisonlypossibleiftheinitialdeviation ofs onlyalimitednumberofpatterns canbestoredandretrievedsuccessful mean activitiesξµ thestatisticalphysicsbasedtheoryofthehopﬁeldmodel value notethattheweightmatrixconstruction canalsobeinterpretedas hebbianlearning startingfromatabularasastateofthesynapticstrengths withzeroweights asingletermoftheformξµ iξµ jisadded foreachactivity pattern represen tingtheneuronsthatareconnect edbysynapsewij andwji eq canbewrittenasaniteration wij wij µ pξµ iξµ j wher etheincremen talchangeofwijdepends tionandisoftheform hopﬁeldmodelservesasaprototypicalexampleofhighlyconnect ed neura themodelhasprovidedmany theoreticalandconcept ualinsightsintoneura lcomput ationandcontinuesto doso moregeneralrecurrentneura lnetworksareappliedinvariousdomainsthat includes amongothers robotics speechorhandwr itingrecognition neuralnetworks thro ughoutthisreader network architecture system neuro rangedinlayersandinformationisprocesse direction critical value isoftenreferredtoasαcintheliterature butitshould notbeconfused thestoragecapacit orwardnetworksin sec orkarchitectu re rward singlelayerofunits thetoplayerintheillustration represen tsexternalinput biologicalcontext onemightthinkofthephotorecept orsintheretinaorother senso ryneuro nswhichcanbeactivatedbyexternalstimuli thestateoftheneuronsinallotherlayersofthenetworkisdeterminedvia synapticinteractions andactivationsoftheform s k jw k ijs theactivitys k iofneuroniinlayerkisdeterminedfromtheweightedsum ofactivitiesinthepreviouslayer informationcontainedintheinput isprocessed thelastlayerinthestructure layerintheillustration represen tsthenetwork soutput input unit buttheextensiontoalayerofseveraloutputsisstraightforward essen tionprocessing neuronsreceiveonlyinput aconsequence ticalfunct ionthatmapsthevectorofinput nodesreceiveinput fromseveralpreviouslayers orinotherwords connect ion skip wewillnotconsider thisoptioninthefollowing lationislostassoonasanyformoffeedba ckispresent backwardconnect ionsfeedinginformationintoprevious higher duce feedbackloops makingitnecessa rytodescr ibethesystemintermsofits fulldynamics neuronsthatdonotcommunicatedirectlywiththeenvironmen t unitsthatareneitherinput noroutputnodes aretermed hidden unit node neuro n whichformhidden rwardarchitecture workcomprisesonelayerofhidden unit responseofthesystemtoaninput conﬁguration ξn s ξ hereweassume forsimplicity thatallhidden andoutputnodesempl oythe sameactivationfunct iong thisrestrictioncanberelaxedby deﬁni orks s ξ s ξ amultilayeredarchitecture forwardnetworkwithalayerofinput neuro n onehidden layer andasingle outputunit quantitieswkjdenotetheweightsconnect thhidden k wher ekisthetotalnumberofhidden unit altogether thearchitectureandconnect ivity theactivationfunct ionandits parameter gain thresholdetc andthesetofallweightsdeterminetheactual ξ rward theextensiontoseveraloutputunits onal funct ionvalues isconcept uallystraightforward withoutgoingintodetailsyet wenotethatwecontrolthefunct ionthatis actuallyimplemen tedbysettingtheweightsandotherfreeparametersinthe ting atargetfunct ion thetermlearningisused forthisadaptationorﬁttingprocess tobemoreprecise thissituationconstitutesanexampleofsupervisedlearning asdiscussed inthenextsection tosumma rize rwardneura eterizationofa funct ionaldependence rathermild condition rwardnetworkswithsuitable continuousactivationfunct ion thismeansthatanetworkcan approximateany licious continuousfunct iontoarbitraryprecision providedthenetworkcomprisesasuﬃci entlylarge problemdependen t berofhidden unitsinasuitablearchitecture rwardnetsinquitegeneralregressiontasks iftheresponseofthenetworkisdiscretized ingoperationthatyields s ξ c thesystemperformstheassignmen tofallpossibleinputsξtooneofccategories theauthor spersonal preference layerednetworksaredrawnfromtop input tobottom output alternativ eorientations canbeachievedbyrotating orkarchitectu re orclasses rwardnetworkconstitutesaclassiﬁerwhichcanbe adaptedtoexampledatabychoiceofweightsandotherfreeparameters rwardclassiﬁer willserveasa perceptronisdeﬁned alinearthresholdclassiﬁerwithresponse s ξ toanypossibleinput n correspondingtoanassignmenttooneoftwo classesrepresen showsthatitcan beinterpretedasasinglemccul lochpittsneuro nwhichreceivesinput fromn theperceptronwillbediscussed thatprovidesvaluableinsightsintothefunda mentalsofmachinelearning otherarchitectures fullyconnect edattractorneura lnetworksandstrictly rwardlayerednets alargevarietyofnetworktypeshavebeenconsidered anddesigned oftenwithspeciﬁcapplicationdomainsinmind rwardstructureswith forinstance layersofhighly interconnect edunitsareempl oyedinthecontextofreservoircomputing foroverviewsandreferences recently ularityinthecontextofdeeplearning poolinglayerswillbediscussed workswithspeciﬁc sedactivationfunct ionsinthehidden unit prototype regression ervisedlearningwill bediscussed learningfromexampledata youlive inthefollowingsection scena rio supervised learningandunsup wewillbrieﬂydiscuss interestingandfruitfulrelationsofmachinelearningwithstatisticalmodelling themainchaptersofthese notesdealwithsupervisedlearning withempha si concept sandmethods canhoweverbetransfer redtounsup unsupervisedlearning unsup ervisedlearningisanumbrellatermcomprisingvariousmethodsforthe targetasitwouldbethecaseinclassiﬁcationor isnodirectfeedb ackavailablefromtheenvironmen t orateacherthatwouldfacilitatetheevaluationofthesystem sperformance tationthereo fisnotavailableorpossible formoreaboutthebackground ofunsup ervisedlearning thereaderis referredtotheliterature ciﬁcalgorithms andapplicationscanalsobefound inlecturenotesbydalya baron theframeworkofunsup erviseddata edata ofunsup potentialaimsofunsup ervisedlearningarequitediverse afewexamplesbeing frequentlyitmakessense torepresen tlargeamountsofdatabyfewer exemplarsorprototypes whichareofthesameformanddimensi onasthe originaldataandcapturetheessen tialpropertiesoftheoriginal larger anotherformofunsup ervisedlearningaimsatreplacingoriginaldata onalrepresen tationswithoutreduci ngtheactualnumber ofdatapoints mappingtolowerdimensi couldbedonebyexplicitlyselectingareduced setoffeatures forinstance onal spaceorrepresen tationsthatareguidedbythepreservationofrelative distance orneighborhoodrelations onalrepresen tationsofadatasetcanbeused forthe itcanbeviewedasaspecial caseofcompressionandmanytechniquescanused addition morespeciﬁctoolshavebeendevisedforvisualizationtasksonly tion ticprocessaccordingtoamodeldensi parameter ofthedensi tyareoptimized forinstanceaimingatahighlikelihoodasa measure ofhowwellthemodelexplainstheobservations oneimportantgoalofunsup tionsintoclustersofsimilardatapointswhichjointlydisplayproperties ingisformulatedintermsofaspeciﬁc similarityordistancemeasure whichisused tocomparediﬀerentfeaturevectors theabovementionedandotherunsup ervisedtechniquescanbeempl oyed toidentifyrepresen tationsofadatasetsuitableforfurtherprocessing consequently unsup ervisedlearningisfrequentlyconsidered auseful processingstepalsoforsupervisedlearningtasks tionedhere canbecloselyrelatedand arios toseveralofthem forinstance densi turemodels gmm couldbeinterpretedasaprobabilisticclusteringmethod andtheobtainedcentersofthegmm canalsoserveasprototypesinthecontext ofvectorquantization severalrelevanttechniquesofunsup ervisedlearningarediscussed inchapter wher eadditionalreferences canalsobefound inasense inunsup ervisedlearningthere isno right wrong canbeillustratedinthecontextofatoyclusteringproblem beroffruitaccordingtoshapeandtaste wewouldmostlikelygrouppears wecan or withoutfurtherinformationorrequiremen tsdeﬁned bytheenvironmen t clusteringstrategiesandoutcomes tratesthefactthatthechoiceofhowthedataisrepresen tedandwhichtypes importantcandeterminetheoutcomeof anunsup ervisedlearningprocesstothelargestextent importantpointtokeepinmindisthat ultimately theusersdeﬁne thegoaloftheunsup ervisedanalysisthemsel formulatingaspeciﬁccostfunct ionorobjectivefunct ionwhichreﬂect sthetask andguidesthetrainingprocess selectionordeﬁni tionofacostfunct ion canbequitesubjectiveand itsoptimizationcanevencompletelyfail toachievetheimplicitlyintended goaloftheanalysis seesec discussi onofvectorquantizationasanexample asaconsequence rionandobjectivefunct ionconstitutesakeydiﬃcul tyinunsupervisedlearning asuitablemodelandmathema ticalframeworkhastobechosenthat servesthepurposeinmind supervisedlearning insupervisedlearning targetvalues dataisanalysedinordertotuneparametersofamodel whichcanbeused topredictthe hopefullycorrect targetvaluesfornoveldata thatwasnotcontainedinthetrainingset gener allyspeaking supervisedmachinelearningisapromisingapproachif thetargettaskisdiﬃcul torimpossibletodeﬁne intermsofasetofsimple rule whileexampledataisavailablethatcanbeanalysed wewillconsiderthefollowingmajortasksinsupervisedlearning inregression oftheweightofacow basedonsomemeasured featuresliketheanimal s heightandlength discussion ector ial relational orother data structur esisexcluded edata thesecondimportantexampleofsupervisedproblemsistheassignmen tof observationstooneofseveralcategoriesorclasses toverstrainedexampleisthediscrimination ofcatsanddogsbasedonphotographicimages ﬁcationtasks includingtimeseriesprediction riskassessmen tinmedicine tonameonlyafew beca usetargetvaluesaretakenintoaccount wecandeﬁne andevaluate clearqualitycriteria ofdataortheexpectedmeansquareerror mse supervisedlearningappearswelldeﬁned incomparisontounsup ervisedtasks ingfulobjectivefunct ionswhichcanbeused toguidethelearningprocesswith respecttothegiventrainingdata alsoinsupervisedlearning anumberofissues havetobeaddressed carefully toosimplistic oroverlycomplexsystems canhinder thesuccess discussed detailsofthe trainingprocedur emayinﬂuence actualrepresen tationofobservationsandtheselectionofappropriatefeatures isessen tialforthesuccess ofsupervisedtrainingaswell inthefollowing visedlearningwher e amodelorhypothesisaboutthetargetruleisformulatedinatraining forinstance rwardneura lnetwork b thelearnedhypothesis canbeappliedtonoveldatain theworkingphase aftertraining frequently anintermedi atevalidationphaseisinsertedafter inorderto estimatetheexpectedperformanceofthesysteminphase b orinorderto tunemodel parametersandcomparediﬀerentsetups infact validation constitutesakeystepinsupervisedlearning itisimportanttokeepinmindthatmanyrealisticsituationsdeviatefrom thisidealizedscena theexamplesavailablefortrainingand validationarenottrulyrepresen tativeofthedatathatthesystemisconfronted evenchangewhilethesystemistrained thisveryrelevantproblemisaddressed strategyforthesupervisedtrainingofaclassiﬁerisbasedon selectingonlyhypotheses thatareconsistentwiththeavailabletrainingdata andperfectlyreproduce atlengthinthecontextoftheperceptronclassiﬁer arios inversionspacereliesontheassumpt ionthat thetargetcanberealized bythetrainedsysteminprincipleandthat b thetrainingdataisperfectly assumpt ionsarehardlyeverrealizedin practice theconsiderationoftheidealizedscena rioprovidesinsightintohow learningoccursbyeliminationofhypotheses moreandmoredatabecomes available thatinteger numbershavetobeassignedtooneoftwoclassesdenotedas b furthermo rethatthefollowingexampleassignmen tsareprovided observationswecouldconclude forinstance aistheclassofevenintegers couldalsocometotheconclusionthatallintegersi areperfectlyconsistentwiththeavailabledata andsoaremanyothers itisinfactpossibletoformulateaninﬁnitenumberof consistenthypotheses basedonthefewexamplesgiven asmoredatabecomes available wemighthavetoreviseorextendour wouldrule outtheabovementionedconcept s whiletheassignmen tofallprimenumbers toclassbwould constituteaconsistenthypothesisnow wewilldiscuss learninginversionspaceingreaterdetailinthecontextof onlymakessense thedataitself hastobeconsistentwiththeunknownrulethatwewanttoinfer obviously simpletoyexamplealsoillustratesthefactthatthespaceofallowed hypothesis hypothesis maybearbitrarilycomplex wecanalwaysconstructaconsistentone forinstance simplytakingoverthegivenlistofexamplesandclaimingthat allotherintegersbelongtoclassa orjustaswell toclassb thisapproachwouldnotinferanyuseful informationfromthedata andsucha largelyarbitraryhypothesiscannotbeexpectedtogeneralizetointegersoutside summa generalizationbeginswherestorageends pletelymisstheultimategoaloflearning whichisinference ofuseful information abouttheunder moreformallywithrespect toneura lnetworksforclassiﬁcation theaboveargumen regression theconcept ofconsistenthypotheses hastobesoftened mentwiththedatasetisingeneralquantiﬁedbyacontinuouserrormeasure rephrasing gener alization begin wher elearning end edata themainideaofsupervisedlearningremainsthesame additional dataprovidesevidence forsomehypotheses whileothers becomelesslikely otherlearningscenarios avarietyofspeciﬁc relevantscena rioscanbeconsidered whichdeviatefrom followingexampleshighlightjustsometasksorpracticalsituationsthatrequire ypointtojust selectedreview editedvolume ormonographforfurtherreference pervisedlearning frequently onlyasubset beendevelopedwhich inasense combinesupervisedandunsup ervised techniquesinsuchsituations invariouspracticalcontexts feedba ckontheperformanceofalearning systemonlybecomesavailableafterasequence ofdecisionshasbeentaken rewardreceivedonlyafteranumberofstepsinagameorinapathﬁndi ng probleminrobotics ifthetrainingsamplesarenotrepresen tativeforthedatathatthesystem isconfrontedwithintheworkingphase adjust mentsmightbenecessa ry inordertomaintainaccept ableperformance justoneexamplecouldbe theanalysisofmedi calimageswhichwereobtainedbyusingsimilar notidenticaltechnicalplatforms driftprocesses tscanplayanimportantrole thetargetitselfcanchangewhilethesystemisbeingtrained asystem ilmessa ge forinstance hastobeadapted regressionsystemsandclassiﬁers typicallyreﬂect correlationstheyhave inferredfromthedata whichallowtomakesomeformofpredictionbased thisdoesnotexplicitlytakecausal sophisticated methodsofanalysis inthismaterial problem ofsupervisedlearninginstationaryenvironmen sume thattrainingdataisrepresen tativeoftheproblemathandandthatitis lmodelling inthesciences ithappensquitefrequentlythatthesameorverysimilarconcept s andtechniquesaredevelopedorrediscoveredindiﬀerent discipline inparallelorwithsigniﬁcantdelay certainlevelofredunda samequestionscanoccur inverydiﬀerentsettings anddiﬀerent canbebeneﬁci altocomeacrosscertainproblemsindiﬀerentcontextsandto viewthem fromdiﬀerentangles itisnotatallsurprisingthatthisisalsotruefortheareaofmachinelearning tionsfrombiology psychology mathema tic physicsandothers diﬀeren cesandcommon alities anarea whichisoftenviewedascompeting complemen tary riortomachinelearningisthatofinference say statisticalmodellingversusmachinelearning willyield numer ouslinkstodiscussi onsoftheirdiﬀerences andcommo thestatemen therestismarketing machinelearningisforbigdata conclusion msurestatisticians willbewhiningthattheydiditearlierandbetter andsimilaropinionsreﬂect acertainlevelofcompetition whichcan becounterproductiveattimes therelationbetween machinelearningandstatisticalmodellingwillbehighlightedintermsofan illustrativeexample oneofthemostcomprehensi ve yetaccessi blepresentationsofstatistical modellingbasedlearningisgivenintheexcellenttextbooktheelementsof n viewonmanyimportantmethods includingdensi tyestimationandexpectation maximizationalgorithmsisprovidedinneuralnetworksforpatternrecognition andthemorerecentpatternrecognitionandmachinelearning andlinksarenotprovided inthebestinterestoftheoriginator edata inboth machinelearningandstatisticalmodelling theaimistoextract thisisdonebygeneratingamathema ticalmodelofsomesortandﬁttingits parameterstotheavailabledata quiteoften machinelearningandstatisticalmodelshaveverysimilaror identicalstructuresand frequently thesamemathema ticaltoolsoralgorithms areused diﬀerences lieintheempha sisthatisusuallyputondiﬀerent aspectsofthemodellingorlearning gener allyspeaking themainaimofstatisticalinference istodescr ibe alsoexplainandunder usually takeintoaccountexplicitassumpt thepossiblegoalofconﬁrmingorfalsifyinghypotheses withadesiredsigniﬁcanceorconﬁdence inmachinelearning onthecontrary dictionswithrespecttonoveldata basedonpatternsdetectedintheprevious thisdoesnotrelyonexplicitassumpt ionsintermsof statisticalpropertiesofthedatabutempl oysheuristicconcept goalisnotsomuchthefaithful descr iptionorinterpretationofthedata butrather itistheapplicationofthederivedhypotheses tonoveldatathatisin correspondingperformance forinstancequantiﬁed asanexpectederrorinclassiﬁcationorregression istheultimateguideline obviously way genuinestatisticalmethodslikebayesianclassiﬁcationcanclearlybeused sophisticatedheuri ticmachinelearningtechniqueslikerelevancelearningaredesignedtoobtain insightintomechanismsunder lyingthedata veryoften odswhichcanbeused itisonlytheunder lying philosophyandmotivationthatdistinguishes thetwoapproaches inthefollowingsection wewillhavealookataverybasic illustrative problem learningtaskacoupleoftimes itservesasanillustrationof therelationbetweenmachinelearningandstatisticalmodellingapproachesand theirunder lyingconcept anexamplecase linearregres sion linearregressionconstitutesoneoftheearliest mostimportantandclearest examplesofinference asa bynow historicalapplication considerthetheoryofanexpanding universeaccordingtowhichthevelocityvoffarawaygalaxiesshouldbedirectly proportionaltotheirdistancedfromtheobserver itisveryimportanttorealize implicit assumption arealwaysmade instance choosing aparticular machine learning framew orktobegin lmodelling v gram iesasafunct tanced takenfrom notethatthecorrectunitsof topnas ﬁgureandarticle main afteredwi nhubble oneofthekeyﬁguresinmodern linear dependence oftheform mpc totheastronomyliteraturefordetails foraquickstart twomajorlessonscanbelearntfromthisexample sionhasbeenandcontinuestobeahighlyuseful tool evenforveryfunda mental scientiﬁcquestions b thepredictivepowerofaﬁtdepends stronglyon tisevidenced bythefact thatmorerecentestimatesofthehubbleconstant basedonmoredataofbetter quality mpc obviously aresultoftheform summa tionaldatainadescriptivefashionandallowsustoformulateconclusionsthat itmakesitpossible toapplytheunder wecantest conﬁrmorfalsifythemodelanditsassumpt ionsanddetecttheneed ingreaterdetail notethatthefollowingdiscussi onisbynomeansintended endreandgauss seehttps hi story itmerel yservesasarelativelysimple illustrativeexampleproblem aheuristic machinelearningapproach equation represen tsasimplelineardependence ofatargetfunct ionv d atargetvaluey ξ isassignedtoanumberofargumen tswhichareconcatenated inthestandardsettingofmultiplelinearregression asetofexamples ξµ yµ p edata fh ξ isassumed torepresen torapproximatethedependence y ξ lyingthe riosconsidered later wewillrefertothecoeﬃcientswjalsoasweightsandcombinethem ina anyoftheequivalentnotationsfor thescalarproduct willbeused notethataconstanttermcouldbeincorporatedformallywithoutexplicit modiﬁcationofeq vector withanadditionalclampeddimensi ξn wn θ anyinhomogeneoushypothesisfh ξ bewrittenasahomogeneousfunct onsforanappropriately extended inputspace t willbeused laterinthecontextoflinearlyseparableclassiﬁers aquiteintuitiveapproachtotheselectionofthemodelparameters weightsw istoconsidertheavailabledataandtoaimatasmalldeviationof fh ξµ quantifythisgoal thequadraticdeviationorsum ofsquarederror sse probablythemostfrequentlyused fh ξµ wher efh ξµ andthesum isoverallexamplesind thequadraticdeviationdisregardswhet herfh ξµ isgreaterorlowerthan tiveswithrespecttotheweightsbutisotherwi quentlyreplacethesse bythemeansquarederror mse whichisdeﬁned ofcourse alsoirrelevantfor theminimizationandthepropertiesoftheoptima necessa ryandsuﬃci entconditionsforthepresence ofa local minimumin adiﬀerentiablecostfunct ionarebrieﬂysumma rizedanddiscussed inappendix wher weconsideronlythe necessa ξµ lmodelling notethatthesse isalsoapopularobjectivefunct wecanrewriteeq andsolveitformally lefty wher leftisthe left inverseoftherectangularmatrix χ ifthe matrix onlybethecaseforp n fortheprecisedeﬁni verse asa comprehensi vesourceofinformationinthecontextofmatrixmanipulations heuristically inthecaseofsingularmatrices onecanenforcethe existence onal identitymatrixin sincethesymmet ric ativeeigenvalues thematrixonthe trainingandmoregeneralregressionlater e wewillalsodiscuss thecaseofunder inanalogytotheabove itisstraightforwardtoshowthattheresulting λcorrespondtotheminimumofthemodiﬁedobjectivefunct ion esse λ fh ξµ wehaveeﬀect ivelyintroduced apenaltyterm whichfavorsweightvectors concept isknownasweightdecay seesec notethatnearlysingularmatrices wouldleadtolargemagnitudeweights thisisourﬁrstencounterofregularization spaceinalearningproblemwiththegoalofimprovingtheoutcomeofthe trainingprocess infact weightdecayisappliedinavarietyofproblemsand readabilit y χisused instead ofξ properlycapitalized version ofξ edata willbediscussed inthecontextofoverﬁttinginneura wewillrevisitlinearregressionagaininlaterchaptersandshowthatitcan approachcircumventstheproblemofhavingtochooseanappropriateweight thestatistic almodellingperspective inastatisticalmodellingapproach weaimatexplainingtheobserveddatadin wehavetomakeandformalizecertain assumpt wecanassume thatthelabelsyµaregenerated indep enden tlyaccordingtoaconditionaldensi tyoftheform p w weassume thattheobservedtargetsessen tiallyreﬂect dence butaresubjecttogaussiannoise withindep enden t contrasttotheprevious heuri stictreatment tionforhowandwhytheobservedvaluesdeviatefromthelineardependence inthefollowing weconsideronlywasparametersofourmodel whileσ marevery wellpossiblebutnotessen tialforthecomparisonwiththeheuristicmachine learningapproach inthesimplestcaseweassume penden foragivenmodelwithweightsw thelikelihoodofobserving p χ w logp w w wher eweinsertedthegaussianmodel thesecondtermis likelihoodunder theassumpt ionofmodel areexactlythosethatminimize thesse wearriveatthesameformalsolutionasgivenin lmodelling thiscorrespondence ofthemaximumlikelihoodsolutioninthegaussian modelwithaquadraticerrormeasure isofcourseduetothespeciﬁcmathema icalformofthenormaldistributionandcanberediscoveredinvariousother assumpt ionofgaussiannoiseisrarelystrictlyjustiﬁed butit isverypopularandappearsnaturalinabsence ofmoreconcret eknowledge frequently ityieldspracticalmethodsandcanbeseen asthebasisofpopular techniqueslikeprincipalcomponentanalysis mixturemodelsforclustering orlineardiscriminantanalysis note thatthestatisticalapproachismoreﬂexibleinthesense thatwecould forinstance replacetheconditionalmodeldensi tyin analternativeassumpt ionandproceed alongthesamelinestoobtainasuitable objectivefunct ionintermsoftheassociatedlikelihood itispossibletoincorporatepriorknowledge orpriorbeliefs thatweightswithlowmagnitudeare morelikelytooccur evenbeforeanydataisobserved wecouldexpressthisin termsofanappropriatepriordensi ty forinstance po w exploitingbayestheo rem p p b p weobtainfromthe datalikelihoodp p po w withthepropernormalizationthisrepresen tstheposteriorprobabilityofweights p afterhavingseen χ y andtakingintoaccountthe dataindep enden tpriorpo w assumi ngtheindep enden ticularlyconvenientgaussianprior wecanwritethelogarithmofthe posterioras log p withasuitableparameterγthatdepends onτoandisobtainedeasilybyworking outthelogarithmofp theimportantobservationisthatmaximizingtheposteriorprobabilitywith respecttothesetofweightswisequivalenttominimizingtheobjectivefunct ion givenineq themaximumaposteriori map estimateof theparameterswisformallyidenticalwiththemse estimatewhen amended isingly manydiﬀerentnames havebeencoinedforthisformofregularizationanditsvariants regularization ression theabovediscussed maximumlikelihoodandmapresultsareexamplesof oneparticularsetofmodelparameters w selectedaccordingtothespeciﬁccriterioninuse thestatisticalmodellingidea allowsustogoevenfurther intheframeworkofbayesianinference edata wecanconsiderallpossiblemodelsettingsatatime yieldingtheposterior predictiveprobability p d p w p po w dnw properlynormalized thisdeﬁnes theprobabilityofresponsey ξ toanarbitrary novel input ξafterhavingseen overthespeciﬁcmodelresponsesp w givenaparticularw butintegrated overallpossiblemodelswiththeposteriorp asaweightingfactor formalismyieldsaprobabilisticassignmen tofthetargety whichalso makesitpossibletoquantifytheassociateduncer fthisconstitutes oneresortsto convenientparametricformsofmodelandpriordensi handle approximationsoftheposteriorpredictivedistribution conclusion niﬁcantlyintermsoftheirconcept ualfounda inpractice diﬀerences seemi ticmethodsofmachinelearningcanbederivedfromthestatisticalinference perspectiveunder suitablemodelassumpt trainingalgorithms canbeused forthemaingoalsofmachinelearning sophisticatedmachinelearningtechniquescan alsoaimatunder standingandexplainingtheobserveddata agoalwhichis frequentlyattributedtostatisticallearning insumma ry theauthor srecommenda tionistoacquireknowledgeofboth usethebestofbothcomplemen taryframeworkswithoutbeingdogmaticor religiousinpreferringoneovertheother ngdebatesandclaims ofsuperioritybybothcommunitiesareessen eﬀortsshouldbecombinedinordertoachieveabetter theperceptron theperceptronhasshownitselfworthydespi te andevenbecauseof itssevere itslinearity itsintriguing learningtheorem tation termperceptronisused ughoutthistext itwillexclusivelyrefertoasystemrepresen alayerofunits whichareconnect edtoasinglebinaryoutputunitofthe mccul ξ istakentorepresen ttwo thatdoesnotcomprisehidden unitsorlayers si lda thenaivebayesclassiﬁer andthepopularlogisticregression lr classicalmethodsarebasedonconcept sofstatisticalmodellinganddiﬀer mostlyinthewaytheirparametersaredeterminedorconstructedfromagiven dataset intheliterature tinuousoutputareoftenreferredtoas multilayer soft anybinar youtput servethesame purpose unit butinthese lecturenotesthetermperceptronalwaysrefers tothesingle layer binaryclassiﬁer eventheverysimple limitedperceptronarchitectureisofinterestfora multitudeofreasons theperceptronhasbeen oneoftheearliest verysuccessful machinelearningconcept sanddevices anditwasevenrealizedinhardware whichis guaranteedtoconverge spondingperceptronconvergence theo remisoneofthemostfunda talresultsinmachinelearningandhascontributedlargelytotheinitial popularityandsuccess powerfulsystems asmanfredopper putit perceptronis thehydrogenatomofneura concept uallyextended thesupp tormachine svm theperceptron persiststobeused successful lyinalargevarietyofpracticalapplications thepreciserelationofthesvmtothesimpleperceptronwillbediscussed historyoftheperceptronprovidesinsightsintohowthescientiﬁc communitydealswithhighexpectationsanddisillusionmen tsleadingto severaloriginaltextsfromtheearlydaysoftheperceptronareavailable highlyinterestingoﬃcialmanualoftheperceptronmarkihardware androsenblatt smonographprinciplesofneurodynamics estingtvdocumen tationisavailableat impactonthemachinelearningcommunityisanalysedinanarticleentitleda sociologicalstudyoftheoﬃc ialhistoryoftheperceptroncontroversybym ozaran clearpresentationsoftherosenblattalgorithmcanbefound invirtually onswhicharequiteclosetothese lecturenotes apartfromnotationissues hertz instance countingargumen tforthenumberoflinearlyseparablefunct ion sec itshouldbeuseful n hardwarerealizationatcornell withkindpermissionfromcornell input photosenso tedbypotentiometersthatcould betuned schema ture basedonaﬁguretakenfrom triangular shadedregion added totheoriginalillustration marksasubset ofunitsthatiscommo nly referredtoastheperceptroninthistext latterworkalsopresentsthe extensionofthecountingargumen machine withk hidden unit responseofaperceptronwithweightvectorwisobtainedbyapplyinga thresholdoperationtotheweightedsum ofinputs sw θ ξ wher ticalstructuresuggestsanimmedi setofpointsinfeaturespace division lections input weight output onal inputsandabinaryoutputofthemccul lochpittstype correspondstoa θresultinperceptronoutput whilevectorsξ theperceptronrealizesalinearly separable funct ion featurevectorswithperceptronoutput separatedbythehyperplane twocasescanbedistinguished funct ionscanbewrittenas sw ξ forthelatter thecorrespondinghyperplane hasnooﬀset andincludes theorigin inthefollowing tions tialrestrictionbecauseany ioncanbeinterpretedasahomogeneousonein ahigherdimensi onalspace considerthefunct ionsw θ ξ withw themodiﬁed onalweight vector wn θ andaugmentallfeaturevectorsbyanauxiliary clamped input dimensi ξn weobservethat asaconsequence writtenasanadditionalweightinatriviallyaugmentedfeaturespaceand formally thetwocasescanbetreatedonthesamegrounds gumen tisanalogoustotheformalinclusionofaconstantterminmultiplelinear regression seeeq wewillencountersubtletieswhichrequiremore preciseconsiderations butfornowwewillrestrictourselvestohomogeneous funct ionsandsimplyrefertothem aslinearlyseparableforbrevity oftheplane fromtheoriginisexactly θincaseofnormalized wwith n θ terpretationoftheperceptron thehyperplaneorthogonaltow θfromtheorigin separatesfeaturevectorswith respectively inthefollowing ξµ sµ t p homogeneously linearlyseparable ifatleastoneweightvectorwexistswith sµ p wher eweusetheshorthandnotationsµ ξµ withthesubscri pttfortargetortraining itycometomind givenalinearlyseparabledataset canweﬁndaperceptronweight vectorwthatsatisﬁes givenanarbitrarydatasetwithbinarytargetlabels canwedetermine whet heritislinearlyseparablewithout attempt ingtotraina perceptron arethere eﬃcientiterativealgorithms whichindicate atan earlystage thatadatasetisnotlinearlyseparable howseriousistherestrictiontolinearseparabilityinthespaceofbinary targetfunct ion howmanylinearlyseparablefunct ionsexist vectorsinndimensi ons canwelearnanunder tainedind howdoestherealizationor storage ofthegivenlabelsin drelatetothelearningoftheunknownrule whichoneisthebest wha tisameaningfulmeasure ofqualityand howcanthecorrespondingoptimalweightvectorbefound ifdisnotseparable canwestillapproximatethetargetclassiﬁcationby meansofaperceptron whichalternativesorextensionsexist mostofthese question willbeaddressed comingsections classiﬁcationschemes beyondlinearseparability toalargeextent thesuccess oftheperceptronhasbeenduetotheexistence atrainingalgorithmandtheassociatedconvergence theorem bothpresentedby frankrosenblatt weﬁrstpreciselydeﬁne basicgoalofthetrainingprocess outlinethegeneralformofiterativeperceptron algorithm andpresentrosenblatt salgorithm asakeyresult wereproduce thecorrespondingproofofconvergence eventually theperceptron storageproblem hereweaddressquestion weconsiderthetaskofreproducingthelabelsofagivendatasetofform psp perceptronstorageproblem ξµ sµ t p thetermstoragerefers tothefactthatwearenot aimingattheapplication ofthefunct ionsw ξ thecorrectassignmen toflabelswithinthedatasetbymeansofaperceptron thisaimcouldbeachievedbystoringdinamemo ry tableandlookupthecorrectsµ twhen needed inordertorewritethepsp wenotethatsign thetermindicatesavaguerelationto themem branepotentials p weobtainanequivalentformulationofthepsp intermsofasetofinequalities perceptronstorageproblem ii ξµ sµ t p wehaveintroduced aconstantc eµ tiallyirrelevant considervectors eµ theexistence ofthe factthatthefunct ionsw ξ onlydepends onthedirectionofw iterativehebbiantrainingalgorith m inordertoanswerquestion insec gorithms ξν t sν t attimesteptofthetraining process thesequence tation ν t wher eeachloopthroughtheexamplesindiscalledanepochintheliterature afrequentlyused alternativeisrandomsequentialpresentation wher eateach thespeciﬁcformofperceptronupdatesweconsiderinthefollowingis genericiterativeperceptronupdates weight atdiscretetimestept ν t ofthecurrenttrainingexample ethelocalpotentialeν t t t sν t t w t nf eν t ξν t sν t scalingoftheupdatewithnisarbitraryandisused sistency intoapracticaltraining algorithm theprescriptionhastobecompletedbyspecifyinginitialcondition w andbydeﬁni ngastoppingcriterion obviously togetherwiththedeﬁni tionofthesequence ν t funct ionf determinestheactualtrainingalgorithmandweassume thatitdepends constitutesarealizationofhebbianlearning ponentwjoftheweightvectorisproportionaltothe pr input ξµ jand outputsµ asaconsequence theweightvectoraccum ulateshebbiantermsξµsµ ingfromthegiveninitializationw calledtabularasainitialization afterperforming thiscanberepresentedbythefunction ν t p τtheweightvectorisboundtohavetheform w τ τ ξµsµ τ quantifytheirspeciﬁccontributions assumi ngthatw itisalsopossibletorewritetheupdate theactual weightvectorcanbeconstructedaccordingtoeq thefollowing formulationisequivalentto genericiterativeperceptronupdates embedding strength atdiscretetimestept ν t ofthecurrenttrainingexample ethelocalpotentialeν t t t sν t t t accordingto xν t t t eν t allotherembeddingstrengthsremainunchangedattimet manyperceptronalgorithms canbeformulateddirectlyintheweights cf orintermsoftheembeddingstrengthsasin equivalent wenotethatthenumberofvariablesused torepresen tronduringtrainingisnintheweightvectorformulationandpifembedding thecomput ationaleﬃciency oftrainingandthe correspondingstorageneeds inpractice notethatthesimplecorrespondence canbelostif thestructureoftheactualupdatesismodiﬁed areimposed forinstance intheadatronalgorithm forthe perceptronofoptimalstability seesec formulationofthetrainingscheme onecanalwaysconstructtheweightvectorviarelation therosenblattperceptron algorith m intermsofthegenericalgorithm rithmisspeciﬁedby w equivalently xµ p cyclicpresentationoftheexamplesindaccordingto f eµ c withtheheavisidefunct ionθ x x insumma ry theprescriptions rosenblattperceptronalgorithm atdiscretetimestept ν t ofthecurrentexampleaccordingto ethelocalpotentialeν t t t sν t t w t nθ t ξν t sν t t equivalently incremen tthecorrespondingembeddingstrength xν t t t t allotherembeddingstrengthsremainunchangedattimet update modiﬁesw t onlyiftheexampleinput ismisclassiﬁed bythecurrentweightvectororcorrectlyclassiﬁedwitheν t ahebbiantermisadded quantityxν t remainsunchangedorincrea s fortabularasainitialization ativeintegers frequently andtheresultingscheme isoftencalledthe rosenblatt perceptronalgorithm theunder lyingprinciple ofaddingahebbiantermformisclassiﬁedexamplesisreferredtoas learning frommistakes discussed forthcomingsections classiﬁed theevaluationofthemodulationfunct ionyieldsθ forthcomingupdatesteps algorithmisillustratedintermsofasimple onalfeaturespaceandadatasetdcomprisingsixlabeledinputs w w w zeroupdate w w zeroupdate w w rosenblattperceptronalgorithm illustrationofthetrainingscheme onal featurespace y circlesrepresen ﬁlledcirclesmarkdata tabularasa onlyoneepochoftrainingisconsidered withν t step byaddingahebbiantermifexampletismisclassiﬁed itremainsunchangedifthecurrentclassiﬁcationiscorrectalready timesteps aregivenbytheaddition st orsubtraction st nξtwhichis resultingweightvectorisshown allexamples thedatasethasto dcorrectly andthealgorithmstopsalreadyafteronesweepthroughthedata set wewillshowinsec converges inaﬁnitenumberofstepsandﬁnds tronstorageproblem providedthegivendatasetisindeed linearly separable theperceptron algorith masgradientdescent wehaveintroduced therosenblattalgorithmmoreorlessintuitivelyasaformof wecanmotivateitastheminimization ofthecostfunct ion e w w t t whichiswrittenasasumovercontributionseµofthegivenexamplesinthedata examplespeciﬁctermiszeroifthedatapointisclassiﬁedcorrectly t canditcontributesthelinearcosts t se notethattheobjectivefunct ion isfrequentlyreferredtoasthehingeloss seeforinstance strictlyspeaking thefunct ioneisnotdiﬀerentiablewher tyonecanresortto radientmethod amoredetaileddiscussi wecanignorethis subtletyhere anddeviseagradientbasedminimizationasoutlinedinappendix nθ t ξµsµ t theupdate canbewrittenas w t t t wher ethecurrentlyconsidered exampleisgivenbyν t accordingto updatestepalongthenegativegradienttends todecreaseeν t totalcostfunct ione wewillencounteranumberoflearningalgorithms inquitediversesettings whichareguidedbythegradientofanobjectivefunct ionwithrespecttothe entiredatasetorsingleexamplecontributionsasabove ageneraldiscussi rosenblattalgorithmresem blestheminimizationofe w ticgradientdescen t sgd whichispresentedanddiscussed moregenerallyin theoriginal prescriptionassuggestedbyrosenblattanddiscussed inthedeterministicsequentialorder dingstrengthshasthestructureofcoordinatedescentasdiscussed inappendix ateverystep itspotentialincremen t anotherimportantdiﬀerence tothegenericsgdisthat duetothespeciﬁc structureoftheproblem itisnotnecessa rytointroduce unealearning rateinordertoachieveconvergence infact onecanshowthatfortabularasa initializationw aconstantlearningrateηasin w t t t t nθ t ξν t sν t t wouldsimplyrescaletheresultingembeddingstrengthsandthusalsotheweight vectorbyafactorηascomparedto vergence behaviornortheresultingclassiﬁcationscheme note theinﬂuence enden tη t seeforinstance whiletheinterpretationasagradientbasedmethodhelpstorelatethe rosenblattalgorithmtoothermachinelearningschemes wewillnotmakeuse gence canbeshownexplicitlywithoutreferringtogradientdescen theperceptron convergen cetheorem theperceptronconvergence theo remisoneofthemostimportant funda oftherosenblattperceptron algorithmforlinearlyseparableproblemshasplayedakeyroleforthepopularity oftheperceptronframework perceptronconve rgencetheorem pct ξµ sµ t p therosenblattperceptron algorithmstopsafteraﬁnitenumberofupdatesteps inthefollowingweoutlinetheproofofconvergence weconsideralinearly ξµ sµ t perceptronstorageproblem existswith sign equivalently forsomepositiveconstantc toworkoutthederivativeexplicitly infact foragivendthere bemanysolutionsoftheform buthere itissuﬃci enttoassume existence redenoteitssquarednormas notethatanypairofvectorsw w asdiscussed oftheform w t t ξµsµ tforw intherosenblattalgorithmthequantityxµ t isanintegerthatcountshow oftenexampleµhascontributedahebbiantermtotheweights updatesis therefo givenby m t t nowletusconsidertheprojectionr t t ting ploitingthecondition weobtainthefollowinglowerbound r t t t t ncm t similarly weconsiderthesquarednormq t t t ofthetrainedweight t itchangesas q w t t ξν t sν t t t eν t t t inanyﬁnitedatasetd cantherefo realwaysidentifythequantity nmax wher ethescalingwithdimensi x x rewenotethat alsoconsider thesimpler lessgener alcase ofnormalized input θ t t learningstep θ t t learningstep asaconsequence wecanreplacealleν t bycineq toobtaintheupper bound q t ncθ t nγθ t oupdateswithθ takingintoaccounttheinitialvalueq wecanconcludethat q t n m t wher em t ry tainedthetwobounds r t ncm t q t n m t wecanwrite w t t t t t n m t m t weconcludethat m t wher etherighthandsideinvolvesonlyconstants nandγareobtaineddirectly inanycase eq olearningstepsremainsﬁnite ifa learningsteps stepmustoccur ineveryepoch thedependence vesfurtherattention inthe theupperbound appearstodiverge ifd islinearlyseparable whichwe alreadydiscussed inthecontextofeq theupperboundbecomes lim wecanexpressthepct afewrema rks thenumberoftrainingsteps accordingtothepct therequirednumberoftrainingstepsisﬁniteforlinearly theiractualnumberdepends tiesofdandcanbeverylarge foradiscussi onofthecomput ational complexityoftherosenblattalgorithmandfurtherreferences moreeﬃcientalternativestothesimplerosenblattalgorithmcanbedevised forinstancebasedonlinearprogrammingmethods asdiscussed fortheconvergence proof wehadtoassume theexistence ofasolution perceptronconvergence theo remdoesnotprovide directinsightintothealgorithm considertheproblemofﬁndingapproximativesolutionswithminimumorlow existenceofasolution inpractice itturnsoutdiﬃcul ttodecidewhet heragivendislinearlyseparable ornot cf afteranumberofsteps thiscouldimplythatitsimplyshouldberunformore stepsorthat asolutiondoesnotexist atheoremborrowedfromthetheoryofdualityinoptimization providesasurpri earseparablity whichiscloselyrelatedtofarkas lemma asgordan stheoremofthealternativeintheliterature notationfamiliarfromtheprevioussectionsitreads gord stheoremofthealternative exactlyoneofthefollowing statemen tsistrue µ p ifthematrixχcomprisestheorientedinput vectorsofagivendatasetd t t ξpsp wecaninterpretwastheweightvectorofaperceptronandstatemen t correspondsto homogeneous ontheotherhand statesthatalinearcombinationoftheform p tcolumns ξµsµ tofχ inequalitiesrequirestheexistence ative alsointermsof ofthesymmet ric makeuseofthematrixcfrequentlyinlatersections inpractice checkingthevalidityofcondition ingordan stheo remmay bequiteinvolved comput trivial ingeneral learningtheunlearnable inthiscontext severalalgorithms havebeensuggestedthatconvergeforboth ein problem aconsistentperceptronvectorisfound unlikethe rosenblattalgorithm thealgorithmterminatesalsointhepresence reference toother similarconcept sareprovidedandthesuggestedscheme comparedwithtechniquesbasedonlinearprogramming existence ofsuccessful trainingalgorithms inother word howserioustherestrictiontolinearlyseparablefunct ionsis linearseparabilitydepends ondetailsoftheindividual surpr isinglygeneralqualitativeandquantitativeresultscanbeobtained whichrequireonlymildassumpt ionsaboutthedata thenumberoflinearlyseparabledichotomies inthissectionweaddress p n oflinearlyseparablebinaryclassassignmen onalinput isingly thisispossibleunder rathermildconditionsontheinput data ther earetwokinds ofpeople coucheverything indichotomies andthose ofahyperplane mensi onalinputsξcanbeseparatedintotwoclassesby theorigininc indep enden tofp thederivationforgeneralpandnhasbeenpublished severaltimesinthe literature andwasalsoreviewedin followthepresentationin theresult onalgeometry obtainedbytheswissmathema ticianludwi ready morerecently pointofviewinthecontextofdeepnetworks relatetomoregeneraltheorems partitionsofspacebyhyperplanes speciﬁcally countingthenumberofresponseregionsinlayerednetworkswith piecewi selinearactivationfunct ionsleadstoverysimilarconsiderationsand result someresultsforsmallnorp weﬁrstaddressstraightforwardcasesinvolvinglowdimensi obviousresultisthat c n byaahyperplanethroughtheoriginandsettingtheorientationcorresponding whichcouldalways thesecondobservation c reﬂect mensi theoriginintwoways ξ ξ dichotomiescanberealized panela havetoexcludeaspecialcase ifthetwoinput vectorsarecollinearwiththe origin itisimpossibletoseparatethem byalinerepresen tingahomogeneously linearlyseparablefunct ion panelsb c panel evenfor genericdatasets inpanel b oftheeightpossibledichotomies thetwocases b c d panel onalfeaturevectors largeﬁlled circle ingeneralposition smallﬁlledcircle fourlinearlyseparabledichotomiesexist eitherbothinputsareassigned panel b c eachonerepresen tedby b thetwo inpanel c therightmost d asindicatedbythe dottedline consequence ionscanbefound inpanel c oneofthedatapointscannotbeseparatedfrom theothertwo whichalsoexcludes c theresultisonlyvalidifnosubset oftwodatapointsfallsontoaline d onlyfour ion asitisnotpossible theycanonlybeassignedtothesameclass asforinstancebythetwodecision bounda riesshownintheillustration similar onaldata onehastoexcludespeciﬁc conﬁgurationsinwhichtwoormorefeaturevectorsarealigned subset onalplanethat includes theorigin generalnandp onal space itisindeed possibletoobtainc p n generalnandpunder rathermildassumpt collinearinput weformulatetheconditionthat thedatasetshouldbeingeneralposition ofahyperplane allseparating toflabels generalpositioncondit ion ifeverysubset ξρ k tsislinearlyindep enden forp n obviously subset sofmorethannelemen tsinndimensi onsare alwayslinearlydependen thegeneralpositionconditions requiresthatthevectorsinpdonotformmorehyperplanesthannecessary inasense thegeneralpositionconditioncorrespondstotheabsence isfrequentlyproneto itisevenassumed andexploitedthatnominally onalmanifoldsdueto correlationsandinterdependenci e inthemodellingandsimulationoflearningprocesses investigating trainingalgorithmperformances oneoftenresortstorandomizedfeaturevectors ofnindep enden tcomponents popularexampleofzero unitvariancegaussianrandomnumbers forinstance leadstodatasetsthat areingeneralpositionwithprobabilityone forthefollowingargumentweconsideraﬁxedsetofpinput nofthese featurevectorsassignsasetof binarylabels dp n respondtolinearlyseparablefunct c p n asanillustration dimensi sethasbeenlabeledintwodiﬀerentways bothofwhichare linearlyseparable tothedatasetoffig funct ionasintheleftpanel orthelabelisambiguous shownintherightpanel throughtheoriginthatfallintotheshadedregionwouldseparatethetwoclasses asrepresen tedbyﬁlled versusempt ycircles nowweassume tothedata set tiallydiﬀerentsituations canoccur intheleftpaneloftheﬁgure allperceptronsthatseparatethe thenewdatapoint onthecontrary intherightpaneltheresponseisambiguouswithrespectto theadditionalfeaturevector z p n thenumberofambiguous e p n dichotomiesdp wenoteimmedi atelythatthetotalnumberoflinearlyseparabledichotomiesis c p n p n p n obviously ncontributesexactlyonelin n eachofthez p n ambiguouslabellings vector wehave c n p n p n p n p n unfortunatelythisisnotyetauseful recursionbecausez p n isnotknown edbythegerman ofahyperplane h h projectionofdataintotheauxiliarysubspa eitherlinearlyseparableinh rightpanel ambiguouscase ornotseparableas leftpanel respectively howeverthefollowingconsiderationshowsthatz p n canberelatedto theauxiliary onalsubspa ceh hfallsintothegrey shadedareaofseparatinghyperplanes whileintherightpanelitdoesnot weprojectthepfeaturevectorsintotheauxiliaryspacehasshown iﬁedbytheleftpanel theprojectionsofthep featurevectorsintothe theirlabelscorrespondtoalinearlyseparable dichotomyofthepexamplesin dimensi ons oncanshowthatthere isindeed betweenthe z p n ionsandalllinearlyseparabledichotomiesin ngthatthere isnothingspecialabouth weconcludethatz p n c p weobtaintherecursion c n p n p theconditionis infact thatthedatapointsareingeneralpositionasdeﬁned abovein forinstancetheadded datapointwouldfallontoalinewith anotherdatapointandtheorigin clearlyhcouldnotbeconsidered ageneric subspa wecanassume thathhasthe samepropertiesasany cewould thenumberoflinearlyseparabledichotomies itisstraightforwardtoshowthatthefollowingexpressionsatisﬁes therecursion andmatchestheabovegiveninitialvaluesc n c p n forp n fraction p n separablefunct ofn c p n k expressioninthesecondlineof wouldalsoreproduce p n ofthetwocasesin isonlyprovidedforthesakeofclarity theproofcanbedonebyinduct ionandexploitsthat forarbitraryi ther efore c n accordingtoeq satisﬁes c p c p n whichcorrespondstotherecursionrelation insteadofthenumbersc p n themsel f wecanalsoconsiderthefraction oflinearlyseparabledichotomies pls p n p n forp n thisallowstocompareandrepresen ttheresultsgraphicallyfordiﬀerentnas setofrandomlyassignedlabels p discussionoftheresult fortheaboveargumen tsandderivationitisessen tialtoassume thatthedata violationsofthisconditionwillreduce theresultsgivenineqs canbeinterpretedasanupperboundeveninmorerealistic ofahyperplane p n asafunct forselectedvaluesofninonegraph isthatc p n arelinearlyseparable aslongasthenumberoffeaturevectorsdoesnotexceed theirdimens thestatemen thediscussi panela forp n thefractionoflinearlyseparabledichotomiesdecreasesand approachesitslimitingvaluepls α fordimensi withincrea sing isgivenas inhighdimensi ons linearlyseparabilityisguaranteed withprobability whichmotivatestodeﬁne storagecapacityoftheperceptron ingeneral thestoragecapacitiesofmorepowerfulnetworkarchitecturesare seeforinstance forallnandpintheperceptronwehaveobtainedtheevenstrongerresult latterisoftenreferred vc dimensi onoftheperceptron foradiscussi onandfurtherreferences atﬁrstsight thestoragecapacityoftheperceptronorotherstuden tsystems relatesonlytotheirabilitytoreproduce agivendatasetandimplemen tthe aswewillseeinthefollowing storagecapacity onplayanimportantrolewithrespecttothegeneralization abilityandlearningofarulefromexampledata time forapizzaorsome cake toapopularmathema ticalpuzzl eknownasthelazycaterer sproblem onal pizza orapancake dependingonculturalbackground isgiven c pizzaconnnect ion kplanarcutsthroughthecenter e leftpanel throughthe ceofeachﬂattenedhemi spher e rightpanel onalobjectsisknownasthecake number cut c infact onecanshowthattherecursionrelation c n k n k albeitwithdiﬀerentinitialvalues fortheproofofasimilarresult wecandirectlyrelatethec p n withthegeneralizedcakenumbersc k n whichiscut correspondtofourfeature vectorsanddividethenormalizedweightspaceintoregionsrepresen markedbytheredcircle onalsimpliﬁedrepresen tationofthesituationisshownintheright panel whichdisplaysaschema ticprojectionontotheselectedequatorialplane apparently terofaspher onal pizzawhichrepresen tsoneofthehemi spher sameholdsforthelower hemi spher thisyieldsthesimplerelation c p n obviouslyitisnottheultimategoalofperceptrontrainingtoreproduce labelsinagivendataset storingdinmemo ryandlookitupwhen needed ingeneral itistheaimofmachinelearningtoextractinformationfrom givendataandformulate parameterize theacquiredinsightasahypothesis whichcanbeappliedtonoveldatanotcontainedindintheworkingphase addressing fromsec weassume thatanunknown rablefunct ionorruleexists whichassignsanypossibleinput thebinaryoutputsr ξ wher ethesubscri ptrstandsfor rule ξµ sµ t shouldinfersomeinformation abouttheunknownsr ξ aslongasthetraininglabelssµ tarecorrelatedwith thecorrectsµ ξµ sµ weassume ξµ sµ r p forinstance notcontainmislabeledexampledataandisnotcorruptedbyanyformofnoise intheinput oroutputchannel wewillusethenotationsµ rorsr ξµ forlabelswhicharegivenbya rule thisindicatesthattheexamplesindrepresen talinearly separablefunct ion ξµ sµ t canalsobelinearlyseparablewithoutdirectcorrespondence ofthesµ ttoa rablefunct ionorexamples corruptedbynoisecanbeverywelllinearlyseparable itisnotaprioriclear thataperceptronisabletoreproduce thelabelsindcorrectly sincethetarget mightnotbelinearlyseparable tofurthersimplifyourconsiderations werestrictourselvestocases inwhich theruleisindeed givenbyafunct ionoftheform sr ξ woulddeﬁne thesameruleand therefo wewillassume withoutlossofgenerality rereferredtoas theteacherperceptron orteacher forshort andcanbethoughtofasproviding thetrainedperceptronwith adaptiveweightswwillbetermed thestuden character istic traitofmanyteachers atleast intheir gener scena sity mentbetweenstuden tvectorwand redshadedarea w w choiceofaspeciﬁcstuden tweightvectorcorrespondstoaparticular hypothesis whichisrepresen tedbythelinearlyseparablefunct ion sw ξ extensivelytomodelmachine learningprocesses aimingataprincipledunder nomena convenientlyallowtocontrolthecomplexityofthetargetrule thusenablingthesystematic studyofavarietyofsetups inthefollowingsectionswewillconsideridealizedsituations dentandteacherbothrepresen tlinearlyseparablefunct dition aplausibleguidelinefortrainingthestuden ξµ sµ r istoachieveperfectagreemen twiththeunknownteacherintermsofthegiven theagreemen tshouldnotonlyconcerndataindasinthe storageproblem itshouldextendorgeneralizetonoveldata ideally frequently ofsuccess ofthelearningprocess inpracticalsituations itwouldcorrespondtothe performanceofthestuden twithrespecttonoveldata forinstanceinatest setwhichwasnotused wecanrevisit thebasicgeometricalinterpretationoflinearlyseparablefunct displaysastuden obviously thatitcanbeinterpretedasrepresen ting onalsubspa cespanned onalvectorsw generally weassume thatatestinput ξisgeneratedaccordingtoanisotropic structureddensi tyanywher t sign betweenstuden tandteacherisdirectlyproportionaltotheareaoftheshaded segment w v tionintermsoflabeledinput left asingle labeledinput asetofplabeledinput vector deﬁnes darker regionv ofallweightvectorswwithcorrectresponse sw ξµ thevectorsξµarenotshown orthogonalstuden whichcorresponds torandomlyguessi latterstatemen tholdstrueindep enden tofthestatistical propertiesofthetestdata obviously learninginversionspace intheclassicalsetupofsupervisedlearning thetrainingdatacomprisesthe itappearsnaturalto requirethatthehypothesisisperfectlyconsistentwithd isthisapromisingtrainingstrategy inotherwords canweexpect toinfer just solvingthe perceptronstorageproblemwithrespecttod inordertoobtainanintuitiveinsight interpretationoflinearseparability leftpanel wecaninterpret ahyperplanethroughtheoriginofthisspace whichseparatesweightvectors ξ product andsw ξ giventhecorrecttargetlabelsr ξ theplane onalweight space consequently asetdofplabeledfeaturevectorsdeﬁnes aregionorvolume ofvectorswwhichreproduce sw ξµ ξµ p v illustrationofperceptronlearninginversionspace left tersecttheversionspacev allweightvectorsinv hyperplaneassociatedwith consequently v correspondstoeitherthesmall triangularregion green ortheremainingdarkarea dependingontheactual rightpanel setofallperceptronweightvectorswhichareconsistentwiththep examplesind p istermed version spaceandcanbedeﬁned µ rforall ξµ sµ r inthefollowing wewillusethenotationv p ifwewanttorefertothenumber ofexamplesindexplicitly inthecontextoftheprevioussection c p n ineq pretedasthenumberofdiﬀerentversionspacesthatcanbeconstructedfora setofpinput onalspacebyassigninglinearlyseparable itisimportanttonotethatdeﬁni ngvasasetofnormalizedvectorswith thenormalizationisirrelevant withrespecttotheconditionssign randcouldbereplacedbyany otherconstantnormorevenbeomitted y ifandonlyifdislinearlyseparable ifa normalized sentedbytheexamplesind necessa atleastthe inabsence ofany informationbeyondthedatasetd e invwithequallikelihood thetermlearninginversionspacerefers potheses givenasetdofpreliableexamplesforalinearlyseparablerule wehave orem thisisalwayspossible onalillustration rightpanel meansthatwecanalwaysplaceastuden tvectorwsomewhere inthedarker regionrepresen tingv considerafourthlabeledexample r leftpanel spacev spacewithrespecttothenew exampleandyieldthesameperceptronoutput sign thisimpliesinturnthat iftheextended dataset ξµ sµ r separable rispossible whichmustbe versionspacewithrespecttotheextended datasetremainsthesameasforthe ourstrategyoflearninginversion spacedoesnotrequiremodifyingthehypothesisorselectinganew studen t r situationisdiﬀerentinthecaseillustratedintherightpaneloffig thedatasetisamended r withtheplaneorthogonalto tsofv ononesideofthehyperplane correspondtotheperceptronresponsesw whiletheothers yield sw roftheadditionalexample theversion spacev correspondstoeitherthegreenareaortheremaininglighterregion theextended dataset ξµ sµ r separableandthenew versionspacev ofconsistentweightvectorsisbound tobesmallerthanthepreviousv volume ofconsistentweight vectorswshrinksduetotheinformationassociatedwiththenewexampledata asweaddmoreexamplestothedataset thecorrespondingversionspace onecanshowthatvwill therathermildconditionofgeneral positiondiscussed intheprevioussection p foranyp sing w forunno rmalizedvectorsw wecanexpectthatlearninginversion spaceyieldshypotheses whichagreewiththeunknownruletoalargeextent cussed generalizationerror subseq uentnormalization inordertomatc hthedeﬁnition precisely generalization arablefunct ion tively fromlefttoright generalizationbeginswherestorageends theabove elemen cretemathema ticalrelationswhich forinstance quantifythegeneralization errorasafunct ionofthetrainingsetsizep wher ewe countedthenumberoflinearlyseparablefunct ionsofpfeaturevectorsinn dimensi onsunder quitegeneralassumpt ion assume thatthetrainingprocess hasexploitedpexamplesforalinearly separableruleandthatwehaveidentiﬁedastuden tweightvectorinv p correspondstooneofthec p n linearlyseparabledichotomiesofthedata implicitly weassume thatthetruerulerepresen tsanyofthec p n linearlyseparablefunct weighteverydichotomywiththe volume oftheassociatedversionspace sionofthissubtletyandrefer thereadertothemorespecializedliterature nowassume thatwepresentanovelinput vectorξtestwithtargetlabelstest tothestuden theperceptronoutputstest p providethecorrectresponse iftheversionspaceisambiguouswithrespectto ξtest afractionoftheweightvectorsinv p willbecorrectwhiletheremaining efore inabsence ofmoredetailed knowledge p willbeincorrect asaconsequence ofambiguousdichotomiesandweobtain p n c p n p c p n withc p n fromeq ionof tinput dimensi ons linearseparabilityforp n leftpanel dimensi onswithall assume asanexample correct normalized forsigniﬁcantdisagreemen tbetweenstuden n ﬁndingindicatesthatforrelativelysmalldatasets theperceptronoutputisno betterthanunbiasedrandomguessi ngby forinstance ﬂippingacoin welearnfromtheabovecountingargumen tthatstoragewithout versionspaceforp nallowstoplacethestuden t vectorveryfarawayfromthe unknown teachervectorinv p withoutcausing disagreemen possibleinanextremesetting result indicatesthatforarandomlyselectedstuden tinsidethe storagewithoutlearning onenk tothestoragecapacity onthepositiveside wealsoseethatfordatasetsizeswhichexceed storagecapacity assoonasthenumberofexamplesexceeds thestoragecapacity dimensi onforﬁniten p ﬁndingsconﬁrmtheintuitionthatlearningandgeneralizationisnot tsystem canimplemen tlargesetsofexampledatawithoutinferringuseful information abouttheunder read α asanalternativetothecountingargumen t methodsborrowedfromstatistical physicshavebeenappliedtocomput ethetypicalversionspacevolume inhigh dimensi ons andyieldthesamebasicdependence forvarioustrainingalgorithmsinthepresence seeforinstance thestatisticalphysicsbasedanalysisandsimilarconsiderations alsoshowthatlearninginversionspacetypicallyperformsbetterthanrandom altoselectspeciﬁcstuden t liketherosenblattscheme typically α morespeciﬁcally theperceptronofoptimalstabilitywhichachievesnearoptimal expectedperformance optimalgeneralization inthestuden weonlyknowthattheteacher ofadditionalknowledgeitcouldbe anywher eintheversionspacewithequalprobability learninginversionspaceplacesthestuden verygeneralcircumst ances w whichitselfisadecreasingfunct ionoftheeuclidean sequence wouldbeachievedbyplacingthestuden tvectorinthecenterofmasswcmof theversionspace vwdnw bydeﬁni tion ithasthesmallestaverage angular distancefromallotherpoints itselfisnotnormalized inprinciple thedeﬁni tion immedi atelysuggestshowtodetermine wewouldhavetodeterminemany random elemen tsw enden tlyandcomput ethesimpleempi ricalestimate w est cm onofoptima lstability inpractice samplingtheversionspacewithuniformdensi theoreticalbackground andpracticalstrategiesforhowtoachieve theoptimalgeneralizationabilitywhen learningalinearlyseparableruleare discussed hereweapproachthequestion theexplicitcomput problemofquadraticoptimizationinstead thestabilitycriterion asameaningful whichisdeﬁned t duetothelinearityofeµinw thequantityκµisinvariantunder λ tionoflinearseparability thescalarproduct ofξµsµ distanceoftheinput vectorfromtheseparatinghyperplane preciselyκµisanorienteddistance forκµ theinput vectorisclassiﬁed correctlybytheperceptron whileforκµ tand theinput islocatedonthewrongsideoftheplane stability itsabsolutevalue quantiﬁeshowrobust fromthehyperplanewillhardlybetakentotheoppositesidebynoiseinthe input channel wedeﬁne thestabilityoftheperceptronasthesmallestofallκµind κ w κµ p notethatiftheperceptrondoesnotseparatetheclassescorrectly κ w κ w andseparatestheclasses κ w correspondstothesmallestdistanceofany examplefromthedecisionbounda thetwoclassesor inotherwords theclassiﬁcationmarginoftheperceptron inalinearseparableproblemitappearsnaturaltoselecttheperceptron weightswhichmaximizeκ w theconcept ofstabilityextends negativeκµaccordingtoeq efore wecanalsodeﬁne theperceptron ξµ sµ t withmoregeneraltargetsst ξµ κ κµ ξµ wmaxκ stabilityκµ deﬁned eq correspondstotheorienteddistanceofξµfromtheplaneorthogonal w isdeﬁned asthesmallestκµinthe setofexamples w κµ allinputsareclassiﬁedcorrectly withκµ restrictingthestuden thypotheses toweightvectorswith κ w κforagivendataset selectsweightvectorswinthecenterregionof largestpossiblevalueofκsinglesouttheperceptronof optimalstabilitywmax wedeﬁne theperceptronofoptimalstability notverypreciselytermed optimalperceptron occasionally asthetargetofthefollowingproblem perceptronofoptimal stability ξµ sµ t p w forκ w t forlinearlyseparabledata thesearchforwcouldbelimitedtotheversion spacev formoregeneraldata setsofunknownseparability theversionspacemightnotevenexist κ wmax theuseful ness ofacorrespondingsolutionwmax withnegativestabilityκmax inthefollowing perceptronofoptimalstabilitywmaxdoesnotexactlycoincidewith wcm center asdeﬁned bythe maximum possibledistancefromallbounda ries isidenticalwiththecenterofmassonly ifvhasaregular symmet onecanexpectthatwmaxisin generalquiteclosetothetruecenterofmassandcouldserveasanapproximative onofoptima lstability asaconsequence able nearoptimal generalizationbehaviorwhen trainedfromagivenreliable thediﬀerence appearsmarginalfromapractical eralizationintheperceptronsee theminoveralgorith m anintuitivealgorithmthatcanbeshowntoachieveoptimalstabilityfora datasetdhasbeensuggestedin algorithmperformshebbianupdatesforthecurrentlyleaststableexamplein tabularasainitialization butmoregeneral initialstatescouldbeconsidered prescriptionalwaysaimsatimpro thatforagivenweightvectorw t theminimallocalpotentialcoincideswith minimumstabilitysinceκµ t t t theminoverupdateisdeﬁned asfollows minoveralgorithm atdiscretetimesteptwithcurrentw t ethelocalpotentialseµ t t tforallexamplesind eµ t p w t t equivalently incremen tthecorrespondingembeddingstrength t afewremarks bianupdateisonlyperformed ifthecurrentlysmallestlocalpotentialalso satisﬁeseν t mistakesasintherosenblattalgorithm aspointedout whichis equivalentto minoverupdatestheweights dingstrengths evenifallexamplesindareclassiﬁedcorrectlyalready asthealgorithmkeeps hebbianupdates ralchangeoftheweightvectorw t sidered w t w t t ortheargumen tofthearccos forsimplicity κµ t andsimilarcriteria reasonablylargenumbersof trainingstepstshouldbeperformed inordertoallow fornoticeablediﬀerences inbothcriteria t aredisregardedbecausetheydonotaﬀect theclassiﬁcationoritsstability tionoftheminoveralgorithm weseethatitcan ative integerembeddingstrengthswhen thefollowingsection togetherwithseveralotherpropertiesofoptimal stability ceptronweightsofoptimalstability proof ofconvergence issimilarinspirittothatoftheperceptronconvergence theorem instead weshow onlythattheperceptronofoptimalstabilitycanalwaysbewritteninthe form maxξµsµ twithembeddingstrengths p theexistence ofembeddingstrengthsxmaxwillberecoveredenpassantinthe itisinstructivetoprovethestatemen texplicitlyhere tothisend weconsidertwoperceptronweightvectors theﬁrstoneisassumed tobegivenasalinearcombinationofthefamiliarform twithembeddingstrengths p whileforthesecondweightvectorweassume contributionswhichareorthogonaltoallinput vectorsind weobservethat eµ lstability byquadraticoptimiz ation ontheotherhandwehave wher ethemixedtermvanishes asaconsequence weobservethat κµ eµ andtherefo weconcludethatanycontributionorthogonaltoallξµinevitablyreduces deed achievedbyweightsoftheform resultalsoimpliesthatthe frameworkofiterativehebbianlearningissuﬃci enttoﬁndthesolution cannotexistforp n ingeneral obviously ξµ p onalvectorincludingwmax istriviallytrue oursimpleconsiderationdoesnotyieldrestrictionsonthepossiblevalues thattheembeddingstrengthscanassume theprovenconvergence oftheminoveralgorithmimpliesthattheperceptronofoptimalstabilitycan ativexµ moreformallyinthenextsection icalresultsandtechniquesfromoptimizationtheorybecomesavailable providedeeperinsightintothestructureoftheproblemand allowfortheidentiﬁcationofeﬃcienttrainingalgorithms aswewillexempl ify beforederivingand discussi ngthistrainingscheme prototypical trainingscheme fromtheearlydaysofneuralnetworkmodels sadaptivelinearneuronoradaline optimalstabilityreformulated ξµ sµ t p theperceptronofoptimalstability correspondstothesolutionofthefollowingproblem max imize w wher eκ w t whichisjustamorecompactversionof obviously thestabilityκcanbemadelargerbyincrea singtheminimaleµ κincrea previously actualchoiceoftheconstantcisirrelevantbecauseeµislinearinwandwecan asfollows minimize p wehaverewrittentheproblemofmaximalstabilityastheoptimization ofthequadraticcostfunct linearinequalityconstraintsof solutionwmaxthendisplaysthe optimal isirrelevant forthedeﬁni tionoftheproblembutiskeptforconvenience andconsistency notation theproblemofoptimalstabilityintheformulation involvesasystemof weresort tothemorefamiliarcaseofconstraintsgivenbyequations simpleroptimizationproblem minimize infact quadratic costfunct ionandasetof linear discussi historically theproblem neuronoradalinemodelwhichwasintroduced itconstitutesoneoftheearliest artiﬁcialneura lnetworkmodelsandtrulygroundbr eakingworkintheareaof widrow spioneeringworkisavailableat widrowrealizedadalinesystems inhardwareas resistorswithmemo ry andintroduced thetermmemistor concept wasalsoextended tolayeredmadaline manyadaline networksconsistingofseverallinearunits whichwerecombinedinamajorityvoteforclassiﬁcation forourpurposes theadalinecanbeinterpretedasasinglelayerperceptron whichdiﬀersfromrosenblatt smodelonlyintermsofthetrainingprocedur themorerecentconcept ofmemristorelemen t lstability byquadraticoptimiz ation theadalineframeworkessen tiallytreatstheproblemofbinaryclassiﬁcationas alinearregressionwithsubseq wewilltakearatherformalperspectivebasedonthetheoryoflagrange multiplier inglagrangefunct ion w λµ p µλµξµsµ µλµ ofproblem read t wher etheshorthand ent secondoder conditionsforasolution secondcondition merel yreproduces straints whichhavetobesatisﬁed byanycandidatesolution ﬁrst moreinterestingstationaritycondition troduced lagrangeparameterscanbeidentiﬁedastheembeddingstrengthsxµ andcanberenamed wecaneliminatetheweightsfroml andobtain xµ p µ νxµsµ νxνsν ξµsµ µxµ ν itturnsoutuseful toresorttoacompactnotationwhichagainexploitsthat µxµξµsµ thesymmet ric correlationmatrix nsµ tsν anddeﬁne xp ep yielding inaddition whichispopularintheoptimization relatedliteratureandindicatesthataµ notationsareempl wehavefurthermo tsν µ sµ tsν inthisnotation thelagrangefunct ion becomesl whichhastobemaximized seethediscussi onof thewolfedualin asthefollowingunconstrainedproblem max imize comparedto thecostfunct turn paralleladalinealgorithm inabsence ofconstraints itisstraightforwardtomaximizef andwecould empl oyavarietyofmethods wecanresorttosimple gradientascent canalsoidentifyanequivalentupdateintermsofweightswithinitialw byexploitingtherelationw t t ξµsµ t adaline algorithm parallelupdates t t t w t t ξµsµ wher eeµ t t t tudeoftheupdatesteps thisversionoftheadalinealgorithmcorrespondstostandard batchgradientdescen ηcan beﬁnitebuthastobesmallenoughenoughtoensur econvergence ciseconditiondepends onthemathema ticalpropertiesoftheextremum lstability byquadraticoptimiz ation sequentialadalinealgorithm asanimportantalternativetotheabovebatchorparallelupdate sequential edlyin forinstance deterministicorderandupdateasingleembeddingstrength wewouldwanttoincrea seanembeddingstrengthof anyexamplewitheµ increasewith thesequentialadalinealgorithmcorrespondstocoordinateascentinterms avariantofgradientbasedoptimizationwhichisbrieﬂydiscussed adaline algorithm sequentialupdates repeatedpresentationofd presentexampleµ t thecorrespondingembeddingstrength xµ t t t t w t t t ξµ t sµ t convergenceoftheadaline inthefollowingwesketchtheproofofconvergence forthesequentialadaline algorithm fortheupdateofthecurrentcomponentxµwehave µ whileallotherembeddingstrengthsremainunchanged ther efore thechangeoftheobjectivefunct ionunder update f δµ cµµ theobjectivefunct ion willstrictlyincrea seinatleastsomeoftheindividualstepsofthesequential procedur thediagonalelemen ingfeaturevectors ξµ whichareofcourseavailableforanygiven increa seoffunder theiteration thelearningratehastosatisfy max cµµ p onecanalsoshowthattheobjectivefunct ionisbounded fromaboveifcis positivedeﬁni eq cangrowinanunbounded way thenegativequadratictermwillalways dominateandlimittheincrea se updateineachsweepthroughthedata setandfincrea thecostfunct ionisbounded efore oftheadaline convergesandmaximizesfunder ifthelearningratesatisﬁes relationtolinearregressionandthesse severalinterestingrelationstootherproblemsandalgorithms arenoteworthy aspointedoutin thecoordinateascent iscloselyrelatedto fortheiterative whichgives ν µ ν µνξµsµ t sµ t theadalineproblem hasthesamemathema ticalstructureaslinearregressionconsidered whichimmedi atelylinkseq tothepseudo inverse solution respectively itisinterestingtonotethatalthough istheweightspaceequivalent funct ion evenlineartransformationsof variablesdonotpreservethegradientproperty ingeneral canbeseen asgradientdescen tinweightspacewith respecttoadiﬀerent yetrelatedcostfunct ion thesum ofsquarederrors sse forlinearregressionwithtargetvaluessµ sµ ξµsµ notethatwritingesseintermsofembeddingstrengthswouldinvolvethe lstability byquadraticoptimiz ation thisconsiderationalsoindicateswhatthebehavioroftheadalinewillbe thealgorithmﬁnds anapproximatesolutionby minimizingthesse sequentialalgorithm inweightspaceisequivalenttowidrow andhoﬀ soriginallms leastmeansquare methodfortheadaline lm gorithmintheliterature canbeseenasthemostimportantancest orofthe prominentbackpropagationoferrorformultilayeredneuralnetworks ualrelationsbetween classicalalgorithms isgivenin theadaptiveperceptron algorith inrosenblatt sperceptronalgorithmanupdateisperformed whenev sentedexampleismisclassiﬁed formed withthesamemagnitude indep enden toftheexample sdistancefrom theadalinelearningruleisadaptive inthesense thatthemagnitudeoftheupdatedepends ontheactualdeviation andisexpected tospeeduplearning italsoresultsinnegativehebbianupdates unlearning withη adalinecanyieldnegativeembeddingstrengthsxµ theexampleotherwi secouldhaveeµ thissomewha theadaptiveperceptron adatron algorithm concept lem butprohibits theembeddingstrengthsfromassumi ngnegativevalues aswewillsee ativelysimplemodiﬁcationyieldstheperceptronofoptimalstabilityforlinearly separabledata methodoflagrangemultipliershasbeenextended tothetreatment ofinequalityconstraints efore resultsfromoptimizationtheoryinthe whichwerepeathere forthesakeofclarity perceptronofoptimal stability weightspace minimize p formally wehavetoconsiderthesamelagrangefunct ionasforequality constraint alreadygivenineq w λµ p uckertheo remofoptimizationtheory videstheﬁrstordernecessa rystationarityconditionsforgeneral optimizationproblems ucker kt uckerconditions asworkedout asanexampleintheappendix theyread ion optimalstability t embeddingstrengthsλµ linearseparability ativemultipliers complemen tarity theﬁrstconditionisthesameasforequalityconstraintsandfollowsdirectly itimpliesthatthesolutionof theproblemcanbewritteninthefamiliarform thelagrange merel yreﬂect sthe originalconstraint intuitively ativityofthemultipliers ﬂectsthefactthataninequalityconstraintisonlyactiveononesideofthe hyperplanedeﬁned thecorrespondingmultiplier weformallyrecoverthe tedby ativeembeddingstrengths afterrenamingthelagrangemultipliersλµtoxµ werefertoasolution notation thektconditionsofthesimpliﬁedproblemread ion optimalstability simpliﬁed linearseparability ativeembeddings complemen tarity themostinterestingconditionisthatofcomplemen tarity itstatesthatatoptimalstability oembeddingwill lstability byquadraticoptimiz ation linearcombination thispropertyingreaterdetaillater complemen therefo bothsatisfyingthe tion thematrixcissymmet ricand te µ νuµsµ µuµξµsµ ative whileallcomponentsof vector efore µ ν xµ sµ t xν µ xµ ξµsµ thesameweightvector whichis thesolutionisunique thisﬁndingimpliesthatanylocalsolution oftheproblem isindeed manyothercostfunct ionbasedlearningalgorithms localminimadonotplay appliestoallconvexoptimizationproblems itessen tially amountstotheeliminationoftheweightsandtheinterpretationoflagrange multipliersasembeddingstrengths itconstitutesaspecialcaseofwhatis knownasthewolfedualinoptimizationtheory initionandproofofthecorrespondingdualitytheo rem abriefintroduction oyedtorewriteagiven problemintermsofamodiﬁedcostfunct problem perceptronofoptimal stability embeddingstrengths dualproblem max imize theresultingwolfedualstillcomprisesconstraints albeitsimplerones inthissimpliﬁedformulationofoptimalstability goalistomaximizetheobjectivefunct ion rerestrictedtothepositive hyperoctantofrp activesetalgorithm itispossibletocombinethis concept withcoordinateascentasdiscussed satisﬁes theconstraintbysimplyclippingeachxµtozerowhenev erthegradient stepwouldleadintotheexcluded regionofxµ adatronalgorithm sequentialupdates repeatedpresentationofd presentexampleµ t thecorrespondingembeddingstrength xµ t xµ t t t wher nameadatronhasbeencoinedforthisadaptive perceptronalgorithm linealgorithm weobservethatthechangefromequalityconstraints toinequalities leadstotherestrictionofembeddingstrengths ativevalues otherwi se isadaptiveinthe sense ofthediscussi adaline andadatroncandecreaseanembeddingstrengthifeµisalreadylarge aparallelversionofthealgorithmcanbeformulated whichperformssteps butalwaysensur wher enecessa forsuﬃci thecostfunct ionwill alsodecreasemonotonically werefrainfromgivinga moreexplicitmathema ticalformulationanddiscussi onoftheparallelupdates convergenceofthesequentialadatronalgorithm asweshowinthefollowing lstability byquadraticoptimiz ation separable andseethatinbothcases similartoeq fortheadalinealgorithm weobtain f δµ δµ δµ condition forthemonotonicincrea seoffasinthesequentialadaline algorithm furthermo itcanbeshownthatfisbounded withlinearconstraintsithasbeenproventhatthewolfedualisbounded andonlyiftheoriginalproblemhasasolution thisproperty iscloselyrelatedtofarkas lemma andgordan stheo remofthealternative whichwediscussed brieﬂyinsec latterimpliesformatrices p itisstraightforwardtoshowthatfineq cannotbebounded lim insumma ry wehave forlinearlyseparabled thecostfunct ionf isbounded fromabovein b thefunct ionf increa sesmonotonicallyunder o c byconstructionofthealgorithm isaﬁxedpointoftheadatronalgorithmandviceversa incombination represen tsthe unique perceptronofoptimalstability normalized data ed yielding thecondition embeddingstrengthsonly incontrasttotherosenblattoradalinealgorithm itisnotstraightforwardto cancomput ethenew weightvectorasw ξµsµ tafter adirectiterationoftheweightsw t cannotbe providedwithoutinvolvingthexµ t clippingofembeddingsatzerowhichhasnosimpleequivalentinweightspace outcomeoftheadatrontraining whichtakeintoaccount thatforeµ inthe literature seethediscussi forthese correspondinggradientdescen tmethodsinweightspacecanbe derived tronscheme theyarenotidenticaland inparticular additional constraintshavetobeimposedinordertoachieveoptimalstabilityforlinearly separabledata concept spondingextensionoftheadatronalgorithmisdiscussed eﬃcientalgorithms minoverandadatronarepresentedhereasprototypicalapproachesto theprobleminrelativelysmalldatasets thecomput ationalloadmaybecome problematicwhen dealingwithlargenumbersofexamplesand consequently largenumberofvariablesxµ avarietyofalgorithms havebeendevisedaimingatcomput ciency andscalability mainlyinthecontextofsupp ortvectormachines cf sec smo approach tations forlinksandthesvmliteratureforfurtherdiscussi onsandreferences supportvectors thetreatmentofoptimalstabilityasaconstrained lemhasprovideduseful insight fromwhichpracticalalgorithms suchasthe adatronandotherschemes emer ged sence oflocalminimaandtheresultingavailabilityofeﬃcientoptimizationtools isoneofthefounda tionsofthesupp ortvectormachineandhascontributed largelytoitspopularity oneofthemostimportantobservationswithrespecttooptimalstability isaconsequence ofthecomplemen taritycondition lstability byquadraticoptimiz ation supp ortvectorsin theperceptronofoptimalstability arrowrepresen tsthenormalized mem bershipisindicatedbyﬁlled andopensymbols ortvectors markedassquares fallintothetwo circle playgreaterstabilitywithoutbeing embedded explicitly inthegeometricalinterpretationoflinearseparability theformersetoffeature ryand markthemaximumachievablegapbetweenthetwoclasses onlythese example markedassquaresintheﬁgure contributetothelinear combination canbesaidtoform thesupp ortoftheweightsand therefo arereferredtoasthesetofsupp ort theyrepresen tthehardcasesindwhichendupclosest tothedecisionbounda fromtheseparatinghyperplaneanddonotexplicitlycontributetothelinear combination forthesupp ortvectorsweobservethatwmaxistheweightvectorthat otherexamplesappeartobestabilized accidentally ifwewereable toidentifythem beforehandinagivend wecouldsolvethesimplerproblem restrictedtothesupp ortvectorsbyapplying theadalinealgorithm unfortunately thesetofsupp minationisanintegralpartofthetrainingprocess itisimportanttorealizethat despi tethespecialroleofthesupp ortvectors endupwith zeroembedding thecompositionoftheentiredimplicitlydeterminestheset ofsupp ortvectorsand theactualweightvector remark complementarityintheminoveralgorithm minoveralgorithm aspresentedinsec eral ointegerxµ thecomplemen tarycondition issatisﬁed inthesense thattheembeddingstrengthsofthesupp torsincrea seinthetrainingprocess properlyrescaled xµsatisfycondition sofar followingtheargumen tsleadingtoeq homogeneouslyandinhomogeneouslylinearlyseparablefunct ionsonthesame withrespecttothestabilitycriterionwehavetotakea subtletyintoaccount ifweconsideraperceptronwithoutput s ξ theproperdeﬁni tionofthestabilitiesis κµ ratinghyperplanegivenby thecorresponding perceptronofoptimalstabilityisgivenbythesolutionoftheproblem min w notethatifforsuitableθ thechoice ofθwillinﬂuence theresult optimizationhastobedonewithrespecttobothwandθ ifwenaivelyexploittheequivalence ofinhomogeneousseparabilityinn dimensi ons wedeﬁne w θ andaugmentedfeaturevectors ξ tionofthestabilitiesreads andtheresultingoptimalstabilityproblem min p wher etheconstraintsareequivalenttothoseineq treatingθ asanadditionalweightinthespiritof hasalteredtheobjectivefunct ionin comparisonwith asitincludes woulddiﬀerfromthe correct optimalstabilitydeﬁned forthesakeofclarity wehaveexplicitlylimitedourdiscussi appliestothesupp ortvectormachineinsec cationsofthealgorithms andotherresultstothemodiﬁeddeﬁni tions fornonzerolocalthresholdareconcept uallystraightforward rks rks inthischapterwehaveconsidered thesimpleperceptronasaprototypical machinelearningsystem itservesasaframeworkinwhichtoobtaininsights intothebasicconcept ofoptimizationtechniquesandtherelatedtheoryinmachinelearning restrictiontolinearlyseparablefunct ionsis ofcourse rabledata setsandaddressclassiﬁcationproblemsbeyondlinearseparability itisremarkablethattheperceptronstillranksamongthemostfrequently supp ortvectormachineisfrequentlyused withalinearkernel inthiscase itreduces toaclassiﬁerthatisequivalenttothesimple perceptronofoptimalstabilityoritsextensiontothesoftmarginclassiﬁer cf beyondlinearseparability shardtosolve ingstrategyintermsofthesimpleperceptronclassiﬁer weobtainedimportant insightsintotheprinciplesoflearningaruleandobtainedtheconcept ofoptimal stability aspointedoutalready learninginversionspaceonlymakessense anumberofconditions ionsare ξµ sµ t p labelsarecorrectand b unknownruleislinearlyseparable ormoregenerally thestuden t complexityperfectlymatchesthetargettask inpractice avarietyofeﬀect scanimpairthereliabilityofthetrainingdata someexamplescouldbeexplicitlymislabeledbyanunreliableexperttobegin cationbetweenstuden tandteacher alternatively oradditionally someform ofnoiseorcorruptionmayhavedistortedtheinput vectorsξµind potentially render ingsomeofthelabelssµ tinconsistent infact realworldtrainingscena riosanddatasetswillhardlyevermeet theconditions b restrictingthehypothesisspacetolinearlyseparablefunct ion thefollowing situationsaremuchmorelikelytooccur theunknowntargetruleislinearlyseparable butdcontainsmislabeled example forinstanceinthepresence ofexamplespandthedegreeofthecorruptionthefollowingmayoccur inparticular smalldatasetsdwithfewmislabeledsamplescanstill belinearlyseparableandthelabelssµ tcanbereproduced ceptronstuden yversionspacevisconsistent withd itisnotperfectlyrepresen tativeofthetargetruleandthe success oftrainingwillbeimpaired thedatasetdisnotlinearlyseparableand consequently aversion spaceoflinearlyseparablefunct ionsdoesnotexist case learninginversionspaceisnotevendeﬁned fortheperceptron thedatasetstillcontainsinformation rupted abouttherule butitisnotobvioushowitcanbeextracted eﬃciently ii targetruleitselfisnotlinearlyseparableandwouldrequirelearning systemsofgreatercomplexitythanthesimpleperceptron theconsequences forperceptrontrainingdependonthesizeoftheavailable dataset smalldatasetsmaybelinearlyseparableandonecanexpectorhope thatastuden certainextent inthissituation thetargetis infact morecomplex ofcourse superpositionsofcasesi andii canalsooccur wecouldhaveto rablerule represen tedbyadatasetwhichinadditionis subjecttosomeformofcorruption asmentionedearlier itisadiﬃcul ttaskinpracticetodeterminewhet oftherosenblattalgorithm forinstance couldsimplyindicatethatalarger ispresentedwhicheitherﬁnds asolutionorterminatesandestablishes separability inthefollowing wediscuss anumberofbasicideasandstrategiesforcoping rabledatasets tdapproximately thesense thatalarge possiblymaximal fractionoftraininglabelsis reproduced correspondingtrainingschemes layerednetworksforclassiﬁcationcanbeconstructedfrom insec onwith error orothersimpleclassiﬁers canbecombinedintoan ensembleinordertotakeadvantageofawisdomofthecrowdeﬀect blesofdecisiontrees constituteoneofthemostprominentexamplesintheliterature onofensem blesandreferto theliterature forfurtherreferences withlinear argumen line theframeworkofthesupp ortvectormachines svm andcontinues andshowthatthesvm canbeseen asa concept ualextensionofthesimple perceptron asaregressionproblem anetworkofcontinuousunits includingthe output canbetrainedbystandardmethodsand eventually theoutput isthresholded wehaveseen oneexamplealreadyintermsoftheadaline scheme oﬀeranother verypowerfulframework theexampleoflearningvectorquantization lvq whichimplemen selinearorpiecewi sequadraticdecision bounda assume thatagivendataset ξµ sµ t isnot linearlyseparableduetooneorseveralofthereasonsdiscussed inthefollowing wediscuss thiscorrespondstothe assumpt ionthatthedatasetisnearlylinearlyseparableorinotherwords unknownrulecanbeapproximatedbyalinearlyseparablefunct ion minimalnumberoferrors aimingatalinearlyseparableapproximation onemightwanttominimize asimpleperceptronstuden shouldrealizethattheoutcomewillbeverysensi tivetoindividual misclassiﬁed examplesind formally minimal numb eroferrors perceptron ξµ sµ t p minimize w w ξµ sµ t t notethatthiscostfunct iondoesnotdiﬀerentiatebetweennearlycorrect ryandclearcaseswher ethe featurevectorfallsdeep ce abovedeﬁned problemprovesverydiﬃcul misclassiﬁedexamplesisobviouslyintegerandcanonlydisplaydiscontinuous einfeature gradientbasedmethodscannotbeempl oyedforthe minimizationorapproximateminimizationofherr aprescriptionthatapplieshebbianupdatestepswhichcanbeseen asa modiﬁcationoftherosenblattalgorithm hasbeenintroduced gallant sentationofsimpleexamplesincombinationwiththeprincipleoflearningfrom yetotherwi se conventional rosenblattupdatesofthevectorw thesofarbest withrespect toherr intothepocket pocketalgorithm tabularasa selectasinglefeaturevectorξµwithclasslabelsµ t ethelocalpotentialeµ t t t t performanupdateaccordingto rosenblattalgorithm w t t ξµsµ eh w t t accordingto w ifh t improvemen t t ifh t onwith error obviously t t cannever increa seunder theupdates onecanshowthatthestochastic selectionofthetrainingsampleguaranteesthat inprinciple t approach theminimumofherrwithprobabilityone thisquiteweakconvergenceinprobabilitydoesnotallowtomake statemen tsabouttheexpectednumberofupdatesrequiredtoachieveasolution inthecontextoftheperceptronconvergence theo rem severalalternative approacheshavebeenconsidered whichcan beshowntoconvergeinamoreconventionalsense attheexpense ofhavingto accept ptimalherr alongthelinesdiscussed towardstheendofsec thegradientbased minimizationofseveralcostfunct ionssimilartoeq discusses objectivefunct ionswhich correspondto h k w kθ t eq theabovefunct ionsare notquite precisely despi te theconcept ualsimilarity w optimalstabilitywouldhavetobeenforced softmargin classiﬁer anexplicitextensionofthelargemarginconcept hasbeensuggestedwhich allowsforthecontrolledaccept inthecontextofthesvm andreferences therei formalismpresentedthere reduces tothesoftmarginperceptroninthecaseof alinearkernelfunct ion acorresponding extended optimizationproblemsimilarto read softmarginperceptron weightspace minimize w p weintroduce βµ werecovertheunmo diﬁedproblemofoptimalstability p zero βµ witheµ whichincludes potentialmisclassiﬁcations eµ βµare accept ed βp canrewritetheproblemintermsofembeddingstrengths softmarginperceptron embeddingstrengths minimize thederivationofthewolfedual amountstotheelimination weobtaina modiﬁedcostfunct ionwithsimplerconstraints softmarginperceptron dualproblem max imize incomparisontothedualproblem parameterγlimitsthemagnitudes ofthexµ inanalogytothederivationoftheadatronalgorithm wecandevise asimilar talgorithm adatronwitherrors sequentialupdates repeatedpresentationofd t t µ gradientstep ativeembeddings xµ γ withlimitedmagnitude onwith error supp ortvectorsinthe softmarginperceptron thearrowrepresen tsthenormalized andopensymbolscorrespondtothe classessµ portvectors displayedassquares fallontothetwohyperplaneswith intotheregionbetweenthe plane markedbycircles displayeµ withoutexplicitembedding theonlydiﬀerence isthe asintheseparablecase tionalong anindividualembeddingstrengthwill increa seifthecorrespondingexamplehasalocalpotentialeµ including misclassiﬁcationswitheµ disnotseparable thecorrespondingxµwouldgrowindeﬁni marginversion theproblemis andtheadatronwitherrors ﬁnds refrainfromfurtheranalysisandaformalproof itisimportanttorealizethattheprecisenatureofthesolutiondepends stronglyonthesettingoftheparameterγ itcontrolsthecompro misebetween maximizingthe margin theempha sisisnotexplicitlyonthenumberoferrors butontheviolationsof forinstanceamismatched toosmall valueofγischosen cationswillbeaccept edandfavored practice asuitablevaluecanbedeterminedbymeansofavalidationprocedur e whichestimatestheperformancefordiﬀerentchoicesofγ inanalogytosec ortvectorsarecharacterizedbyxµ xµ γwilllieexactlyinoneof softmarginconcept evantinthecontextofthesupp ortvectormachine input layer k hidden σk ξ architectureofa machine asintroduced insec numberkofhidden unitsoftheperceptrontypeareconnect edbyadaptive layer naryresponses ξ ﬁxedfunct ionaldependence overcometherestrictiontolinearlyseparablefunct intermsofafamilyofsystems whichareoccasionallytermed machinesin theliterature reference therei layerrepresen unit σk ξ binaryoutput determinedbyaﬁxedfunct ionf σk funct ionfultimatelydeterminesthenetwork itisassumed process learningisrestrictedtothewkconnect inginput andhidden layer bereplaced byaformalweightfromaclamp edinput asoutlined orksofperceptr committee andparitymachines twospeciﬁcmachineshaveattractedparticularinterest cm thecommitteemachinecombinesthehidden unitstatesσkinamajority vote ξ themajorityvoteisreminiscentofanensem bleofindep enden tlytrained ismeanttobetrainedasawhole pm intheparitymachinetheoutputiscomput edasaproduct overthehidden unitstatesσk ξ binaryoutputspm ξ notethattheproduct depends onwhet herthenumberofunitswith fpm ityoperation cannotbe represen funct wedonothavetospecifyaneura lrealization onthecontrary asdiscussed insec rightpanel comparedtothegeneralformoftheoutput eq vationfunct iong z z manytheoreticalresultsareavailableforcmandpmnetworksandmore theirstoragecapacityandgeneralization abilityhavebeenaddressed forexamples forfurtherreferences gistofminskyand papert sbook isoftenreduced tothis single insigh t whichisagrossinjustice toboth thebookandtheperceptr outputfunct ionofcommitteemachineandparitymachine appliedtoidenticalsetsoffeaturevectorsξ ﬁlledandempt ycircles illustrationcorrespondstothesurfa chine threehidden unitsstatesaremarked network response majority ofσkdeterminesthetotalresponseofthecm dashed linesmarkredundant piece rightpanel inthepm everywher einfeaturespace theparitymachine auniversalclassiﬁer suﬃci entlymanyhidden unitscanimplemen ξµ sµ t p inthefollowing addunits toaneura lnetworkwithsubseq uenttraininguntilthedesiredperformanceis achievedand forexample agivendatasetdhasbeenimplemen rd andparisicoinedthetermtilingalgorithmforaparticularprocedur e whichaddsneuro havebeen suggested forexamplesandreferences andanalysed rilydesignedasapracticaltrainingprescription orksofperceptr paritymachine initialization ξ ξ aimingatalargenumberofcorrectlyclassiﬁed ii aftertrainingofmunits giventhepmwithmhidden unit σm indicesµoftheexamplessuchthatthepmoutputis sm ξµ ξµ tforqm wher eqmisthenumberofcorrectlyclassiﬁedexamplesind deﬁne ξµ sm ξµ sµ t p withlabels sm ξµ sµ t smwascorrect smwaswrong iii trainingstep addandtrainthenexthidden ξ astoachievealownumberoferrors notethatifasolutionwithzeroerror iii unitσm thetotaloutputofthepmis ξµ tforallexamplesind bythepmofmunits itissurpri ﬁedfeaturevectorscanbeincrea sedbyatleastone qm adding iii oftheprocedur e tothisend weconsiderasetofnormalizedinput vectorsintheprocedur e couldalwaysbeimplemen istriviallysatisﬁed ineverydatasetδcan bedeterminedbycomput ithms discussed insec beused inthisstep geneit y aclamp edinput asineq canbeemplo yed forsimplicit grandmot herneuron considerasetoffeaturevectors ξµ p p δ ν constructaperceptronweightvectorandthresholdas itresultsintheinhomogeneouslylinearlyseparableclassiﬁcation sw θ ξµ thecorrespondingperceptronseparatesexactlyonefeaturevector ξp fromall termgrandmotherneuronhasbeencoinedforthistype thatasingleneuro ninourbrainis activatedspeciﬁcallywhenev erweseeourgrandmo thisimpliesthat byuseofagrandmo ther unit wecanalwaysseparateξpfromallotherexamplesinthetrainingstep iii unitresponseforthisinput ξp whichcorrects themisclassiﬁcationastheincorrectoutputsm ξp tismultipliedwith ξp atleastoneerrorcanbecorrectedbyaddingaunittothegrowing pm withatmostpunitsintheworstcase thenumberoferrorsiszeroand alllabelsindarereproduced correctly afewremarks grandmo therunit servesatbestasaminimalsolutioninthe useof o p perceptronunitsforthelabellingofpexampleswouldbehighly ineﬃci ent iii canbeimprovedsigniﬁcantlyascomparedtotheconstructive solutionbyusingeﬃcienttrainingalgorithms suchasthesoftmargin adatron whichispossibly notquite asunrealistic asitmayseem seeforinstance foradiscussion jennif eraniston cell orksofperceptr added tothesystemlateraresupp osedtocorrectonlythe hopefully contradictstheattractiveconcept robust distributedmemo ries strengthofthetilingconcept ness unlimitedcomplexityandstoragecapacitycanbeachievedby addingmoreandmoreunitstothesystem ofearlystopping whichlimitsthemaximumnumberofunitsinthepm accordingtovalidationperformance weconcludethattheparitymachineisauniversalclassiﬁerinthesense apmwithsuﬃci entlymanyhidden unitscanimplemen universalclassifier paritymachine ξµ sµ t p p andcanbefound spm ξµ similartheorems havebeenderivedforother shallow architectureswith asinglelayerofhidden ﬁndingsare certainlyoffunda mentalimportance incontrasttotheperceptronconvergence theo rem andsimilarpropositionsareingeneralnotassociated withpractical eﬃcienttrainingalgorithms theresultparallelstheﬁndingsofcybenkoandothers networkswithasingle suﬃci entlylargehidden unitsconstituteuniversalfunct ionapproximators itisinterestingtonotethatalsoextremelydeep andnarrownetworkscan beuniversalclassiﬁers asanexample stacksofsingleperceptronunitswith shortcutconnectionstotheinput layerhavebeeninvestigatedin thecapacityofmachines comparedtothesimpleperceptron thegreatercomplexityofthecmorpm shouldleadtoanincrea followingtheworkof speakingthepmdoesnotfallintothisclass asfpmcannot berealized bya single perceptr mitchisonanddurbin wecanextendtheargumen tspresentedinsec thenumberck p n denunitscanrealizeforpfeaturevectorsinndimensi onsisobviouslybounded fromaboveas ck p n p n k withc p n fromeq thatthenetwork ionsrealizedbytheperceptronunitsinthe hiddenlayer wewouldexpectanequalityin correlation betweenthehidden unitsandotherredunda nciesorrestrictionswillreduce ck p n ascomparedtotheupperbound fortheprobabilityofarandomlabellingtobeseparablewithkhidden unit pk p n c p n k wher etheminimumisappliedtoexplicitlyavoidpk p n theupperbound furtheranalyticaltreatmentofeq isinvolved includingthelimit wecanobtainnumer icalestimates upperbounds ofthe storagecapacitybyidentifying forgivenkandn c p n k thismarksthecharacteristicpointαc k pk p n drop whichisinlinewiththeobservationthatc n andafew smallvaluesofkare αc αc αc whichalreadyshowsthatαc k displaysasuperlineardependence particular αc k wher perceptronswithoutanysynergyeﬀect canshowthatforlargevaluesofkthecapacitybound isgivenby αc k whichisconsistentwithafasterthanlineargrowthwiththenumberofhidden thesmallest integer pforwhichtheupperbound exceeds rmachines ityαc k mate solidlinecorresponds tothenaivelowerbound combiningkperceptrons thata parity machinewithasuﬃci entlylargenumberofunitscanimplemen t anygivendichotomy italsoindicatesthatthenumberofhidden unitsshouldbecarefullyselectedinordertoavoidtherealizationoflargestorage capacityattheexpense ofpoorgeneralizationbehavior ofthestoragecapacityforlargekhas beenconﬁrmed explicitlyfortheparitymachinealsobymeansofstatistical physicsbasedconsiderations thecommitteemachine theincrea seofαcappearstobeslightlyweakeras lnk thisdiﬀerence intheasymptoticgrowthofαc k isconsistentwithour intuitiveinsightthatthecommitteemachineissubjecttoredunda ncies theparitymachinemakesfulluseoftheseparatinghyperplanesinitshidden layer onsofavarietyofnetworktypes andarchitectureshasbeenofgreatinterestinthemachinelearningcommunity thisisduetotherolethatthecapacityplayswithrespecttotheultimategoal oflearning viewcanbefoundin seealsoreferences therei supp ortvectormachine svm constitutesoneofthemostsuccessful ceptualsimplicityofthelargemarginlinearclassiﬁer optimalstability dimensi onalspaces inparticular itempl makesitpossibletorealizethetransformationimplicitly foradetailedpresentationofthesvmframeworkandfurtherreferences forinstance itoryofmaterials includingashorthistoryoftheapproachisprovidedat concept ofapplyingakernelfunct seeaizerma netal ﬁrstpracticalversionofsupp ortvectormachines closetotheircurrentform wasintroduced byboser andrelatesto earlyalgorithms toisabelleguyon theminoveralgorithm terestintheconcept oflargemarginswhichwasthencombinedwiththekernel theimportantandpracticallyrelevantextensiontosoft marginclassiﬁcationwasintroduced tiontohigherdimen sion ﬁrstimportantconcept mappingoftheform ξ ξ wher emcanbediﬀerentfromn ionoftheform s ξ ξ isbydeﬁni tionlinearlyseparableinthespaceoftransformed whileretainingthebasicstructureoftheperceptron formally wewillbeableto realizefunct infact rosenblattalreadyincluded thisconcept ceptron thethresholdfunct ionsign isappliedtotheweightedsumofstates inanassociationlayer referredtoasmasksorpredicateunitsintheliterature forinstance connect edtosubset runitsandperformed athreshold operationonaneﬀect ivelyrandomizedweightedsum oftheincomingvoltages fordetails inthesupp ortvectormachine mensi onalspacewithm ninorder toachievelinearseparabilityoftheclasses asanillustrationoftheconcept wediscuss asimpleexamplewhichwaspresentedbyrainerdietrichin withtwoclasses leftpanel weapplytheexplicittransformation ψ example ons seethecenterandrightpanelsof applicationsofthesvmthedimensi rmachines illustrationcourtesyofrainerdietrich onal datasetwithtwoclassesthatarenotlinearlyseparable leftpanel canbecome mensi onalspace centerandrightpanel twodiﬀerentviewpoints separability whiletheconcept appearsappealing itisyetunclearhowweshouldidentify wereturntothisproblem andactuallycircumventitelegantly wediscuss theactualtraining dimensi onalspace largemargin classiﬁer letusassume thatforagiven sµ wehavefoundasuitabletransformationsuchthat sµ isindeed linearlyseparableinmdimensi wecanapplyconventional onalspaceand bymeansoftheperceptron convergence theo rem weareevenguaranteedtoﬁndasolution ingeneral wedonothaveexplicitcontrolof tionabout howdiﬃcul wewouldwishto onalψinordertoguarantee separabilityandmakeiteasytoﬁnd thestuden riosdiscussed thecorrespondingversion spaceofconsistenthypotheses wmightbeunnecessa rilylarge svmaimsatresolvingthisdilemma bydeterminingthesolutionof thepotentiallyverylargefreedo tsprovidedinsec generalizationability themathema ticalstructureofthecorrespondingproblemisfullyanalogous totheoriginal perceptronofoptimal stability ψµ sµ t p wκ w withκ w t obviouslywecansimplytranslateallresults concept sandalgorithmsfromsec space sofarwehaveassumed onal versionoftheminoveralgorithm mizationtheoreticalconcept thisimpliesthattheresultingclassiﬁer canbeexpressed intermsofsupp ortvectors whichultimatelymotivatesthe useofthetermsupp ortvectormachine thekerneltrick inanalogytotheoriginalstabilityproblem wecanintroduce xp ψ ξµ wealsodeﬁne thecorrelationmatrix γwithelemen msµ t andanalogoustoeqs weobtain eventually perceptronofoptimal stability minimize andproceed forinstance thecorresponding adatronalgorithm rmachines s ξ ξ ξ onal formed input vectorwiththetransformed deﬁne ion k ξ mψ ξ ξ ψj whichrepresen tsthescalarproduct s ξ ξµ ξ doesnotinvolvethetransformationψ deﬁned asafunct wehave txνk ξµ ξν onecanalsoformulatetheadatronalgorithmforoptimalstabilityinthe kerneladatronwasintroduced anddiscussed andhasbeenappliedinavarietyofpracticalproblems inanalogyto itisgivenas kerneladatron sequentialupdates repeatedpresentationofd xµ xµ t txνk ξµ ξν trainingalgorithmisalsoexpressed intermsofthekernelanddoesnot requireexplicituseofthetransformationψ formally sofar theaboveinsightssuggestastrategyalongthefollowinglines foragiven rabledn onsthatachieveslinearseparabilityofdm b comput ethekernelfunct ionforallpairsofexampleinputs ξµ ξν c forinstancebyuseoftheadatron d s ξ maxsµk ξµ ξ inpractice ofcourse theproblemistoﬁndandimplemen observethatoncestep isperformed itlyused isnotdirectlyupdatedininthetraining c norisitused fortheclassiﬁcation inworkingphase d therepresen tation isused ultimately placeandreplacestep foragiven rabledn identifyasuitablekernelfunct ionk ξ andproceed fromthere asbefore thiscanonlybemathema ticallysoundiftheselectedkernelk ξ tionrepresen tssomemeaningfultransformation uctsψ ξ thereverseislessclear givenaparticularkernel canweguaranteethatthere isavalid tion fortunately suchstatemen tscanbemadewithrespecttoalargeclassof funct ionswithouthavingtoworkouttheunder suﬃci entconditionsforakerneltobevalidcanbeprovidedaccordingto mercer stheo rem seealso intothemathema ticaldetailsandpotentialadditionalconditions itcanbe summa rizedforourpurposesas mercer scondit ion suﬃci entconditionforvalidityofakernel agivenkernelfunct ionkcanbewrittenask ξ mψ ξ g ξ k ξ g g ξ severalfamiliesofkernelfunct ionshavebeenshowntosatisfymercer scondition andarereferredtoasmercerkernels discussed rmachines polynomialkernels apolynomialkernelofdegreeqcanbewrittenas k ξµ ξ qyieldings ξ t asaspecialcase letusconsiderthesimplestpolynomialkernel linearkernel k ξµ ξ withs ξ t t tξµ inthiscase wecanprovideanimmedi ate almosttrivialinterpretationofthe kernel itcorrespondstotherealizationofalinearlyseparablefunct ioninthe originalfeaturespace thesvm eiseven anunfortunatetrendtorefertoitas thesvm ithastobe speciﬁedbydeﬁni ngthekernelinuse inparticular thelinearkernelreduces thesvmtothefamiliarperceptronofoptimalstability withlocalthreshold θ inordertotakefulladvantageofthesvmconcept wehavetoempl oymore quadratickernel k ξµ ξ j ξj j jξµ k ξjξk theoutputs ξ ineq neouslylinearlyseparablefunct ionintermsofthenoriginalfeaturesaugmented byn ξjξk whichincludes thesquaresoffeatures asintuitivelyexpected theuseofthequadratickernelrepresen formed feature original squaresandmixedproducts isreminiscentofquadraticdiscriminantanalysis qda ingatadiﬀerentobjectivefunct ioninthetraining similarly forthegeneralpolynomialkernel arybecomes ageneralpolynomialsurfa ceandthedimensi formed featurespacegrowsrapidlywithitsdegreeq weconsiderasomewha textreme yetverypopularchoice radialbasisfunctions rbf kernel k ξµ ξ whichinvolvesthesquaredeuclideandistanceandawidthparameterσ inanattempt tointerpretthepopularrbfkernelalongthelinesofthe discussi onofpolynomialkernelswecouldconsiderthetaylorseries exp x k whichshowsthatthedimensi asallpowersandproductsoftheoriginalfeaturesareinvolved therbfkernelhasbecomeoneofthemostpopularchoicesintheliterature thefactthatansvmwiththisextremelypowerfulkernelwith formally nstratestheimportanceoftherestrictiontooptimal stability thelargemarginconcept whichconstitutesaneﬃcientregularization oftheclassiﬁer afewrema rks selectionofkernelsandparametersetting inpractice thechoiceoftheactualkernelfunct ioncaninﬂuence kernelsmay containparameterswhichhavetobetuned dationtechniques acontrolparameter thewidthσineq kernelparametersaspartofthetrainingprocesshasalsobeendiscussed inthe literature forjustoneexample inhomogeneousseparationofclasses inanalogytotheperceptronofoptimalstability seesec anoﬀset onalfeaturespacecanbeconsidered inthesvm forthesakeofsimplicitywehaverestrictedthediscussi ontothehomogeneous uallystraightforward extensiontoinhomogeneousseparation rmachines rginsvm inadditiontothechoiceofthekernelandpotentialparametersthereo f tenresortstoasoftmarginversionofthesvm seealso considerationsofsec tronimmedi atelycarryovertothesvmformalism onceakernelisdeﬁned themodiﬁedoptimizationproblems areeasilygeneralizedto thecaseofthesupp ortvectormachinebyreplacingtheembeddingstrengths respectively consequently wecanimmedi atelyderivesuitabletrainingalgorithms adatronwitherrors algorithm thesoftmarginextensionintroduces anadditionalparameterinthetraining process theparameterγin implicitlycontrolsthetoleranceofconstraint violation orevenmisclassiﬁcations itshouldbedeterminedbymeansofasuitablevalidationprocedur overﬁtting intheearlydaysofthesvm occasionallytheclaimwasmadethatthestrong intrinsicregularizationrelatedtothelargemarginideawouldeliminatetherisk ofoverﬁttingtoalargeextent practiceshowsthat theuseoftoocomplexkernelsorlowtolerancetowardsmisclassiﬁcationcan resultinpoorgeneralization interestingly thesvmoﬀersasignalofoverﬁttingwhichdoesnoteven requiretheexplicitestimationofthegeneralizationerror thenumberofsupp ort vectorsnswithnonzeroembeddingstrengthsxµ thefactthatonlyveryfewexamplesinthedatasetarestabilizedbyembedding theotherssuggestsinferiorclassiﬁcationperformancewithrespecttonoveldata kerneltobeginwith forthetuningofitsparametersandforthechoiceofcontrol parametersintheoptimization eﬃcientimplementations remarkattheendofsec ationaleﬃciency eﬃci entimplemen tations forinstancebasedontheconcept ofsequential minimaloptimization smo areavailableforavarietyofplatforms asjustonesourceofinformation thereaderisreferredtoalistoflinksprovided atwww svms regressionandclassiﬁcation theﬁsher men inthenorthofspainhavebeenusingdeepnetworks layeredneuralnetworkshaveregainedsigniﬁcantpopularityduetotheir impressi vesuccesses inthecontextofdeeplearningapplicationssuchasimage basicdesignsandtrainingtechniqueshavebeenestablished decadesago thefollowing ticclassiﬁcationanddiscuss theirabilitytoapproximatecontinuousfunct ion wepresentmethodsfortheirtrainingbyoptimizationtechniqueslikegradient descen tandvariantsthereo tecturesandverybrieﬂyaddresstheuseofdeep networkswithmanyhidden layersandstrategiesfortheirtraining approxima tor weﬁrstrevisitthebasicarchitectureanddeﬁni rward inprinciple suitablenetworkscanapproximate orksforregr essio nandclassification j w kj n s k k s k k w kj k l k σ ξ l rwardneuralnetworkwithninput unit layersandasingleoutputunit anyreasonablefunct serveastoolsforquitegeneralproblems thatsuitablelayerednetworkscanapproximateany reasonablefunct ionbasedtrainingforthe learningfromexamplesisdiscussed sisongradientbased method ﬁgure suggestsaconvergentarchitecturewiththenumberofhidden unitsperlayer isbynomeansrequiredinthefollowing rward hidden unitdepends resultinghidden unitactivationis s m m m kjs j m wher eadaptiveweightsw m kjconnect unitoflayermandθ m wecouldintroduce anadditional clampedunits m napproximators represen tthelocalthresholdbyaweightw m ourrepresen tationofinhomogeneouslylinearseparablefunct wecanincludetheinput layerinthethenotationofeq bydeﬁni ng s wecanrenamethesingleoutputass l layerwithk l σ ξ l l k l l l l theoutputaccordingtoeq togetherwith canbeinterpretedas afunct ion σ architectureincludingitsconnect ivityandtheactivationfunct wewillassume thatthesametransfer funct iong deﬁnes theactivationofall hidden unit whiletheoutputmightresultfromaspeciﬁcactivationgout universalapproximators rwardnetworkwithasingle outputtoimplemen tafunct nconcept realizeorapproximatefunct ionaldependenci esbythesuperpositionofspeciﬁc basisfunct tationintermsof fourierserieswhichexploitpropertiesofthetrigonometricbasisfunct eﬃcientsarechosenastoapproximateatargetfunctiontoarequiredprecision frequently theyareﬁttedtoasetofdiscretepointsandtheresultingseriesis used thesituationisreminiscentof neuralnetworksoftheformdiscussed intheprevioussectioncanbeseen aparticularframeworkforfunct ionalapproximation foragivenarchitecture andconnect ivity thefunct ionσ adaptivequantities inthefollowingwewillseethattheconsidered workscanapproximatevirtuallyanyfunct anetworkforpiecewise constantapproximation hereweshowbyconstructionthatanyreasonablefunct ionf beapproximatedbyalayeredneuralnetworkcomprisingsigmoidalandlinear subtlety areignor edheretosome orksforregr essio nandclassification z sigm linear z b z leftpanel asigmoidalactivationoftheform diﬀerence oftwosteepsigmoidals respectively singlesoutargumen inset showsagraphicalrepresen tationofthesigmoidalswithequalγconnect ed toinput zwiththresholdsa b respectively andalinearunitcomput ingthe diﬀerence oftheiractivations wewillconsiderfunct ionswhichmapinputsfroma compactsubset tto n whichcanalwaysbegeneralized bytransformationslikerescalingandtranslationininput space letusﬁrstconsidersigmoidalneuronswith forinstance theactivation gγ zo z withtheargumen thresholdzoandsteepness parameterγ leftpanel steep sigmoidalunitscanbe eﬀect ively gγ b z z b z z b wher etheapproximateidentitybecomes ai bi foreachdimensi nseparately deﬁni ngregionsofinterest roi r j j b j j b j j n b j n canbeconstructedtocoverthevolume ofallpossibleinputs n whichimpliesthatmgrowsexponentiallywithn ifweaddupthecorrespondingnactivationsforoneregionofinterestwe ai bi ξi twecanignor ethediﬀerence betweenopenandclosed napproximators ai bi ξi gγ nthresholdnodes steepsigmoidalunits whichselectaspeciﬁc intervalperinput dimensi oncanbecombinedbyaddinguptheiractivation e allthresholdunitsareactivated thesum ai bi ξi additionalthresholdoperationcanbeempl oyedtosingleoutaregionofinterest asillustratedintherightpanel hereweconsideroneparticularroiandhaveomittedthesuperscript j beappliedtothesum correspondtotheroiofeq weselectonerepresen tative ξ j ofthetargetfunct ionineachroi forinstancewithξ j thecenterofr j simple efore theresultingnetworkresponse ξ k amountstoapiecewi seconstantapproximationofthetargetfunct ionin constructiveargumen tshowsthatthenetworkinprincipleconstitutes afewremarksareinplace tion rainedrepresen ngthatwesplit say kequalsizeintervals ineachofthenfeaturedimensi ons priseo nk unitsinthesecondandthirdlayer represen tingallpossiblecombinationsofnintervalsin hidden unit rwardnetworkbymeansofexampledata appearssomewha tobscur settingoftheweights ξ j ineq inviewofthepreviousremark theprocedur orksforregr essio nandclassification theconstructednetworkforpiecewi seconstantfunct imation eachinput unit toplayer isconnect edtoasetofsigmoidalunits connect tolinearunitsinthethirdlayer whichselectspeciﬁcintervals ai bi bi ci ticularsum ofnactivationsinthethirdlayer correspondingtooneselected intervalineachfeaturedimensi j ξ correspondstoarepresen tativeξ wouldrequireanumberpofexamplesthatgrowsexponentiallywiththe dimensi onn theconstructionofroiparallelstheuseofgrandmother neuronswhen showingthattheparitymachineisauniversalclassiﬁerin whiletheargumen rwardnetworksin principle itdoesnotprovideinsightintohowtodesignandhowtotrainsuch asysteminpractice ited intheaboveconstructionscheme fourlayersofprocessingunitsare suﬃci shallownetworksaresuﬃci enttoachievethisproperty thisdoesnotimplythatshallowarchitecturesarenecessa rilysuitable therecentsuccess ofdeepnetworksappears tosuggestthecontrary napproximators input layer weight w k k hidden w k k imationasconsidered incybenko stheo unitswith sigmoidalactivationandadaptivethresholdsθ k areconnect tiveweightvectorsw k layer linearresponseσ ξ w k k variantsoftheuniversalapproximationtheorem intheliteraturevariousincarnationsoftheuniversalapproximationtheorem havebeenpresented diﬀeringinthedegreeofgeneralityandpracticalrelevance someofthem addressparticularclasses oftargetfunct ion focuson speciﬁctypesofactivationfunct ionsornetworkarchitectures forexamplesandreviews inthefollowingwepresentanearlyandimportantformulationwhichisdue theoremstatesthatarelativelysimplenetwork withasinglehidden layerofsigmoidalunitsandalinearoutputisauniversal funct ionapproximator cybenko stheorem nandacontinuous sigmoidalfunct ion g z letw k θ k ﬁnitesums oftheform σ ξ w k k aredense inthespaceofcontinuousfunct ionsc orksforregr essio nandclassification thisimpliesthatforanycontinuoustargetfunct n andagiven realnumberε parameter w k θ k ξ ξ parameterscanbeinterpretedastheweightsandthresholdsofanetwork withasinglehidden termsoftcommitteemachinehasbeencoinedforthisarchitecture andreferences therei namerefers tothenetwork ssimilarity withthe discreteoutput committeemachineforclassiﬁcationtaskswhichis discussed inasense cybenko stheo remprovidesastrongerandmoreuseful statemen t thanthebasicinsightobtainedbytheconstructionintheprevioussection theproblemremainsthataverylargenumberkofhidden unit thetheoremitselfstatesonlytheexistence ofsuitableparameters itdoesnot suggesthowtoﬁndthem choiceofappropriatenetworkparametersbasedonexampledatais addressed whichare basedontheminimizationofappropriatecostfunct ionsbymeansofgradient descen ttechniques aswehaveseen rwardneura lnetworkscanserveasuniversalfunction itappearsnaturaltoempl oythem ion σ classiﬁcationschemes ationontheoutputσorbyappropriatebinninginthecaseofmultipleclasses classmem bershipscouldalsoberepresen tedbycodingscheme sappliedtoa numberofoutputunitsasdiscussed inaforthcomingsection formally convenientﬂattenednotationfacilitatesa uniﬁeddiscussi wehave w w k n θ θ k andwecanrefertoanyoftheadaptiveparametersasacomponentwj forsimplicitywefocusonnetworkswithasingle continuousoutputσ ξ goaloftrainingistoimplemen torapproximateatarget funct ionτ ξµ τ ξµ p tothisend wedeﬁne anerrormeasure whichissuitableforthecomparison ofthenetworkoutputσ ξ withthetargetfunct ionτ ξ foragiveninput vector apopularandintuitivechoiceisthesimplequadraticdeviation e σ τ hereandinthefollowing shorthand ξ ξµ ξ ξµ refertoagenericinput ξoraparticularexampleinput respectively whilemanyalternativemeasures canbeconsidered seethediscussi onin thequadraticerror remainsverypopularandisparticularly τandσ τinasymmet ricwayandyields twiththetarget givenasetofexamplesdwecandeﬁneacorrespondingtrainingsetspeciﬁc costfunct e w σµ τµ inthecontextofregression e w playstheroleofthetrainingerrorand expectationisthatthetrainednetworkrepresen tsahypothesisthatcanbe appliedsuccessful lytonoveldataintheworkingphase aplethoraofnumer icaloptimizationmethodscouldbeused tooptimizethe costfunct empl oylocalgradientinformationor higherorderderivativesofe w inordertoiterativelyﬁnda local minimum ofthecostfunct derivativescanbecomput edanalyticallyor areestimatednumer conjugategradientdescen t wher eanoverview canbeobtainedandfurtherreferences areprovided relativelysimplegradientdescen ttechniqueshavebeenparticularlypopular inthecontextofmachinelearningfordecades asaveryearlyexamplewehave alreadydiscussed theadalinealgorithm fromanoptimizationtheoreticalpointofview simplegradientdescen ti tant verypopulartoolinmanymachinelearningframeworks ual rwardnetworksappearsparticularlysuitableforthe comput ationoftherequiredgradients althoughthetrainingisgenericallyformulatedasanoptimization theactualminimizationofeservesonlyasaproxyfortheultimategoal termsfrequentlyused inthis contextare objectiv efunction loss function orksforregr essio nandclassification isthesuccessful precisiontowhichaminimumisdeterminedcanplayaminorroleandthe potentialexistence ofmany suboptimal localminimaofeisnotasproblematic asonemightexpect comp utingthegradient backpropagationoferror vationfunct ionsisthatthenetworkoutputitselfisadiﬀerentiablefunct ionof theoutputandtheerrormeasuree σ τ arediﬀerentiable withrespecttotheadaptiveparametersinthenetworkforanygiveninput ξ σ τ consequently alsothedatasetspeciﬁccostfunct σµ τµ isa diﬀerentiablefunct ionofallcomponentsofw tainderivativesofσwithrespecttoanynetworkparameterwjrecursivelyby applyingthechainrule mathema ticalstructure facilitatesaveryeﬃcientcalculationofthegradient weightsandthresholds serveascoeﬃcientswhen comput ingtheoutputforagiveninput issaidtopropagateback towardstheinput comput ingthederivativesin foranexampleofashallownetworkarchitectureareworkedoutexplicitlyin thetermbackpropagationoferror backpropagationorbackpropforshort wasoriginallyused fortheeﬃcientimplemen tationofthegradientonly rwardnetworksandisnowadaysmostlyused inthissense refrainfromtakingpartinthese discussi onsofquestionableuseful ness someoriginalarticlesandreviewsofthehistoryofbackpropagation batchgradientdescent inprinciple theminimizationofe w couldbedonebyanysuitablemethod relatively wefocusontheuseofstandardgradientdescen arealsodiscussed threadinitiated uberintheconne ctionis http ionist decemb inanalogytoeq intheappendix thebasicformoftheupdatesis givenas batchgradie ntdescent basicform atdiscretetimesteptperformanupdatestepoftheform w t t withthelearningrateηandcostfunct ioneoftheform ateachtimestep thegradientwithrespecttoalladaptivequantitieswis comput edasasum overallexamplesind σ ξµ ξµ ξµ wher butcanbeworkedoutfor alternativecostfunct ionsaswell termsbatchoroﬄinegradientdescen trefertothefactthattheentire setdofexampledataisused ionineachindividual stepand consequently thedescen costfunct ontheinitializationw ofthesystem obviously onthenetworkarchitecture theactivationfunct ionsandthesetofalladaptive committeemachine generaldiscussi onofgradientdescen tintheappendixshowsthatits convergence ric positivedeﬁni tehessianofsecondderivatives w inalocalminimum alleigenvalues ρi m bymagnitude constantlearningrateη termreferstolocalpropertiesofthefunction andpossibly includes global orksforregr essio nandclassification ρmaxb ρmax ρmaxc η ρmax illustrationofthebehaviorofgradientdescen markedbythereddotinthecenter contourlinesrepresen t thequadraticapproximationofe w inthevicinityofalocalminimum blacksymbolscorrespondtotheiteratesw t ineq η panela theiterationconvergessmoothlyintotheminimum intermedi ate stepsizes panelb convergeisobservedfortoolargestepsizes panelc forsmall ﬁnitelearningrates theiterationapproachesthelocalminimum smoothlyandconvergestolim t theapproachcanbecomeunnecessa rilyslow b inthisregime convergence isstillachieved butinatleastoneofthe inpanel b thealternatingbehavioroccursineigendirections withlargecurvature whichcorrespondtonarrowtroughsinthelandscape e w ρiinwhicheresem blesashallowbasin c η toolargelearningratesresultindivergentbehavioroftheiterationsas c ρi thedistancefromtheminimumcanincrea seinone orall twoimportant validclosetoalocalminimum wher ethetaylorexpansionuptosecondorder eq playverydiﬀerentpropertiesintermsofthehessiananditseigenvalues inany case minimaarenotknowninadvance whichwouldrender necessa ionecanhavemanylocalminima ofthetrainingprocessmaydependstronglyontheinitializationofthesystem ther efore thepracticalrelevanceofthemathema ticalanalysisislimited inpractice arelativelylargeηcouldbeused intheinitialphaseoftraining assumi ngthatthesystemisfarawayfromanylocalminimum scheme beensuggestedintheliterature whichmonitortheiterationsandadjust learningrate forinstancewheneverazigzaggingbehaviorisobserved intuitiveexampleofaheuristicstepsizeadaptationinbatchgradientdescen t ispresentedin themostimportantinsightofthissectionisthatinbatchgradientdescen t ﬁnitelearningratescanbeused toreachalocalminimum thisisin contrasttothestochasticgradientdescentstrategydiscussed inthenextsection ther e thelearningratehastobereduced tozerointhecourseoftrainingin ordertoenforceconvergence ofthenetworkconﬁguration stochasticgradientdescent thecostfunct ione w eq isgivenasasum overexamplesind e w w wher eeµquantiﬁesthecontributionofanindividualexampletothetotalcosts virtuallyallmachinelearningobjectivesmentionedanddiscussed inthistext canbewritteninsuchaform withtheactualfunct ioneµ w cisedeﬁni tionofddependingonthedetails thesse inregression quantizationerrorinunsup ervisedvectorquantization orthe objectivefunct ionofthegener alizedlvqscheme introduced wenotethatcostsoftheform canbeinterpretedasanempi rical averageofeµoverthedataset correspondingtorandomlydrawingexamples thegradientofeasineq canalsobeseen asaconsequence wecanapproximatethegradientofebycomput inga restrictedempi ricalmeanoverarandomsubset case caninspectsingle randomlyselectedexamples stochasticgradie ntdescent sgd atdiscretetimestept ξµ t τµ t randomplywithequalprobability w t t t t t t t anderrortermseµasgivenineq orksforregr essio nandclassification t inordertoindicateapossibleexplicit endence andtodistinguishitfromηinbatchgradientdescen t eq thecomput ationalcostsperupdatearelowerthaninthebatch procedur ewhichinvolvesthesum ofpgradienttermsineachstep theupdateoftheform isreferredtoasonlineorstochasticgradient descent incontrasttooﬄinebatchalgorithms itcanbeseen asaspecialcase ofstochasticapproximation thestochasticapproximationofthegradientintroduces noiseinthetraining process asaconsequence e w mayincrea seinindividualupdatesteps intuitivemotivationforstochasticdescen tisthatthisnoisehelpsthesystemto explorethesearchspacemoreeﬃcientlyandtoovercomebarriers pointsofe whichseparatediﬀerentlocalminimaofthecostfunct ion index µ t ofthepresentedexampleattimetineq isdrawn randomlyfromtheset p t willdeviatefromthedirectionofsteepestdescen t onaverageovertherandomselectionofanexamplefromd theupdate efore t t t wher e denotestheaverageovertheselectionofµ t notation indicatesthatatermisevaluatedinw t theaverageupdatebecomes zerointhelocalminimum o byconsidering t ingeneral wouldbeaminimumofeinwhichallindividualeµareminimized forthe σ ξµ minimume eσ ξµ theiterationfollowsaseemi domsequence wecanenforceconvergence nearalocalminimuminthesense lim t lim t byempl enden t whichdecreases appropriatelywiththenumberofdescen tsteps schema red contourlinesrepresen tthequadraticapproximationofe w inthevicinityof blacksymbolscorrespondtotheiteratesw t thedescen averageofthemostrecent positionsw t ofthestochasticdescen t largeﬁlledcircles resultsin afavorableestimate largeempt ycircle ofthelocalminimum rateschedul t andmonro whichintroduced theconcept ofstochasticapproximationin originallyinthecontextofﬁndingzerosofafunct andmoredetaileddiscussi onscanbefoundinseveraltextbooks robbinsandmonroshowedthatschedul eswhichsatisfylim t lim t ii lim t facilitateconvergence intuitively theﬁrstcondition t hasto decreasefastenoughinordertoachieveatrulystationaryconﬁgurationwith t t efore condition ii impliesthatthedecreaseisslowenoughsothattheentiresearchspacecanbe exploredeﬃciently simpleschedul eswhichreduce t lar realizationofsuchadecreaseisoftheform t withconstantparametersa b orksforregr essio nandclassification variousschedul eswhichsatisfytheconditions ii ofeq canbeconsidered t logarithmicschedul eslike t tlnt enden tschemes seeforinstance andreferences therei stochasticgradientdescen tisarguablythemostpopularbasicscheme forthe trainingofneura lnetworks includingsystemswithmanylayersinthecontext ofdeeplearning practicalaspectsandmodiﬁcations numer ousalternativeapproachesormodiﬁcationsofthegradientbasedschemes havebeensuggestedandareofgreatpracticalrelevance inparticularinthe onlyafewcanbementionedandexplained concept scanalsobeuseful inbatchgradient descen inpractice wedonothavetoactuallydrawa randomexamplefromdindep enden tlyateachtimestep mostfrequently updatesareorganizedinepochs p andpresentingtheentiredinthisorderbeforemovingontothe nextepochwithanovelrandomizedorderofexamples insteadofperformingthestochasticapproximation withrespecttoasingleexample arandomsubset ofdcanbeempl strategyretainstheadvantagesofsgd ationalcostsandthe introductionofnoise practicetoachievegoodperformanceandeﬃciency averagedsgd sultinaconvergingbehaviorw t onecanexpecttheiteratesw t toapproachthevicinityofalocalminimumandtoassume potentiallymoving averageoftheform wav t whichtakesintoaccountthelastkiterationsofsgd theaverageisperformed rightpanel theaveragedwavisexpectedtobecloserto thelocalminimumthantheindividualw t oncethetraininghasreachedthe vicinityoftheoptimum averagingsgd forfasterandsmootherconvergence wassuggestedandstudiedin efunctio n locallearningrates forboth batchandstochasticgradientdescen t hasbeensuggestedtouselocallearningratesfordiﬀerentlayers node simpleruleofthumb plautetal suggesttouselocallearningratesinverselyproportional seethediscussi onin ofthelocalpropertiesofthecostfunct ionintermsofsecondderivativesas motivatedbynewton smethod onlythediagonal elemen tsofthelocalhessianareused tocomput eanindividuallearningrate fortheupdateofwjwhichis forinstance forfurtherreferences notethatgradientbasedalgorithms withlocalorevenindividuallearning ratesdonotfollowthesteepestdescen theystillrealize adescen tprocedur e seethediscussi momentum alreadyin amodiﬁcationofsimplegradientdescen t wassuggested inwhichtheupdatecontainsamemorytermrepresen mationaboutrecentlyperformed linearcombinationofthe stochastic gradienttermandthepreviousupdate t w t withα decayfactorαcontrolstheinﬂuence momentumcouldalsobeincorporatedinbatch gradientdescen ideaistoovercomeﬂatregionswher momentumshouldmilden oscillatorybehavioroftheupdateswhen edisplaysanisotropiccurvatures adaptivelearningrateschedules ingrateschedul esandschemes ηinbatchalgorithms quently thelearningrateadaptationiscombinedwiththeconcept tum prominentexamplesarealgorithms termed adagrad rmspr op adam sedsgd vsgd ofseveralpopularschemes canbefound inthecontextof sofarwehavediscussed thetrainingoflayerednetworksbasedonthequadratic deviation arguably constitutesthemostprominentcostfunct ion orksforregr essio nandclassification heuri sticassumpt ionsorconcret tivatetheuseofalternativeobjectivefunct ionsinthetrainingprocess theuseofdiﬀerentiableneuralnetworksforclassiﬁcationtasksmotivates theuseofcostfunct ionswhicharedesignedforthisparticularpurpose inthefollowingwebrieﬂydiscuss tionsforregressionandclassiﬁcationbasedonlayeredneura lnetworkswith diﬀerentiableactivationfunct ion costfunctionsforregres sion veryintuitivequadraticdeviationorsse costfunct ionappearsplausible andsuitableforavarietyofregressionproblems jectivefunct ionshavebeensuggestedintheliteraturethatcanbeoptimizedby gradientdescen torsimilarprocedur e heuristic costfunctions numer ousheuri sticschemes havebeensuggested whichadjust aninstantaneousobjectivefunct ionwhiletrainingproceeds goalcouldbetosmoothoutthecostsinitiallybylevellingoutdetails eventually thegenuineobjective isoptimized asjustoneexample msv iﬁedquadraticcosts e w increa singduringthetrainingprocess endence γ t deviation training willfocusonachievingagreemen tintermsofsign σµ τµ uned eventually inthefollowingwefocusonaclassofcostfunct ion thatcanbederivedfromanoisemodelwhichisassumed todescr ticalpropertiesofthedataathand popularquadraticdeviationcanbeexplicitlymotivatedbyassumi ng couldresultfrom additive gaussiannoisecorruptingthetruetargetvaluesintheavailabledata asineq inthespeciﬁcexampleoflinearregression thatacorrespondingmaximumlikelihoodapproachleadsimmedi atelytothe iterion startingfromdiﬀerentassumpt ionsaboutthestatisticalpropertiesleadsto speciﬁcchoicesofthecostfunct forinstance efunctio n labelsinaregressionproblemdeviatefromthetruetargetfunct ionbyindep dentnoisetermsηµwith p ηµ p ηµ densi spondingmaximumlikelihoodcriterionintroduces acostfunct ionoftheform w wher etermsindep enden tofthenetworkparameterswhavebeenomitted thisobjectivefunct seealso criterionisrecovered costswithr tiveto outlier thantheconventionalsse costfunctionsforclassiﬁcation heuristically wecanapplynetworkswithdiﬀerentiableactivationfunct ionsand outputalsoforclassiﬁcation retainingregressiontypecostfunct ionslikethe simplequadraticdeviationor classeswecouldperformoneadditionalthresholdoperationonasingleoutput ofthetrainednetworktoobtainacrispbinaryclassiﬁer similarideascanbe amoresystematicapproachrealizesnetworkresponses pretedasaprobabilisticassignmen toftheinput vectortooneoftheclasses herewefollowtoalargeextentthepresentationin werestrict ourselvestothecaseoftwoclasses represen sume thatalsotheoutputofthenetworksatisﬁes ξµ asforinstance realizedbyapropersigmoidaloutputactivation aftertraining wewanttointerpretσ ξ bershipprobability p ξ andp ξ thiscanbewrittenconvenientlyinthecompactform p ξ τ ξ ifweconsiderthisasourmodelfortheoccurrence ofalabelτinthedataset andwefurthermo reassume thattheexamplesaregeneratedindep enden tly weignor ethesubtle diﬃcult ythat orksforregr essio nandclassification likelihoodofgeneratingagivensetoflabels τµ p σµ τµ ξµ maximizingthislikelihoodbychoiceofthenetworkparameterswisequivalent e w ln incontrasttothesse forsimplicity costfunct terpretasprobabilitieswiththetargetsτµintermsoftheircrossentropy frombelowbytheentropy ln onwwecansubtractitfromthecrossentropyandconsidertheequivalent objectivefunct ion e w τµlnσµ betweenthespeciﬁcprobabilitydistributionsσandτ seeforinstance foradiscussi crossentropye w eq divergencedklconstitutediﬀerentiableobjectivefunct ξµ τµ p anetworkwithoutputsσ ξ thatcanbeinterpretedasaclassmem bership probability theformalismcanbeextended lemswithtargetsτµ c thesimplecaseofcrisptraininglabelswehavethatforeachexampleexactly oneτµ obviouslywehavetoconsideranetworkarchitectureandactivationswhich canrepresen tcassignmen canbeachieved forinstance ergence isingener mmetr ic dkl ation functio n nowtheequivalentofcostfunct ion dkl k τµ whichreduces givenanetworkstructurewithcoutputsσkasdescr ibedabove wecan determinetheadaptiveparametersofthesystembyminimizationoftheabove diﬀerentiablecostfunct discussed canbeapplied designinganeuralnetworkforagiventask thekeystepisthechoiceof ionsisequally important asitshouldreﬂect ously theoutputunit orunits shouldrealizetheappropriaterangeofpossible response inaregressionproblem inclassiﬁcation properlydeﬁned output shouldencodethecrisporprobabilisticclassassignmen vationsinintermedi ate hidden layersinﬂuences thecomplexityofthenetwork andcanbecrucialforthesuccess oftraining torother technique sofarwehavemainlyconsidered thresholdorsigmoidalactivationfunct ion withthenotableexceptionofsimplelinearunitswhen constructingauniversal funct ionshavebeen suggestedandinvestigatedintheliterature inthissection inthefollowingwerefrainfromincludinggainparameters oldsorsimilarparametersinthedescr iption thecorrespondingextensionsare scaled forexample asigmoidalfunct ionh x x transformed asg x x x stoodthat allfunct ionsgivenasg x inthefollowingcanbemodiﬁedlikeag γx needed sigmo idalandrelatedfunctions ionsasarough approximationofbiologicalneuronresponses offunct ionssatisfytheconditions leftpaneldisplayserf x chainline green tanh x dashed blue andthelogisticfunct latterwasshiftedand scaledtorealizetherangeg x theheavisidestep funct ionofthemccul lochpittsneuronandapiecewi selinearactivation resem blesasigmoidalfunct ion orksforregr essio nandclassification xg x xg x sigmoidalandrelatedactivationfunct diﬀerentialsigmoidalfunct thelimitingcaseofthemccul lochpittsactivation heavisidestepfunct ion andapiecewi selinearfunct ion simplestofallactivationfunct ion thetrivialidentityg x bounded inthecontextofbiologicallyinspiredﬁringratemodelsthisdoesnot makesense astherearenolimitstothefrequency inartiﬁcialnetworks linearneuro nsareveryoftenempl oyedandindeed useful forspeciﬁctasks semore recitﬁedlinearunitandvariationsthereof linearunitorreluactivation leftpanel hasgained correspondsto g x x x weignorehere thesubtletythattherelufunct notethatthefunct ion hasbeenknownandused forlonginvarious mathema tical technicalandengineeringﬁelds rangingfromsignalprocessing andﬁlteringtoﬁnancemathema itisknowas therampfunction hockeystick orhingefunction intheliterature severaladvantagesareassociatedwiththereluactivation comparedto forinstance sigmoidalactivations thereluiscomput ationallycheap andsoisitsderivative inmodernneuralnetworks thedefault recommendation isto usetherectiﬁed linear unit ation functio n xg x xg x unbounded plelinearactivationg x dotted rectiﬁedlinearunitrelu solid eq andleakyrelu eq dashed ponentiallinearunitelu dashed black swish solid green andsoftplus dotted blue x considerablefractionofunitswilldisplayzeroactivityinagivennetwork anetworkofrectiﬁedlinearunitsrealizessparse activitywhichisconsidered advantageousinmanycases comput ingderivativesviathechainruleinanetworkofmany layer osedly theproblemisabsentin relunetworkswher x vationshavebeenpublished inwhichrelunetworksdisplayfavorable trainingbehaviorandperformance recently theoreticalstudiesofmodel situationsseem tosupp ortthese claim lrelu activation theconstantzero forx itreads g x x ifx x diﬀerentiable unboundedactivations severaldiﬀerentiablefunct ion whichmaintainorapproximatethelinearbehaviorg x tiveargumen tsx right panel displaysthreeexamples ion g x x orksforregr essio nandclassification elu g x exp x x ion g x interestingly imuminanegativevalueoftheargumen ricalstudiesseem showfavorableconvergence behaviorofgradientdescen tbasedtrainingand impro tions exponentialandnorma lizedactivations frequently unitswithinaparticularlayerarecoupled ization whichdeviatesfromtheactivationbylocalsynapticinteraction softmaxfunction mostprominentexampleistherepresen tationof assignmen tprobabilitiesinanoutputlayerofcunits σk c sec xk c xk c βxk βxj notethattherequirednormalizationcouplestheunitsσkandtheirstates cannotbeinterpretedasindep enden tlyactivatedbysynapticinteraction unitdepends beequal radialbasisfunctions rbf anotherpopularclassofactivationsalso deviatesfromthefamiliarconcept ofagivenunitσwithinput fromneurons sj l edas activationdepends termradialbasisfunctionrefers icarchitectu re amostprominentexampleisthegaussianrbf whichisfrequentlyreferred toastherbf withparameterβ frequently normalizedgaussianrbfareconsidered inalayerofhidden outputunitsσk k σk thenormalizedgaussianrbfsinglesouttheunitwith smallest dimensi onalinput layer asinglehidden layerwithkunits andalinear outputunitwithadjust remark universalfunctionapproximation insec seconstantfunct ionapproximatorusing forwardtoextendthese considerationstootheractivationfunct instance thatthecombinationoftworeluunits equippedwithsuitablelocal thresholdsandgainparameters canreplaceapiecewi selinearactivationofthe rightpanel forx b usingtheresultingpiecewi selinearactivation wecanimplemen ttheselection ofroi inanalogytothesigmoidalactivationsassumed selinearunitsalsoconstituteuniversal tionfunct ion similarly unitswithnormalizedrbfactivation eq canbereadily used todeﬁne roiininput spaceandfacilitateuniversalfunct ionapproximation combinedwithpiecewi seconstantrepresen inthissectionweconsideraselectionofnetworkarchitectureswhichplayarole inpracticalapplicationsandcanbehandledwiththealgorithmicapproaches troduced thedesignandtrainingofmultilayered deepneura orksforregr essio nandclassification popularshallownetworks sofar wehavedevelopedtrainingprescriptionsintermsofshallow rward architectureswithonlyoneorveryfewhidden thatasinglehidden layerissuﬃci enttoprovideuniversalfunct istheparitymachine forclassiﬁcation withhidden andoutputunitsofthemccul asoftversionofthecommitteemachine tivationcanbeshowntobeprovideuniversalfunct ionapproximationinthe contextofregression seesec tectureswithalternativehidden activation radialbasisfunctionnetworks radialbasisfunct ion rbf asactivationfunct ionshavebeenaddressed unit andlinearoutputunitsarereferredtoasrbfnetworks inthepopularcaseofgaussianrbfandasingle linearoutputwithbias σ ξ ξ ξ thiscorrespondstoeq j thecenterof therecept wealsoincludeanadaptive wecouldintroduce anadditional hidden unitwithconstantactivationφo ξ similartotheclamped inputempl rbfnetworksofthisformareuniversalapproximators seethegeneral discussi onin andspeciﬁcally wecanempl oynetworks ofthetype forgeneralregressiontasks thecomplexityoftherbfnetworkcanbeincrea sedbyallowingforadaptive inverse covariancematricesinthegaussianactivations φj ξ j nominally thisintroduces n fewerunitsmightberequiredtoachieve thesameaccuracyandperformanceasalargernetworkwithhidden tivationsoftheform strictiontodiagonalσjorthepoolingofcovariances withasingleadaptive paritymachine isstrictly speakingnotan architectur e seesec icarchitectu re input layer weight m hidden illustrationofanextreme learningmachine elm dimensi onalinput isconnect edtoahidden thesingleoutputunitislinear ofoutputsarestraightforward rbfarchitecturecouldbeused forclassiﬁcationtasksbyattaching asingleormultipleoutputclassiﬁertothehidden layer φj ξ m naturalapproachistonormalizethemactivationsinthehidden layerasin andinterpretthem asprobabilisticclassassignmen t ξ ξ ξ withφjfrom wecan trainthesystemaccordingtoaclassiﬁcationspeciﬁccostfunct ionlike forbinaryproblemsand remark rbfsystems forclassiﬁcationdisplayastrikingsimilaritywith prototypescorrespondtothecentervectorscjandthesoftmaxscheme ofthe classiﬁerwouldbereplacedbyacrispnearestprototypeclassiﬁcation npc similarly jin local extremelearningmachines frankrosenblattalreadysuggestedrandomizedconnect ionsfromaninput layer whichthenwasclassiﬁedbythresholdunitsin themarkirealizationoftheperceptron morerecently randomprojectionshavebeenbecomepopularasatechniqueto achievesparse onalrepresen onaldatasets seeforinstance rwardarchitecture termed theextremelearningmachine elm wasintroduced orksforregr essio nandclassification originalinput m encoder m dernetwork inputsarerepresen tedinahidden layerwithm n linear outputunitsrepresen theextensionto represen onal hiddenlayermakesit forinstance gressiontaskswithasinglelinear threshold unitwhichwouldnotbesuﬃci ent basicideaofthesupp ortvectormachine relationofelmand svmwasﬁrstdiscussed shallowautoencoders rwardtypeofnetworkscanbeused onal represen ξµ p wecan empl vectorsξinasinglehidden layerofm tionerrorinthedecoder ξµ trainingamountstothe minimizationoferecwithrespect tothenetworkweightswm themathema ticalstructureis thesameasinfunct ionapproximation withthespecialtargettoapproximate theidentityfunct onallatentvariables ym p onalrepresen onaldata tobelinear ticularlyinteresting forlinearactivationsginthehidden structionξrec theminimizationofthereconstructionerror icarchitectu re tothewellknownprincipalcomponentanalysis moreprecisely theweightvectorswm whichminimizeerec spanthesame ceasthemleadingprincipalcomponentsofthedataset dernetwork generalizestheconcept ofpca dimensi onalrepresen ders wher eseveralhidden layersrepresen t andprocessthedatainternally derimmedi ately forthepurposeofvisualizingcomplex dertorealizeafaithfulinternalrepresen tation rwardclassiﬁerorregressionnetworktothehidden layerandapplysupervisedlearningtorealizeatargetfunct ion deep andconvolutionalneuralnetworks ataglance thetermdeeplearningrefers rwardneural networkswithmanyhidden tionignores severalaspectsofdeeplearning ckandrecurrent system wewillfocushere ondeep goodfellow beng ioandcourville thereisno consensusabouthowmuchdepthamodelrequiresto qualifyas deep deeplearningcansafelyberegardedas chinelearningdoes theenormoussuccess ofdeeplearninganditspopularityafter beattributedtoanumberofdevelopmen t includingthefollowing availabilityoflargeamountsofdata largecollectionsofcommer ce worksfromunspeciﬁcdata base transfer learning singcomput ationalpower comput ersorlocalsolutions tof totheautomaticadaptationoflearningratesoreﬃcientregularization techniquessuchasdropout exploitationandcombinationofconcept sthathadbeendeveloped earlierforshallownetworks orksforregr essio nandclassification convolutional thedimensi onofthe data useofactivationfunct ionsthat supp osedly impro vetheeﬃciency andperformanceofnetworksintrainingandworkingphase forexample therectiﬁedlinearunit relu considerationofparticulararchitectures network cnn designedfortheanalysisofspeciﬁctypesofdata asimages language onal spatial temporalorfunct ionalstructure quiteafewofthese concept shavebeenknownwellbeforetheriseofdeep workspossible convolutionalandpoolinglayers allconvolutionalneura lnetworks cnn inimages ortimeseriesdata weexpectlocalizedinformation wise pixelsinaphotographicimageareexpectedtobesimilarinintensityand coloriftheybelongtothesameobject whilefarawaypixelsmaybetotally cnninthecontextofimages transfer tootherstructureddataisstraightforward thelocalizationisaccountedforbyconnect ingunitsinaﬁrstlayertolimited neighborhoodsorpatchesoftheinput data choiceoftheweightsinsuchaﬁlterkernel nodescanimplemen taparticular weightscanbeadaptedinthetraining termisused somewhat loosely icarchitectu re upper oraveragepooling lower process sizeintheinput thenumber ofadaptiveweightsdepends onlyonkandonthedimensi onandtypeofthe kernel whileitisindep enden tofthedimensi seebelow forfukushima sneocognitronasanexample ﬁrstconvolutionallayerinacnntherefo rerepresen tstheinput data intermsofmanyversionsoftheimageobtainedthroughapotentiallylargeset mayincludebutarenotlimitedto approximation intuitiveoperationslikethedetectionofedgesorotherlocalpatterns mostfrequently afterconvolution thedimensi onalitybycombining usuallysmall averagepooling orbythemaximum activityinthepatch thepoolingnodesarehardwired althoughonecouldforinstanceconsideradaptiveweighted averagesforpooling aftertheﬁrstconvolutionandpooling theinput imageisrepresen tedby anumberofﬁlteredanddimensi onreduced convolutional andpoolinglayersarestackedinalternatingfashion yieldingincrea stractrepresen tationsofdecreasingdimensi abledenselayers fullyconnect rwardarchitectures areempl oyedtorepresen tthetargetclassiﬁcationorregression alargenumberofnetworksimilararchitectureshavebeendevelopedand aremadeavailableinthepublicdomain seehttps uned bytheuser forthespeciﬁctaskathand technique deepnetworksoftencompriseahugenumberofadaptiveweights itappearsoftensurpri orksforregr essio nandclassification anearlydeep architecture schema tic nitron ﬁrstintroduced besidesinput andoutput recognition withkindpermissionfromtheauthor lyatall andsuﬀer sthanonemightexpecton theoreticalgrounds earlyexamples neocognitronandlenet inspi redbyanearlymodelofhumanvisionofhubelandwiesel hikofukushimaintroduced injapanese seeprecedi foramorerecentpresentationanddiscussi onofdiﬀerentversionsofthebasic architecture thenetworkconsistsof aninput andoutputlayer withstackedhidden aretypicallyconnect edtopatchesofnodesintheprecedi tractionlayers empl oylocalﬁlterstothese patch wiesel whichareactivated forinstance bycharacteristicpatternslikestraight uentlayerofcomplexor penden tofthepreciselocationofthestimulationwithintheirrecept iveﬁeldin theprecedi l tronis toacertaindegree insensi tivetoshiftsanddistortionsofinput icarchitectu re adeep architecture schema tic knownaslenet speciﬁcally introduced bylecun license sequence ofsandclayersrepresen tstheinput indecreasingdetailand ultimately thenetworkresponse isprovidedintheoutput fukushimadidnottrain ttechniques instead theﬁlters ervised learningtechniques theneocognitronhasbeenstudiedandused patternrecognition includinghandwr ittendigitrecognitionandsimilartasks itconstitutesagroundbr eakingworkthatinspiredmany ifnotallmodern deep networksforvisualpatternrecognitionandsimilartasks anothergroundbr eakingarchitecture knownaslenetisduetolecun collaborator work cnn forpatternrecognitionandimageprocessingandcanbeconsidered lartotheabovediscussed layersprocess aninput withincrea singabstractiontowardsafullyconnect ed bygradientbased forthetaskofhandwr ittendigit recognition dereading groupsofnodesperformthesametaskondiﬀerentpatchesoftheinput consequently manynodescansharethesameweightvalueswhichreduces theeﬀect whichtodateplaysanimportantrole inthetrainingofdeep network seealsosec vemen tsof thenetworkperformancewereachievedbydevelopingandapplyingaspeciﬁc pruningtechniquenamed optimalbraindamage whichweintroduce appraisalandcritiqueofdeeplearning success ofdeeplearning mainlyinthecontextofimageprocessing triggeredalotofexcitemen tin academi includes orksforregr essio nandclassification incriticalareaslikeclinicalmedi cine recently severalscholarshaveexpressed criticismofdeeplearningandthe hypesurroundingit beingjustafewexamples inasense thesituationishighlyreminiscentofthestrongexpectationsandthe laterdisappointmentsinpreviouswavesofmachinelearningpopularity inthebiasedopinionoftheauthorofthistext severaldeplorabletrends canbeobservedinacademi aandinthegeneralpublic supp osedlyoldschoolmachinelearning ignoringdecadesofresearchthatfacilitatedtherecentdevelopmen t scene analysis pealingproblems machinelearningshouldbeseen fromamuchbroader perspective dtendency tivelysimple speciﬁcapplicationswithoutinvestingathoughtfulanalysis parisonwithbaselinetechniques tswhilehaving alimitedunder standingofthebasicunder suchsystemsoﬀeronlylimitedusercontrolortheoptionsarenotexploited properly ofinterestintoabetterunder standingoftherelevantpheno mena lastpointsareparticularlyunfortunateinviewofthemanyinteresting challengesandopenquestionsposedbydeeplearning marizedin despitethese andotherpointsofcriticism new tinuetoprovidehighlyinterestingtheoreticalchallengesthatdeser vesigniﬁcant attention thisisoftencombined displa yingcodeinillegible small butcolor fulfonts classiﬁers onecanstate withoutexaggeration thattheobservationofandthesearchfor similaritiesanddiﬀerences arethebasisofallhumanknowledge useofdistances ordissimilaritiesforthecomparisonofobservations withasetoflabeledreference datapointsprovidesasimpleyetpowerfultool theuseofprototypesorexemplars derivedfrom agivendataset isthebasisforaverysuccessful familyofmachinelearning approach areappealingforanumberofreasons calrepresen tatives theprototypes isparticularlytransparentandintuitive contrasttomany thesameistruefortheworking phase inwhichnoveldataarecomparedwiththeprototypesbyuseofasuitable similarityordistancemeasure prototypesystemsarefrequentlyempl oyedfortheunsup ervisedanalysisof complexdatasets aimingatthedetectionofunder lyingstructures suchas clustersorhierarchicalrelations forinstance n totypesinthecontextofunsup ervisedlearning inthefollowingtheempha system inparticular tization lvq ingprescriptionswepresentextensionsoftheframeworktounconventional distance andtotheuseofadaptivemeasures scheme theaimofthischapterisfarfromgivingacompletereviewoftheongoing funda sed classifier leftpanel illustrationofthenearestneighbor nn ﬁerforanartiﬁcialdatasetcontainingthreediﬀerentclasses rightpanel acorrespondingnpcscheme ted arebasedoneuclideandistanceandyield piecewi selineardecisionbounda ries atbest ﬁrstinsightsintosupervisedschemes andcanserve asastartingpointfortheinterestedreader theempha siswillbeonteuvokohonen slearningvectorquantizationand itsextensions ofunconventionaldistancemeasures isdiscussed asanimportantconcept ual extensionoflvq relevancelearningisintroduced withmatrixrelevance lvqservingasanexample inasense thephilosophiesbehindlvqandthesvmarediametrically opposedtoeachother whilesupp ortvectorsrepresen tthediﬃcultcasesin thedataset whichareclosest tothedecisionbounda ry lvq represen arsrelativelyfarfromthe classborders notethatlvqsystemscouldbeformulatedandinterpretedaslayeredneura l networkswithspeciﬁc sedactivationsandacrispoutputreﬂect ing afteryearsofdenyingtherelationin theliterature ithasbecomepopularagaintopointouttheconcept ualvicinity theembeddingoflvq modulesindeep learningapproaches sed classifier amongthemanyframeworksdevelopedforsupervisedmachinelearning ﬂexible andeasytoimplemen althoughwerestrictthediscussi ontoclassiﬁcationproblems ceptscarryovertoregressionor toacertainextent alsotounsup ing havebeenconsidered intheliterature someofthem ervisedschemes likethe whichcanbe extended thefocusis lvq aframeworkwhichwas originallysuggestedbyteuvokohonen cussi knn approach toclassiﬁcation nearestneigh borandnearestprotot ypeclassiﬁers nearestneighborclassiﬁers constituteoneofthesimplestand mostpopularclassiﬁcationschemes inthisclassicalapproach anumberof labeledfeaturevectorsisstoredinareference set ξµ ξµ p onoftheperceptronandsimilar system therefo redenotethe c wher ec isthenumberofclasses cordingtoits similaritiestothesamplesstoredinthereference data tothisend itsdistancefromallreference ed mostfrequently thesimple squared euclideandistanceisused inthiscontext d ξ ξµ query ξisthenassignedtotheclassofitsnearest neighborexempl theassignmen ti determinedbymeansofavotingscheme thatconsiders thekclosestreference vector nnorknnclassiﬁerisobviouslyveryeasytoimplemen tasitdoes approachbearsthepotentialtorealizebayesoptimalperformanceifthenumber kofneighborsischosencarefully methodserves todate asanimportantbaselinealgorithmandisfrequently used asabenchmarktocompareperformances leftpanel illustratesthennclassiﬁeranddisplayshowthesystem implemen tspiecewi selinearclassborders severaldiﬃcul tiesareevidentalready canbeoverlycomplex forinstanceif sed classifier factthateverydatapointcontributeswithequalweightcanleadtooverﬁtting eﬀect asaconsequence itmightnotperformwellwhen presentedwithnovel unseen data straightforwardimplemen tationsofknncomput eandsortthedistances reduce thecomput ationalcoststoacertaindegree theproblempersistsandis deﬁni telyrelevantforverylargedatasets bothdrawbackscouldbeattenuatedbyreduci ngthenumberofreference datainanintelligentwaywhilekeepingthemostrelevantpropertiesofthedata theselectionofasuitablesubset ofreference vectorsbythinningout dwasalreadysuggestedin essen approachisconsidered inthefollowingsections learningvectorquantization thissuccessful duced andputforwardbyteuvokohonen databyrelativelyfew represen tativeprototypevectors lvqwasoriginallymotivatedasasimplifyingapproximationofabayes classiﬁerunder theassumpt ionthattheunder lyingdensi tyofdatacorresponds toasuperpositionofgaussians mationbyasimpleandrobust ofthecclassesistoberepresen tedby atleast onerepresen weconsiderthesetofprototypevectors wj c wj indicatewhichclassthecorresponding prototypeissupp osedtorepresen npc assignsanarbitrary oftheclosestprototype ξ ξ d wj ξ wher etiescanbebrokenarbitrarily inthefollowing ξ ofagiveninput vectorwillbe tof ξ itisobviouswhichinput vectoritrefers rightpanel illustratesthenpcconcept classborders spondingtorelativelyfewprototypesaresmootherthanthecorrespondingnn decisionbounda sed classifier canbeexpectedtobemorerobust andlesspronetooverﬁttingeﬀect theperformanceoflvqsystemshasproventobecompetitiveinavarietyof practicalclassiﬁcationproblems prototypesareobtainedandcanbeinterpretedwithinthespaceofobserved data onwithdomainexpertsand standsincontrasttomanyother lesstransparentmachinelearningframeworks anlvqsystemfornearestprototypeclassiﬁcationcanbeinterpretedasa neura lnetworkwithasinglehidden forexample itisnotessen tialforthefollowing lvqtrainingalgorith m sofarwehavenotaddressed thequestionofwher havebeen suggestedintheliterature bgh theﬁrst originalscheme suggestedbykohonen essen tially italreadyincludes allaspectsofthemanymodiﬁcationsthatwere algorithmcanbesumma rizedintermsofthefollowing step randomsequentialpresentationofdata selectasinglefeaturevectorξµwithclasslabelyµ ξµ givenbyd µ ξµ d wj ξµ µ wta update µ µ t µ yµ withψ c y updatestep movesthewinningprototypeevenclosertothepresented sed classifier ψ µ yµ µismovedfartherawayfromξµifthe winningprototyperepresen tsaclassdiﬀerentfromyµ yµ nditional meanvectorsinthedataset wj yµ cj yµ cj j oyedperclass indep enden trandomvariationscouldbeadded totypes escanberealized nsprocedur eineachclassseparately afterrepeatedpresentationsoftheentiretrainingset theprototypesshould represen ttheirrespectiveclassbyassumi space ideally numer ousmodiﬁcationsofthisbasiclvqscheme havebeenconsidered theliterature seeforinstance bgh erences therei funct ionshavebeensuggested allowfortrainingintermsofgradient descen torotheroptimizationschemes ticschemes cannotbeinterpretedasdescen talgorithms inastraightforward fashion oneparticularcostfunct soft lvq rslvq elling populargener alizedlvq glvq isguided byanobjectivefunct ionthatrelatestotheconcept tion eµ wj µ ξµ wk µ ξµ d wjµ ξµ wkµ ξµ thevectorwj µdenotestheclosestofallprototypeswhichcarrythesame labelastheexampleξµ wk wewillfrequentlyrefertothese vectorsasthecorrectwinnerwj µandtheincorrectwinnerwk µ respectively thecostfunct ion increa singfunct ionφ e e whiletheauthorsof suggesttheuseofasigmoidalφ e exp wher eγ negativevalueseµ correctlyclassiﬁedinthenpcscheme sincethend wj µ ξµ d wk µ ξµ largevaluesofthesteepness γthecostsapproximatethenumberofmisclassiﬁed theargumen sed classifier trainingdata whileforsmallγtheminimizationofeglvqcorrespondsto apopularandconcept uallysimplestrategytooptimizeeglvqisstochastic gradientdescentinwhichsingleexamplesarepresentedinrandomizedorder twoprototypesareupdatedineach stepoftheglvqprocedur e generalize dlvq glvq stochasticgradientdescen t selectasinglefeaturevectorξµwithclasslabelyµ wj µwithd wj µ ξµ d wj ξµ wk µwithd wk µ ξµ d wj ξµ withclasslabelscj respectively wj µ µ t eµ wk µ µ t eµ wher ethegradientsareevaluatedinwl µ t k forthefullformofthegradienttermswereferthereaderto arealongthegradients wl µ ξµ wl k aregivenbyψ cl yµ glvqperformsupdateswhichmovethecorrect incorrect prototypetowards away fromthefeaturevector thebasic concept veryoftenadecreasinglearningrateηwisused toensur econvergence oftheprototypepositions scheme odscanbeapplied whichwewillnotdiscuss sed classifier sofar thediscussi onfocussed oneuclideandistanceasastandardmeasure thatother athand unconventionalmeasures mightoutperformeuclideandistancebyfar theselectionofaspeciﬁcdistanceconstitutesakeystepinthedesignof thepossibilitytochooseadistancebasedon priorinformationandinsightintotheproblemcontributestotheﬂexibilityof theapproach lvqbeyondeuclideandistance asdiscussed callyyieldprototypedisplacemen tsalongthevector replacingtheeuclideandistancebyamoregeneral diﬀerentiablemeasure δ ξµ w allowsfortheanalogousderivationoflvqtrainingschemes isconvenientlydoneincostfunct ionbasedschemes likeglvq plehere asageneralizationofeq weobtaintheanalogouswtaupdate fromexampleµattimet µ µ t µ yµ µ ξµ µ obviously µhastobedeterminedbyuseofthesamemeasure δ forthesakeofconsistency alongthese line ilarities providedthedistanceδisdiﬀerentiablewithrespecttotheprototype aminimalcondition ativityδ w ξ andδ ξ ξ notethatcostfunct ionbasedapproachescanalsoempl measure quiretheuseofgradients diﬀerentiableapproximations foradiscussi onthereo inthefollowing dardeuclideanmetricsthathavebeenused inthecontextoflvqclassiﬁers referto formoredetaileddiscussi onsandfurther reference statisticalpropertiesofagivendatasetcanbetakenintoaccountexplicitly byempl measure detaileddiscussi onandseveralapplicationexamples standardminkowskidistances suresandrelevancelearning dp ξ forξ whichincludes smaller valuesofpputempha smaller ξ veperformanceinseveralpractical application forspeciﬁcexamples thesquaredeuclideandistancecanberewrittenintermsofscalarproducts d w ξ replaceallinner productsin bya kernelfunct ionκ dκ w ξ w w w ξ ξ ξ asinthesvmformalism thefunct mensi svmtrainingonetakesadvantageofthefactthatdatacanbecomelinearly separableduetothetransformation asdiscussed insec kernel distance canbeempl oyedinthecontextoflvqinordertoachievebetter classiﬁcationperformance foraparticularapplication asalastexample statisticaldivergences canbeused similarityofdensi imagedataisfrequently characterizedbycolororotherhistograms similarly textcanberepresen ted byfrequency cationproblems histogram euclideandistanceisfrequentlyinsensi theclassiﬁcationperformancecan beneﬁt fromusingspeciﬁcmeasures suchasstatisticaldivergences isjustoneexampleofmanymeasures andanexample applicationinthecontextoflvqsee e itisalsodemo ricdivergences canbeempl oyedproperlyinthe contextoflvq aslongasthemeasures areused inaconsistentway adaptivedistancesinrelevancelearning intheprevioussubsect ion cussed inpractice aparticularonecouldbeselectedbasedonpriorinsights oraccordingtoanempi ricalcomparisoninavalidationprocedur sed classifier prototypesystems andwaswasintroduced andputforwardinthecontextof lvqin forinstance relevancelearninghasprovenuseful inavarietyofapplications including biomedi calproblemsandimageprocessingtasks seeforinstance inthisveryelegantapproach onlytheparametricformofthedistance measure isﬁxedinadvance itsparametersareconsidered adaptivequantities whichcanbeadjust ideaisveryversatileandcanbeempl exampleinthecontext ofsupervisedlearning similartoseveralotherschemes matrix relevancelvqempl oysageneralizedquadraticdistanceoftheform δλ w ξ λij heuristically diagonalentriesofλquantifytheimportanceofsinglefeature dimensi nitudes t whichreﬂect theinterplayofthediﬀerentdimensi recoversthesimplesquaredeuclideandistance inordertofulﬁlltheminimalrequiremen ativity intermsofanauxiliary unrestricted w ξ ω δλcanbeinterpretedastheconventionalsquaredeuclideandistance deﬁne onlyapseudo λ n implyingthatδλ w ξ obviously wecouldempl oyaﬁxeddistanceoftheform inglvqor keyideaofrelevancelearning istoconsidertheelemen numer restrictiontodiagonalmatricesλcorresponds totheoriginalformulationofrelevancelvqin whichassignsasingle ativeweightingfactortoeachdimensi ncanbeused rankrelevancematrix onalintrinsic represen tationofdatafacilitates forinstance tionofcomplexdata suresandrelevancelearning signiﬁcantlywhen localdistances areused separaterelevancematrices areempl oyedperclassorevenperprototype herewerestrictthediscussi ontothesimplestcaseofasingle heuri sticextensionofthe anditsconvergence behaviorisanalysedin gradientbasedupdatesforthesimultaneousadaptationofprototypesand relevancematrixcanbederivedfromasuitablecostfunct w ξ w ξ fullformsofthegradientswithrespecttothetermseµintheglvq costfunct ionarepresentedin forinstance gener alizedmatrixrelevancelvq gml vq scheme whichcanbeformulated asastochasticgradientdescen tprocedur e generalize dmatrixlvq gmlvq stochasticgradie ntdescent selectasinglefeaturevectorξµwithclasslabelyµ t t identifythecorrectandincorrectwinners wj µwithδλ wj µ ξµ δλ wj ξµ wk µwithδλ wk µ ξµ δλ wj ξµ withclasslabelscj respectively wj µ µ t eµ wk µ µ t eµ ω t eµ wher ethegradientsareevaluatedinω t andwl µ t k inbothgml therelevancematrixisupdatedinorder todecreaseorincrea seδλ wl µ ξµ forthewinningprototype s dependingon theclasslabelsinthe bynow sed classifier visualizationofthegener alizedmatrixrelevancelvqsystem retransformed irisﬂowerdataset seesec detail leftpanel classprototypesareshownasbarplotswithrespecttothefour featurespacecomponentsintheleftcolumn therightcolumn valuespectrumofλ thediagonalelemen tsofλ t tation toptobottom rightpanel eigenvectorsoftherelevancematrixλ frequently thelearningrateofthematrixupdatesischosentoberelatively small inthestochasticgradientdescen tprocedur intuitionthattheprototypesshouldbeenabledtofollowchangesinthedistance measure relativescalingcanbediﬀerentinbatchgradientrealizationsof gml vqasforinstancein dentrandomelemen icaldiﬃcul tie anormalization inthefollowingweillustratematrixrelevancelvqintermsofaclassical fournumer ical featuresareused providedat andused oneprototypeperclassandaglobalrelevance weempl oyedthefreelyavailablebeginner s toolboxforgml vqwithdefaultparametersetting retransformationwasapplied ledfeatureswithzero meanandunitvarianceinthedataset seesec immedi ateinterpretationoftherelevances withouthavingtotakeintoaccount thepotentiallydiﬀerentmagnitudes ofthefeatures classiﬁer resultinglvqsystem achievesalmostperfect rks displaysverygoodgeneralizationbehaviorwithrespecttovalidationortestset performancenotpresentedhere intheleftpanel theprototypesaftertrainingandtheresultingrelevance mentsλiicanbeinterpretedastherelevanceoffeaturesiintheclassiﬁcation apparently lem tsrepresen tthecontributionofpairsofdiﬀerent inmorerealisticandchallengingdatasets vqhasbeenexploitedtoidentify themostrelevantorirrelevantfeatures nosisproblems inthecontextofgalaxyclassiﬁcationbasedonastronomicalcataloguedatais presentedin onalfeaturespace thegml duce o additionaladaptivequantities asaconsequence onemight expectstrongoverﬁttingeﬀect asobservedempi ricallyandanalysedtheoretically therelevance matrixdisplaysastrongtendency tobecomesingularanddisplaysverylow rank λ canbeinterpretedas animplicit intrinsicmechanismofregularization whichlimitsthecomplexity ofthedistancemeasure eﬀect ively inaddition sualizationofthedatabyprojectingfeaturevectors andprototypes ontoits dataset rks forwardadvanced machinelearningtechniques weencouragethereaderto explorerecentdevelopmen suchas theanalysisoffunct ionaldata toname onlyveryfew arecurrentlybeingaddressed forfurther reference atthesametime excitingapplicationareasarebeingexploredina largevarietyofdomains mostrecently inthecontext ofdeeplearning sedmodulesappearsvery forinstance andreferences therei sed modelevaluationand regularization accuracyisnotenough insupervisedlearningtheaimistoinfer relevantinformationfromgiven data toparameterizeitintermsofamodel andtoapplyittonoveldata successful tialtoknoworatleasthavesomeestimateof theperformancethatcanbeexpectedintheworkingphase inthischapterwediscuss wetakearathergeneralperspective onoverﬁttingandunder ﬁttingeﬀect swithoutnecessa beyondthesimpleoverall accuracyfortheevaluationofclassiﬁers andregressionsystems arepresented weaddresstheimportanceofinterpretablemodelsinmachine diﬀerentsourcesoferrorcaninﬂuence theperformanceofsupervisedlearning system tributions deviationsoftrainedmodelsfromthetruetarget whilethetermvariancerefers tovariationsofthemodelperformancewhen trainedfromdiﬀerentrealizations andregu larization da db dc rowofgraphs xi yi squarelinearregression byacubicﬁt andbyﬁttinga polynomialofdegreeseven indep enden tlygenerateddatasetsda b c frequently asathird penden tcontribution forinstance stemfromintrinsicnoise inthetestdatawhichcannotbepredictedevenwithperfectknowledgeofthe werefrain fromincludingitinthediscussi decomposition oftheerror forthepurposeofillustration mensi onalregression problem obtainedinsights carryovertomuchmorecomplex system inourexample leastsquaresﬁtsarebasedondatasetsoftheform xµ yµ p valuesin datasets thetermsbias andvariance areused inmanydiﬀerentscien tiﬁccontextswith areaspeciﬁc g represen tafunct ionf x whichisofcourseunknowntothelearningsystem weassume thatthetraininglabelsarenoisyversionsofthetruetargets xµ thedeviationofthetraininglabelsfrom theunder lyingtargetfunct ionisgivenbyuncorrelated t wecould forinstance considerindep enden tgaussianrandomnumberswith ρ fk x xµ yµ p ajcanbedeterminedbyminimizingthefamiliarquadraticdeviation fh xµ b xµandnoisyyµrepresen tingtheunder lyingtargetfunct ionf x eachofthethreeslightlydiﬀerentdatasets polynomialleastsquareﬁtswere performed linear cubic thesamedatasetswereanalysedbyusingmodelsofdiﬀerentcomplexity inordertoobtainsomeinsightintotheinterplayofmodelcomplexityand expectedperformance weconsiderthethoughtexperimentofperformingthe foraverylargenumberofslightlydiﬀerentdata setsofthesamesize whichallrepresen tthetarget tyofthetrainingdatain theexpectedtotalquadraticdeviationofahypothesisfunct ion fh x x d wher etherandomness ofdisreﬂect thefollowing argumen twouldproceed incompleteanalogyduetothelinearityof weobtain h x x x x notethatthetruetargetf x obviouslydoesnotdependonthedataandcan andregu larization forthesakeofbrevity weomittheargumen ionsfhand ntterms whichadduptozerowecan rewrite x andobtainadecompositionoftheexpectedquadraticdeviationinx variance equalitywith isstraightforwardtoshowbyexpandingthesquares wecanidentifytwocontributionstothetotalexpectederror squared biastermquantiﬁesthedeviationofthemeanpredictionfromthe truetarget wher etheaverageisovermanyrandomizeddatasetsand correspondingtrainingprocesses asmallbiasindicatesthatthere isvery littlesystematicdeviationofthehypotheses fromtheunknowntargetrule d thevariancemeasures howmuchtheindividualpredictions obtainedafter trainingonagivend observationofasmallvarianceimpliesthattheoutcomeofthelearning isrobust withrespecttodetailsofthetrainingdata similarconsiderationsapplytomoregenerallearningproblems including classiﬁcationschemes ideally wewouldliketoachievelowvarianceandlowbiasatthesametime arobust andfaithful legitimate butveryoftentheyconstituteconﬂictingaimsinpractice asfurther illustratedinthefollowing unnecessa rilycomplexsystemsanditscounterpart ﬁttingin simplisticmodels concept leftpanel theillustration theﬁtsoftwodiﬀerentmodelsaredisplayedinthespaceof adaptivequantities lnetwork whilebiasandvariance aredeﬁned wecanassume thatthe g modelb target predictionerror testset training setperformance ameasure ofthemodelcomplexity leftpanel true targetisrepresen ferentdatasetsdiinmodela opencircles showlowbiasandlargevariance whilemodelb ﬁlledcircles schema ticillustrationofunder ﬁttingandoverﬁtting ter expectederrorwithrespecttoatestset generalizationerror andtrainingsetperformanceasafunct ionofthemodelcomplexity overﬁtting forinstance wecangeneratemodelswhichperfectlyreproducethedatapoints fk xµ ineachindividualtrainingprocess beca usethetraininglabelsthemsel vesareassumed tobeunbiasedwith xµ theaveragedﬁtresultwillalsobeinexact agreemen twiththetargetintheargumen sincetheobjective funct ion ofthetrainingtreatspositiveandnegativedeviationssymmet callyaswell isnoreasontoexpectsystematicdeviationsoftheﬁtswith fk x f x orfk x f x forallﬁtsinsomearbitraryvalueofx usingaveryﬂexiblemodelwithlargekwillresultinﬁtswhich right column verydiﬀerentmodelsemer ge fromtheindividualtrainingprocesses forthesamplepointsthemsel f thevarianceofthenearlyperfectﬁtwould beessen eq xµ p thediﬀerentﬁtswillvaryalotintheirpredictionfk x forinstance exampleoftheeﬀect underﬁtting ifempha sisisputontherobustness ofthemodel wewould prefersimplemodelswithlowdegreekin andregu larization column weachievenearlyidenticallinearmodelsfromthediﬀerent apriceispaidfortherobustness systematicdeviations occur ineachtrainingprocedur forinstance thatthelinear ﬁts leftcolumn obtainedfromda b linearfunct ion deviationsaresystematicinthesense theyarereproduced fortheﬁrst x interpolationandextrapolationwillalsobesubjecttosystematicerrors matchedmodelcomplexity inourexample toconstituteanidealcompro mise inthesamplepoints dency consequently x appearalsorobust againstvariationsofthedataset correspondingto arelativelysmallvariance thisisofcoursenotsurpr ising matchthecomplexityoftheunder lying truetargetfunct realizethatthiskindofinformationisrarelyavailableinpracticalsituations infact inabsence ofknowledgeaboutthecomplexityofthetargetrule itis oneofthekeychallengesinsupervisedlearningtoselectanappropriatemodel thatachievesagoodcompromisewithrespecttobiasandvariance inthe followingsectionswewillconsideravarietyofwaystocontrolthecomplexity ofalearningsystemwithempha rwardneuralnetworks aboveconsiderationssuggestthatthere smallvarianceandbias inmanymachine learningscena rightpanel ticallythepossibledependence oftheprediction performanceinthetrainingsetandthegeneralizationerror mance asafunct ionofthemodelcomplexity inoursimpleexample wecouldusethepolynomialdegreekasameasure forinstance thedegreeofa polynomialkernelinthesvm thenumberofhidden l network ofthetrainingalgorithm gener ically weexpectthetrainingerrortobelowerthanthegeneralization theactualoptimizationprocessisbasedonthe availabletrainingexamples simplisticmodelsthatcannotcopewiththecomplexityofthetaskdisplay g model sﬂexibilitywillreduce thebiasand consequently trainingandtestset rightpanel overlytrainingset speciﬁcmodelsdisplayoverﬁtting whilethetrainingerrortypicallydecreases furtherwithincrea singk whichreﬂect stheincrea seofthemodelvariance itisimportanttorealizethattheextenttowhichtheactualbehaviorfollows thescena rioinapracticalsituationdepends onthedetailedpropertiesofthe dataandtheproblemathand whileoneshouldbeawareofthepossible theplausibilityoftheabovediscussed decomposition itselfdoesnotimplytheexistence strictly speaking asarguedanddemo agivenpractical problemdoesnotnecessa rightpanel eisalsonogeneralguarantee thatmeasures whichreduce thevarianceincomplexmodelswillreallyimpro ve theperformanceofthesystem theassumed becontrolledtoacertaindegree andmayserveasaguidingprincipleforthemodelselectionprocess accordingtotheaboveconsiderations areliableestimateoftheexpected generalizationabilitywouldbehighlydesirableinanygivensupervisedlearning scena tobeabletocompareandevaluatetheuseof diﬀerentapproaches theexpectedperformanceshouldguidetheselectionofmodelparameterslike thenumberofhidden aimistoselectthemost suitablestuden appliestoselectingatrainingprocedur eandsuitableparametervalues learningrate insec theunreasonableeﬀectivenessofdeeplearninginartiﬁcialintelligence numbersoflayers thecurrentlyverypopular largelanguagemodelscancomprisebillionsofadaptiveparameters seetable previoussections onewouldexpectseriousoverﬁttingeﬀect sinsuchextremely powerfulsystems inpractice temsaretrainedandappliedwithgreatsuccess inthiscontext apublicationbybelkinetal hasattracteda authorsdiscuss overﬁttingscena thetesterrorfrequentlyunder andregu larization predictionerrortesterror training error numberofparameters modelcomplexityinterpolation threshold classical regime modern regime illustrationofthedoubledescen tpheno meno n descen tasafunct toccurs towhatissometimes t foragivenproblemordataset modelsof increa singcomplexityareconsidered similarpeakingeﬀect scanbeobserved wisedoubledescen t doubledescen tisthesubjectofongoingdiscussi onsandapparentlyhasled severalresearchers andpracticioners tothesomewha thastyconclusionthatthe itisoften assumed thatdoubledescen tisarelativelynovelpheno meno eredspeciﬁcallyindeeplearning motivatingthetermsclassicalandmodern asalreadymentionedin doubledescen t occursalsoinmuchsimplersettings taryregressionsystems theauthorspresentabriefprehistoryof doubledescent pointingoutthatithadbeenobservedalreadyinbasiclearning problemslikelinearregressionorperceptrontraining plausibleexplanationsfortheoccurrence ofdoubledescen dependingonthedetails ofproblemandmethod terswiththecapacityorcomplexityofthemodel aswesuggestivelydidinfig danielawittenpresentsaninsightfuldiscussi oninterms ofﬁttingcubicsplinestoanumberofdatapointsin atwitterthread ther e theinterpolationthresholdcorrespondstothesituationinwhichthe ogoustootherregressionorinterpolationschemes suchasthepoynomialﬁts discussed insec isonlyone possiblesolutionforagivendatasetandtheresultingmodelisverysensi tive manyﬁtsare cientsrestrictstheﬂexibilityofthesystemdrastically resultingintheobserved peakinganddoubledescen ﬁttingunder orkcompl exity ularization wouldnotnecessa rilydisplaythepeakinganddouble descen t inﬂuence ofimplicitandexplicitregularizationonthe emer gence ofdoubledescen tisalsodiscussed inthecontextofordinaryleast squaresregressionin ingeneral tioncanplayanimportantroleindeterminingtheeﬀect tem importantconclusionisthat ifthemodelcomplexityistakeninto accountcorrectly sofarwehavediscussed thecontrolofthestuden tcomplexityintermsofthe actualmodeldesign thenumber ofprototypesinlvq orthesizeofahidden choicesaremadepriortotheactualtrainingandcanbeevaluatedbycomparing diﬀerentsettingsaftertraining inavarietyofapproachesthe eﬀect ive complexityofalearningsystem iscontrolledbyimposingconstraintsonthetrainingprocess twobasicmethods ofweightdecay latterisanimportantexampleforregularizationby introducingapenaltytermintotheobjectivefunct wewillusethetermregularizationmoregenerallyforallmethodsof implicitorexplicitcomplexitycontrol constructivealgorithms whichincorporatetheadditionofunitsorlayers intothetrainingprocess arediscussed insec esthatremoveunne cessaryweightsorunitsduring twotechniquesthatareparticularrelevantinthe contextofdeeplearning arepresentedinsec respectively inpractice allthese methodsrequireorbeneﬁt fromreliableestimatesof assume thatsuchestimatesareavailable forinstancebycomput ingsuitable errormeasures onalargerepresen earlystopping aconcept uallyverysimpleideaistoendthetrainingprocessbeforethesystem entdescen tupdatesafterasuitablenumbertmaxofepochs theresultingweight conﬁgurationmaydisplayarelativelylowvalueoftheobjectivefunct ion withoutrepresen andregu larization w tmax w schema correspondtocontourlinesoftheobjectivefunct resenttheunrestrictedhypotheticalupdatesby forinstance gradientdescen leftpanel aftertmaxepochs thetrainingisstopped whichhinders theweight reddot rightpanel weightsareinitializedtabularasaandrestrictedtosmallnorms panel showsanillustrationoftheeﬀect ofearlystoppingontheoptimization process beingoneofthemostintuitiveconcept sofregularization earlystopping hasbeendiscussed veryearlyinthecontextofneura lnetworks seeforexample onandfurtherreferences canbefoundintext booksaswell earlystoppingparametertmaxplaysarolethatiscomparabletothe degreekintheexampleofpolynomialﬁts parametersthatcontrolthetrainingprocess parametersinordertodistinguishthem fromtheactualadaptivequantities inordertosetdiscreteparameterlikethenumberofhidden unitsinthe network ontheﬂyandstopassoonasovertrainingeﬀect properchoiceof beenaddressed intheliterature forexamples forgeneraldiscussi ons weightdecayandrelatedconcepts wehaveencounteredweightdecayasaregularizationtechniquealreadyinthe discussi onofsimplelinearregressioninsec e apenaltytermwas addedtotheobjectivefunct thiscanbe orkcompl exity basisofassumi ory weightdecayfacilitatestheconstructionorcomput tionoftheregressionproblem hereweextendtheconcept totheimplemen ionsin sticallymotivatedpenaltytermsarediscussed atthe endofthissubsect ion concept gressionproblems lnetworks ofweightdecayhasalsobeen investigatedinmodelscena tive forexamples comparedtothecaseoflinearregression theformulationofweightdecay basedonstatisticallearningtheory theheuristicinterpretationremainsvalid tivelyandthuslimitstheeﬀect right panel illustratestheeﬀect eﬀect ion g x w expansionimpliesthattheactivationisapproximately g x theactivationiseﬀect ivelylinearized bythesameargumen t theoutput willbecomenearlylinear ifthemagnitudeoftheweightsareverysmall ifwerepresen dientdescen twithrespectacostfunct ione w wecanlimitthenormofw heuri sticallybyreduci ngthemagnitudeofweightsinoraftereachupdatestep w t t withthe small weightdecayparameterγ pretedasgradientdescen twithrespecttoamodiﬁedobjectivefunct ion ηw andregu larization whichalsoleadsto asanalternativetotheuseofapenaltyterm aconstraintof thiscanbedoneexplicitly byprojectingbackontothespher erthe isconsidered incombinationwithdropout decayasgivenbyeqs canbeinterpretedasasoftimplemen tation variantsofweightdecay followinganargumen thepenaltyterm jineq oweightsofsimilarmagnitude comparingthepenaltyofapairofweights tothatof inthecontextofsparseclassiﬁers orregressionsystemstheaimisasystem withasigniﬁcantfractionofzeroorverysmallweights whichcouldberemoved thiscanbeachievedbyempl oyingmodiﬁedpenaltytermsandupdaterules forinstancetheonediscussed j ηwk k comparedto themodiﬁedweightdecay aftertraining canserveasamethodforpruningtheneura methodsfortheremovalofunne cessaryweightsinatrainednetworkarebrieﬂy regularization wher ethepenaltyisgivenbythemoregeneralterm jwp eq extension p familiar thescaling ofγwith ηmerelyguaranteesformalequivalence eq orkcompl exity lasso forathoroughdiscussi heuri sticpenalty regularizationcanalsobeused toenforcesomeweightstobecomeexactlyzero itthusalsorelatestofeatureselectionandpruning constructivealgorith m inthecontextofclassiﬁcationwehavediscussed constructivealgorithms scheme unitsorlayers areadded tothenetworkuntilthegiven labeleddatasetcanbeimplemen ted suggestedmethodsandcorrespondingreferences canbefoundin akeyissue ofallconstructivealgorithms istheneed toavoidoverﬁttingdue rrelationscheme addshidden unitsoneatatime incontrasttotheconstructionin theadded nodereceivesinput fromallprevioushidden new unitistrainedbymaximizingthecorrelationofitsoutputwiththeresidual errorachievedsofar speciﬁcalgorithmcanlead toanarchitecturewithmanysingleunithidden layer networkasopposedtotheshallowandwidearchitecturesconsidered insec pruning theconcept ofpruningortrimminganeuralnetworkisdiametricallyopposedto thatofconstructivealgorithms theideaofpruningistoﬁrsttrainarelatively complexsystemwhichiscapableofrealizingthedesiredtasktoasatisfactory overﬁtting trainednetworkshasbeenconsidered veryearly forreferences pruningisusuallydoneaftertraining andthesystemmayberetrained therea itcanberealizedinintermedi atestepsofthetraining procedur animportantingredientofneuralnetwork ashugo tessierputsitonhttps unfortunately thedozens ifnothundredsofpaperspublishedeachyearare intheliterature manyrecentlysuggestedpruningprocedur esappeartobe closelyrelatedtoearlyworkslike forasurvey accordingtoreviewslike bgf manyauthorsfailtorelateandcompare andregu larization ourselftothediscussi onofsomeearlyworksthatrepresen tthebasicideasand inspiredlater morespeciﬁcschemes wealsolimitthediscussi ontostrategies latteris frequentlyreferredtoasstructuralpruning seeforinstance forareview andreferences modiﬁedweightdecayprocedur escanbeempl methodswhicharenotbasedonweightdecay cordingtotheirimportancefortheminimizationofthecostfunct ionthatguides thetrainingprocess optimalbraindamage obd andoptimalbrainsurgeon ob twoclassicalpruningalgorithms lecun denkerandsolla soptimalbraindamage obd andtheoptimalbrainsurgeon ob byhassibi storkand wolﬀ arebasedonarankingofindividualweightswj accordingtotheirsaliency tivityofthesystemwithrespecttotheir removal assume ione w withe e w j ijuj thelineartermvanishes mentofthehessematrixcomput wehavesimplyseparated theauthorssuggesttoavoidthecomput ationofthefull onalhessematrixandfocusonthediagonalterms assumi jj eq reduces e w j aneﬃcientcomput ationofthediagonalelemen k canbeused asaguidelinefortheselectionofweightsthatcouldberemoved fromthesystemwithoutincrea ecanbe summa rizedasfollows orkcompl exity optimal brain damage obd ently wellapproximated kk k weightstozero alternativescheme ofoptimalbrainsurgery ob assuggestedin thesalienciesare deﬁned k kk comparedtoeq thedeﬁni tions betweenobd andobsconcerndetailsoftheprocedur e numer theauthorsof suggestascheme whichcomput esthesaliencieswithrespecttotheestimatedgeneralizationerror ratherthanbasedonthecostfunct theirpruning procedur e termed γobsandγobd aremorecloselyrelatedtotheactual goaloftraining anelemen taryandintuitivemethodtoreduce theﬂexibilityofaneuralnetwork istoconsidersubset sofweightsthatassume thatwewanttoapplyasetof adaptive ﬁlterstopatchesofagiveninput theeﬀect ivenumberofweightsdrasticallyand strategyisubiquitousinthe contextof forinstance convolutionalneuralnetworksforimageclassiﬁcation fordeﬁni ngthesetsofsharedweightsinadvance andregu larization fullnetworkwithall unitsandconnect ionsnetworkafterdilution indropout illustrationofregularizationbydropout redrawnafter panel fourrandomlyselectedunits redcircles goingweightsaretemporarilyremoved thatenforcesthedistributionofweightstofollowamixtureofgaussians parametersofwhicharealsosubjecttoupdatesinthetrainingprocess ally weightscanbegroupedaccordingtotheirmem bershiptothecontributing cluster weightscandisplayverysimilarvaluesbut arenotnecessa rilyidentical furtherdiscussi canbe theauthorsof concludethat isapragmaticoptimizationapproach itisnotanecessi ty incomput sharingemer forexamplewhen trainedfromimagesthatdisplaytranslationalinvariance drop wehavealreadydiscussed methodsfromthedilutionofnetworksbyremoving weightsfromagivennetwork eitherbyweightdecayandrelatedmethodsor byapplyingpruningtechniquesaftertraining aswellassections domdilutionisappliedinthetrainingprocess moreconcret ely inevery individualupdatestep t individualinput hidden unitsareexcluded workis hasbeensuggestedasavariationofthebasic ideathatexcludes randomlyselectedweightsinsteadofunits indropout ateachupdatesteparandomlydeterminedsubnet itedcomplexityisconsidered ther efore dropoutreduces theﬂexibilityof removalofnodesoccursindep enden tlywithprobability eterpdeterminesthefractionofunitspresentinthesubnet meth od forhidden unit unit dropoutdilutioncouldbychanceremoveanentirelayerorsomeotherwaycut allconnect ionsfrominput explicitly butareveryunlikelytooccur inlargenetworks intheworkingphaseandfortestingpurposes thefullnetworkisused updatingwithdropoutwillyieldlargerindividualweightsthanconventional inthedropoutbyafactorpinthefullnetwork dropoutcanbeinterpretedastosimulatethetrainingofanensem bleof simplersystems thesubnet work thecompletenetwork yieldsanestimateofthecorrespondingensem dueto thisanalogy thepotentialuseful nessofdropoutgoesbeyondthepurposeof regularization intheworkingphaseitcanbeused foruncer taintyestimation whiledropoutanddropconnect wereintroduced andaremostlyused thecontextofdeep neuralnetworks theconcept canbetransfer redtoother machinelearningsystems fortheconsiderationofdropoutin learningvectorquantization insupervisedlearning performancemeasures allowsustoformulatethetrainingprocess astheoptimizationofasuitable objectivefunct onehastobeawarethatthisdoesnotnecessa rily reﬂect ioncanonlybedeﬁned withrespecttothetrainingset whilethegenericgoalofmachinelearningisto applytheinferredhypothesistonovel unseen ionsserve atbest asproxiesfortheactualaimoftraining asaconsequence thestrictminimizationofthetrainingerror forexample thisalsoimpliesthatweshouldnot theexistence oflocalminimaingradientdescen tbasedlearningfrequentlyturns theverysuccess minded techniqueslikestochasticgradientdescen trelatestothefactthatstrict minimizationofthecostfunct ionisusuallynottheprimarygoalofmachine theuseofsgd canbe larly severaloftheexplicitregularizationtechniquesdiscussed intheprevious sectionshinder thestrictminimizationofthecostfunct ioninordertoachieve bettergeneralizationbehavior onthedownside itbecomesnecessa rytoacquirereliableinformationabout theexpectedperformanceonnoveldata ifwedonotwanttofaceunpleasant surpri andregu larization requiremen tthatthetrainingdatashouldberepresen tative oftheactualtaskathandseems obvious butisnotalwaysmet inpractice seehttps instag withkindpermissionof theartist remark mnist mnist ularbenchmarkdatabaseofhandwr ittendigits ontheactualtrainingdatadoesnotprovideuswithsuchinsights validationprocedur escanbeempl oyedwhichallowustoatleastestimate theexpectedgeneralizationperformance obvious splittheavailabledataintoatrainingsetandadisjointtestsetof isthenused fortheadaptationofthemodel whichis wecansimulateworkingphase behaviorwhileusingonlytheavailabledata obviously thedataisassumed toberepresen tativeforthetaskathand ionwasalready discussed insec learning simpleideaofsplittingtheavailabledatarandomlyintoonetrainingand onetestsethasseveralproblems asitisveryoftenthecase classiﬁcation taskshavebecome oneofthefewprominen texceptions duetothe availabilit meth od tainedinasubset ofthese inthetraining compositionofthesubset scouldbeluckyorunluckyinthesense aconsequence thetestsetperformancemightbeoverlypessimisticor optimistic respectively ofthesystemwithrespecttodetailsofthetrainingset inthesense allthese issue areaddressed inaverypopularstandardapproachknownas ideaistosplitthe availabledatarandomlyintoanumbernofdisjointsubset sof nearly equal size sandusetheremainingsubset forthevalidation atesubset sofdof approximately equalsize di dj n ondi eaverageandvarianceoftheperformancemeasures overthen trainingprocesses foreachsplitwetraintheclassiﬁerorregressionsystemon eventually wehaveobtainednsystemstrainedonslightlydiﬀerentdatasets withnestimatesoftheperformance forinstanceintermsoftheaccuraciesof aclassiﬁerorthemse inregressionproblems whilethenvalidationsetsaredisjoint wehavetobeawarethatthetraining obtainedestimatesofthegeneralizationerrorand evenmoreso ofthetrainingerroraredeﬁni telynotstatisticallyindep enden theprocedur ewillprovideuswithsome insightintotheexpectedgeneralizationperformanceandtherobustness ofthe systemwithrespecttosmallchangesinthetrainingset obviously loadandthequalityoftheresults manytrainingprocesses havetoperformed eachonebased theindividualvalidation andregu larization thecomput ational workloadisreduced averagesareperformed overfewindividualresults trainingprocess makeuseofarelativelysmallsubset ofthedataand cannottakefulladvantageoftheavailableinformation inapracticalsituation thechoiceofnwilldependprimarilyonthenumber onand correspondingreferences forinstance apparently variant inthe literature sofdatamayalsosuﬀer efore performed withrandomizedsplitsandanadditional averageovertherealizations inprincipleonecouldaimatrealizingallpossiblesplitsofthedatainto growsrapidlywith pandthecomput alsoknownasmontecarlocross validation onegeneratesanumberofindep enden tlygeneratedrandomsplits esaveragesandvariances accordingly whichdiﬀerfrom sofdatawithreplacemen fromathoroughcomparisonanddiscussi onoftheadvantagesanddisadvantages ofthese forexample andtotextbookslike asapopularextreme case ut validation inparticularforverysmalldatasets butselectsjustoneexampleasthesmallest trainingprocesses toobtainanaverageoftheperformancemeasure ofinterest utestimatecanbeunreliable itevenbearstheriskofsystematicallyyieldingmisleadingresults inverysmall datasets leavingoutonesamplefromaspeciﬁcclasscanleadtoabiasinthe trainingsettowardstheotherclass e whichmayresultinoverlypessimistic estimatesofthegeneralizationperformance thatmildens theeﬀect fromeachclass generatingvalidationsetsthatrepresen tallclasses sures modelandparameter selection abovediscussed validationschemes canbeempl oyedinthecontextof modelselectionand similarly forinstance pectedperformanceofdiﬀerentclassiﬁers orregressionsystems wecanalso empl oyittodeterminethesizeofahidden work tosetalgorithmparameterslikethelearningrateingradientdescen t toselectaparticularkernelinansvm forinstance wewouldselectthemodelcomplexitythat similarly parametersoftheregularizationschemes discussed insec considergradientbasedtraining ofanetworkwithonehidden weconcludethatsystemswith say hidden isitjustiﬁedtoexpecttheobserved applyingthesystemtonoveldata problemisthatwehaveused allofd todeterminethesupp drivenlearningprocessandcouldbesubjecttooverﬁttingeﬀect sinitself supp osedlybestchoiceofkmaybeveryspeciﬁctodandcouldfailinthe workingphase inordertoobtainamorereliableestimateoftheexpectedperformancewe wouldhavetoperformanextended validationprocedur intotrainingsetdtrainanddtestonce asystemwith thesupp ofcourse wefacetheproblemof whichsuggeststoperformafullloop oroneofthediscussed variant strictlyspeaking thishastobedoneseparatelyforeveryindep enden ticallimitationsapply inparticularwhen onlysmalldatasetsareavailable andclassiﬁcation despitetheconcept ualclarityofsupervisedlearning eventhechoiceofan appropriatemeasure ofsuccess inpractice ytoevendeﬁne best andregu larization measuresforregres sion inregression adiﬀerentiableobjectiveorlossfunct iontypicallyguidesthe trainingprocess itappearsnaturaltoconsiderthesamefunct ionalsoforthe familiarmeansquarederror mse isused inbothcontexts dependingontheapplicationdomain alternativemeasures diﬀerentfrom thelossfunct ioncanbeempl popularmeasures thatareessen tiallydiﬀerentfromthemse mae whichweightsdeviationsfromthetargetdiﬀerentlythanthemse inaset maealsosatisﬁes wher elowervaluescorrespondtobetterqualityoftheregression cod µ µ wher codcompares themeansquareddeviationofthepredictionsfromthetargets scaledby measure satisﬁes avarietyoffurtherevaluationcriteria relation isprovided measuresforclassiﬁcation actualgoalofmachinelearningforclassiﬁcationproblemsistoachievea modelthatassignsdatatothecorrectclasswithhighprobabilityintheworking theactualobjectivefunct ionsused intrainingprovideatbest ionsforprobabilisticclassiﬁersbasedon tropylike couldbeused fortrainingandevaluation mostfrequently theevaluationofcrispclassiﬁers isguidedbycriteriathat overviewofavarietyofperformancemeasures forclassiﬁcationisgivenin assume wearecomparingtheperformances oftwodiﬀerentclassiﬁers aand b thecod ismost author refuse todenote quantitythat canbecome negativ sures termsoftheoverallerroras say apparently wecouldconcludethataisthebetterclassiﬁerandshouldbeused intheworkingphase acloserlookintotheavailabledatad ofthedatarepresen furthermo remightﬁnd modelbmight correctresponses inbothclasses ofthedataset clearly thisinsightmightmakeusreconsiderourpreviousevaluationof aftergoodoverallaccuracyand havereasontobelievethatthetrueprevalence intherealworld goalistodetectandidentifytherelativelyrareoccurrences classiﬁer bisobviouslytobepreferred thissomewha textreme exampleillustratestwomajorquestionsthatarise inthepracticalapproachtoclassiﬁcationproblems formanceofaclassiﬁer beyondtheiroverallaccuraciesinorderto obtainbetterinsightintotheperformance herewedonotaddressthequestionofhowtotakeclassbiasintoaccountin thetrainingprocess somestrategiesforthetrainingfromimbalanced datasets willbediscussed receiveroperatingcharacteristics bothoftheabovementionedquestionscanbeaddressed roc andterminologygoesback tosignalprocessingtasksoriginally buthasbecomepopularinthemachine learningcommunity mostclassiﬁers wehavediscussed obtainabinaryassignmen tbyapplying simpleperceptron forinstance weassignaninput accordingto g ξ withg ξ asdiscussed toimplemen funct ion wecanintroduce andregu larization thresholdθaftertrainingandconsiderthemodiﬁedclassiﬁcation g ξ whilethisisformallyidenticalwiththeconsiderationofaninhomogeneously ion seesec theperspectiveisdiﬀerent weassume thresholdisintroduced concept couldbeappliedtoanydiscriminatoryfunct ionforbinaryclassiﬁcation quitegenerally foralargefamilyofclassiﬁers itispossibletorealizeand inasimilarway probabilisticmodelscanbeused forcrispclassiﬁcationby thresholdingtheclassmem bershipprobabilitywhichservesasthediscriminative funct θ eciﬁcerrorsthat canoccur terminologyreﬂect forinstancein amedi caltestwhichdiscriminatesdiseased positivetestresult fromhealthy controlpatients negativeoutcome thetermfalsenegativeerror isused complemen se theintroductionofacontrolledbiascanbeachievedinotherclassiﬁcation frameworksaswellandis bynomeans limitedtolinearclassiﬁers forinstance wecanmodifythenearestprototypeclassiﬁcation npc inlvq theclosestoneamongallprototypesrepresen wecanassignanarbitraryfeature thusintroducingamarginθinthecomparisonofdistances similarly forwardneura lnetworkasthediscriminativefunct ionandperformabiased thresholdingalongthesamelinesinordertoobtainacrispclassassignmen foragivenvalueofthethresholdθwecanobtain testset theobservedabsolutenumberoffalsepositiveclassiﬁcationsfp false negativesfn corresponding ratesaredeﬁned errorsareused frequently butthese areavoided sures truepositiveratetpr ξ leftpanel schema extreme y thedashed linerepresen tsrandom biasedguesses rightpanel illustration iong ξ thenegative positive classaredisplayedasgreen light andred dark ﬁlled circle thelargeﬁlledcircleandcorrespondstog variationofthe thresholdbyδθisreferredtointheargumen tsempl thestatisticalinterpretationoftheauc diﬀerentnamesareused forthesamequantitiesintheliterature dependingon cine forinstance thetermsensitivity sen isfrequentlyused forthetpr whilespeciﬁcity spec refers tothe quantitiestp tn fp fnandrates canbefoundin whichalso providesfurtherrelevantreferences thequantitiesineq arenotindep enden t obviously theysatisfy consequently twoofthefourratescanbeselectedtofullycharacterizethe classiﬁcationsθ ξ intheframeworkofreceiveroperatingcharacteristics roc minestpr θ andfpr θ forameaningfulrangeofthresholdsθanddisplays thetruepositiverateasafunct ionofthefalsepositiveratebyeliminatingthe leftpanel lowerleftcorner asmarkedbyanempt ycircle wouldcorrespondtotheextreme timplemen tation idea andregu larization falsepositiverateiszeroforthissetting theclassiﬁerdoesnotgiveanyfalse nopositivecasesaredetectedandtpr alsomarkedbyanopencircle theclassiﬁersimplyassignsevery featurevectortothepositiveclass thusmaximizingthetruepositiverateat theexpense theperformanceofanunmo leftpanel forinstance tothe npcinlvqorthehomogeneous unbiasedperceptron eq θ theuser canrealizeanycombinationof tpr tpr thedomainexpert canadjust theactualclassiﬁeraccordingtothespeciﬁcneeds intheproblemat caldiagnosissystems forinstance highsensi tivity tpr bemoreimportantthanspeciﬁcity orviceversa toacertainextent wecanalsocompensatefortheeﬀect sofunbalanced trainingdata leftpanel whichmightbeaconsequence ofan ion thiscanbecompensated forbybiasingtheclassiﬁertowardsthedetectionofpositivecasesandmovethe workingpointclosertotheupperleftcornerintheroc thehypothetical bestpossiblerocisobviouslygivenbythestepfunct ion acompletelyrandomguess withbiasedprobability dashed lineintheleftpaneloftheillustration theareaundertheroccurve evaluatingdiﬀerentclassiﬁers orframeworks oneoftenresorts tothecomparisonoftheareaunder theroccurve lesspreciselyauc informationaboutthedegreetowhichtherocdeviatesfromthediagonalwith clearly anauc tionandtheaucisoftenused asasinglenumer icalqualitymeasure forthe evaluationofclassiﬁers inprinciple thepreciseshapeoftherocshouldbe takenintoaccountaswell asindividualroccandiﬀersigniﬁcantlyfromthe theaucwithrespecttonoveldatacanbeestimated forinstance inthe intotheperformanceofthetrainedsystemthanasinglespeciﬁcworkingpoint ther efore sures rightpanel classiﬁedaccordingtoadiscriminativefunct ionwhich intheillustration assumed toincrea semonotonicallyalongtheg ξ itis convenient butnotnecessa ry toargueintermsofalinearclassiﬁerlikethe perceptron inwhichtheweightvectorwdeﬁnes thediscriminativedirection intheillustration aparticular ismarkedbyaﬁlledcirclewiththevalueg ofthediscriminativefunct ion inotherwords theconsidered examplewouldbe locatedpreciselyatthedecisionbounda ry nowassume probabilityforsuchanexampletosatisfyg isgivenprecisely bytpr whichisthefractionofpositiveexampleslocatedonthecorrect sideofthedecisionbounda rydeﬁned byg ξ ontheotherhand thelocaldensi tyofnegativeexamplesisgivenbythe shiftingthethresholdbyδθ asmarkedbythegray shadedarea δθmany samplesfromthenegativeclass insumma ry thisimpliesthatforapairoffeaturevectorscomprisingone thepositiveclass theprobabilitythatg g isgivenbytheintegral ϑ dfpr theaucquantiﬁestheprobabilitywithwhicharandomlyselectedpair isordered accordingtoclassmem bershipintermsofthediscriminative funct iong exists atwhichtheclassiﬁerwouldseparatesuchapairofinputscorrectly possibletoperformthetrainingofaclassiﬁerinsuchawaythattheexpected aucismaximized fordetailssee altern avarietyofevaluationcriteriaforbinaryclassiﬁcationschemes pr formalism canbe considered prevaluationisalsobasedon thefourquantitiestp tn fp prec andrecall rec deﬁned similartotheroc theareaunder andregu larization onofsupp oseddisadvantages oradvantagesoftheprformalismovertherocsee andreferences therei likeotherquantities precandreccanalsobecomput edatasingle ciﬁcworkingpointoftheclassiﬁer variousapplicationdomainspeciﬁcmeasures havebeendeﬁned foroverviewsandreferences infact thelargenumberofrelatedqualitymeasures erableconfusi whiletheoverallaccuracyforatestsetofintotalntotsamplesiscomput ed accuracycorrespondstoanequal weightaverageoverclasses itissupp osedlymoresuitableforclassimbalanceintrainingandvalidationsets tp fpandfnaretheabsolutenumbersoftruepositives falsepositives sure whichisgivenbytheharmonicmeanofprecandrec measure canbecomput ed seebelow onalgeneralization oftherocformalismortheprscheme isfarfromobvious confusionmatrix mostcommo nly intheleftpanel eachelemen tcorresponds tothenumberoffeaturevectorswhichbelongtoclassiandareassignedto classjbytheclassiﬁer percentage rounded diagonalelemen seaccuracies tsprovideinsightintowhichclassesarerelativelyeasyor diﬃcul ttoseparate notethatalthoughtheconfusi onmatrixprovidesdetailedinformationabout seperformances siﬁer titiesforagivenworkingpointcanbeextended ina straightforwardfashion sures predictedclass ab predictedclass confusi onmatrixofahypotheticalimbalanced leftpanel matrixelemen tscorrespondtotheabsolutenumberofsamples thecorresponding seaccuracies sequantitiescanbederivedfromtheconfusi onmatrixorby assiﬁcationschemes way allmeasures canbe considered sequantitiescanbecomput edindiﬀerentways weightingeachclassequallyorbytakingthenumberofexamplesperclass thissubtletyisillustratedintermsofprecision prec andrecall rec asdeﬁned ineq sure fromtheconfusi onmatrixc j sequantities j c j j c j leftpanel basedoneq wecanalsocomput seprecisionandrecall valuesinanalogywitheq andregu larization areobtained withequalweightassigned tothecclasses alternatively isfrequentlyconsidered herenidenotesthenumberofsamplesinclassiand nowwehaveatleasttwooptionsfordeﬁni asanarithmet sefi fi b unfortunately bothmeasures appearintheliteratureunder thesamename tenwithoutclarifyingwhichversionwasused arecentlypublished note authorsshowthatthetwoquantitiescan demo problem appearsto bemorerobust authorsconcludethat attheveryleast researcher incontrasttotheabovediscussed consideringtheaverages andcomput ing wenotethat c asbothsums addupallelemen tsoftheconfusi theirharmonicmeanfmic accuracy ingeneral sisoneachclasswhichmakes sense enden tperformance averagetargetstheoverallquality acompro whichaveragingprocedur eshouldbeused temdepends ontheactualtargetandthepreference compositioninthetrainingdatasetcomparedtotheonethatisexpectedin forinstance ticularclassisrepresen tedbyveryfewsamplesinthetrainingset butcanbe bearstheriskofdisregardingtheclassintheevaluationwithpotentialpoor performanceintheworkingphaseasaconsequence abovediscussed qualitymeasures andvalidationprocedur esfocusonthe moresophisticatedmeasures criterion ifthegoalisto say distinguishcatsfromdogsinimagesorperhaps discriminatediseasedpatientsfromhealthycontrolsinadiagnosisproblem machinelearningsystems shouldbeevaluatedandcomparedto eachotheralsoaccordingtocomplemen someofthese cannotevenbeexpressed intermsofsimplequantitativemeasures asan illustration wediscuss anentertainingandfrequentlyquotedexample itillustratesandsumma rizesanimportantissue inmachinelearningalongthe linesoftheopeningquoteofthischapter storyisthataclassiﬁerwastrainedtodistinguishdogsfromwolves toworkperfectlyintrainingand validation tually acheckofthedatabaseshowedthatallwolveshadbeenphotographed inthesnow learned todistinguishtheimagebackgrounds andregu larization asusualwithstorieslikethat itistoldinmanyversions foran interestingaccountofsimilarexamplesofsupp osedlymisleadclassiﬁers parently apublicationin aimwastoillustratetheproblemandtotestamethodforexplaintheinner workingsoftheclassiﬁer themoralofthestoryisdeﬁni telyrelevant unno ticedbiasesin datasetscanresultinseemi nglygoodorevenexcellentperformances eﬀect isfrequentlymuchmoresubtleandmorediﬃcul ttodetectthaninthe asaparticularlyimportantexample medi caldata setsarefrequentlypronetoselectionbiassesthatfacilitateaseemi nglysuccessful theageorgender distributionintheclassescouldbediﬀerent whilebeingessen tiallyunrelated ofmissing valuesinoneofthegroupscouldbeexploitedbythemachinelearningsystem resultinginseemi nglygoodyetuselessperformance itisthenatureofmachine learningsystemsthattheyareexcellentartefactdetectors theevaluationandcomparisonofsupervisedlearningmodelsinterms ofaccuracyonly orsimilarperformanceorientedcriteria canbemisleading leastacertaindegreeofinsightintowhatisthebasisofthesystem sresponse whichfeatures forinstance appearmostrelevantinaclassiﬁcationscheme isitthepropertiesoftheanimalsorthe colorofthebackgroundthattheassignmen treliesupon inthissense machinelearningsystems aneﬀortshouldbemadetounder standhowagiven evancelearningconstitutejusttwoexamplesofapproachesthatcanbeuseful featurescanbeperformed inavarietyoflearningframeworks tectionofpotentialbiases interpretablemodelsalsofacilitatethediscussi withthedomainexpertandincrea setheuser accept anceformachinelearning basedsupp ortsystems consequently thetopicofimprovedinterpretability hasattractedconsiderableinterestwithinthemachinelearningcommunityand eﬀortsaimatclosingthegap ifany thegoalsofstatisticalmodellingandmachinelearningdiscussed cialsessionswhichhavebeenorganizedattheeuropeansymposiumonneural network esann bbv sioncontributionsshouldprovideauseful startingpointforfurtherexplorations vediscussi onofexplainabilityandinterpretability withmanyreferences canalsobefoundin preprocessingand unsupervisedlearning iwillletthedataspeakforitselfwhen itcleansitself inmostofthese lecturenoteswehaveimplicitlyassumed thatfeaturevectors thisis realworlddatasetshavetobethoroughlychecked forinconsistencies missingvaluesorotherissues alistofuseful rulesforinitial dataanalysisisprovidedin quiteoften outliersareremovedfromthedata appearimplausiblebecausetheyareverydiﬀerentfromthebulkofthedataor tobeperformed withutmostcareandinacontrolled criteriaandgoalsofdatasetcurationandcleaningareusuallyveryspeciﬁcto efore itshouldrelyoninsightsfromthedomain expertsanditshouldneverbeguidedbytheideaofmakingthedataconsistent removingsuspected outliersmayseriouslyaﬀect theoverallqualityofthedatasetandintroduce wefocusonbroadlyapplicableandpopularpreprocessingsteps designofaclassiﬁerorregressionsystemusuallybeginswiththechoiceof howtorepresen tthedatatothesystem thiscanamounttotheextraction ofengineeredfeatures alsoseemi nglysimple datasetsofdirectlyobservednumericalfeaturevectorsoftenrequirecareful preprocessing orintheworstcase evenwante dbiases supp ortthedesir edresult gandunsupervised learning dependingonthetypeofdataathand amultitudeofpreprocessingsteps canbeconsidered someoperationshavelittleornoeﬀect onthesubseq uent mayseem naturalbutarefarfromtrivialandcanevenbe method someformofnormalizationorcoordinatetransformation isperformed whichthenfacilitatestheapplicationofaparticularmachine distributionsoffeaturevalues forinstance wher e nominal dimensi onoffeaturevectorsisrelativelyhighcomparedto onalrepresen tationsplaysan sualizationofdatasetsorthetrainedsystems featureselectioncan beinterpretedasaspeciﬁcformofdimensi onalityreduct ion aimingat atheidentiﬁcationofasubset ofavailablefeaturesthatissuitableand suﬃci entforthegiventask erviseddensityestima tion example thedetectionofpronounced clustersofdatainapreprocessing stepcanhelptodesignaspeciﬁcclassiﬁerorregressionsystem essary inparticularwhen dataisscarceandincompletefeaturevectors cannotbesimplydiscarded datasetsare imbalanced withrespecttotheclassesinasupervisedlearningproblem similarly dataaugmentationaimsatenrichingthetrainingdatain ordertoachievebetterrobustness variance inthetrainingprocess trainingimagesinobjectrecognition sionalityreduct ionbyprincipalcomponentanalysisobviouslyconstitutesalso densi tyestimationcanbeused forthe imput ationofmissingvalues probablythemostfrequentlyused preprocessingstepsconcernsomeformof suchoperationsshouldneverbeappliedblindly withoutevaluatingtheireﬀect onthesubseq tions frequently harmonizetherangeorthestatisticalpropertiesofthefeatures givenasetofobservationsormeasuremen tosubtractthemean oneobtainscenteredfeaturevectors wesubtracttheempi rare caseswher ethetruemeanofthecorrespondingprobabilitydensi tyisknown couldbeused toreplacetheempi ricalone onaleuclideanspace thecentering correspondsto asimpletranslationofalldatapoints whichhasnoeﬀect forinstance oranglesdeﬁned bytripletsofdatapoints aslightlymoreinvolvedtransformationisfrequentlyappliedinorderto obtainzeromean unitvariancefeatures zµ jinipasdeﬁned ineq reszµ jdisplayzero meanandunitvarianceinthegivendataset zµ jquantiﬁesbyhowmanystandarddeviationsafeaturevaluediﬀers fromtheempi negative zµ jcorrespondto averagevalues respectively toaccountforfeatures thatscalediﬀerently whichcanbedetrimentalinasupervisedorunsup ervised stherepresen tationindep enden t oflinearrescalingofindividualfeaturesandchoiceofunitsofmeasure inmilesorcentimetersforthelengthofanobject consideringonlysinglefeatures theeﬀect ofthemonotonictransformationis oneshouldbeawarethatitcanaltertherelationsbetween retransformationcanaﬀect theoutcomeofavectorquantizationscheme seethediscussi avarietyofsimilarlineartransformationscanbemotivatedforspeciﬁc wecouldempl oythefeaturemedianand interquartilerange iqr forascalinganalogousto inordertoachieve zeromedi gandunsupervised learning n n log leftpanel manysmallandrelativelyfewlargevaluesinagivendataset illustration rightpanel featurelog sometimesitisdesirabletotransformfeaturestoaﬁxedrangeofvalues reﬂect ingthespeciﬁcrequiremen tsofthemachinelearningsysteminuse min j notethattheuseofminimumandmaximumvaluescanbeverysensi tiveto thepresence orabsence ofextremevaluesinadataset wewillusethetermonly ibedinthe followingsubsect ion eﬀect instanceinthecontextofclassiﬁcationorunsup ervisedclustering obviously largevarietyofsuchtransformationscanbeconsidered havingspeciﬁcgoals inmindandtakingintoaccountdomainknowledgeaboutthepropertiesofthe datasetathand asjustoneparticularexamplewediscuss featuresoftheconsidered datadisplayaskeweddistributionofvalues leftpanel ative featurewhichfrequentlyassumes smallvaluesandonlyrarelylargerones suchcases astrologyasan artefactofimplicitnormalization faceofaspher rightpanel canalsobeviewedasanexample mental overﬁtting reproduced sionfromjohnatkinsonfor cialuse seehttps wronghand brilliantcartoons thetransformed dataisexactlygaussianonlyiftheoriginaldatafollowsa p whilethisisrarelyexactlythecaseinrealworlddata skeweddistributions left arenotuncommo ninpractice thecontextofmedi foranexample inanycase whichtransformationsmakesense pends onavailabledomainknowledgeandinsightsintothedatastructure norma lization oneofthemostfrequentlyappliedtypeoftransformationisthenormalization correspondingtothefamiliareuclideannorm weobtainvectors alldatapointsareprojected ontotheunitspher equivalenttomeasuringdistancesintermsofanglesbetweentheoriginalfeature forinstance inthecontextoftheperceptron inmoregeneralcontexts theeﬀect choice efeatur gandunsupervised learning isasuperbexample unrelatedstarsappeartoformmeaningfulclusterswhen theyareimplicitlyprojectedontoaspher e illustration asanotherexample yieldstransformed ativefeatures tingamountsofchemi calcomponentsinasample thenormalized eventcounts wouldbe transformed tonormalizedfrequenciesorprobabilities normalizationasapreprocessingstepcanbehelpfulandgreatlybeneﬁci al inpracticalproblems inanycase itshouldbeappliedbasedonavailable domainknowledge anditseﬀect sonthesubseq uentanalysisshouldbecarefully evaluated sionalityreduction onalrepresen tantroleinthecontextofdataanalysisandmachinelearning mph bbh foravarietyofmethodsandconcept interconnect ed overlappingmotivationsfordimensi onreduct ﬁed onaldataarepresentedtoamachinelearningsystem numberofadaptivequantitieswillbe atleast ofthesameorder mayhinder successful training inparticularifthenumberofavailable onreduct culty onalrepresen tationshelptoexploreagivendatasetprior say structuresinthedata cesinwhichthefeature inthesubseq uentanalysisbytakingspeciﬁcpropertiesofthedatainto account closelyrelatedtothepreviouspoint tationsprovidevisualizationsofthedatasetwhichfacilitateinteraction withtheuser visualizationcan giveuseful insightintothestructureoftheproblem inretrospect afterthetrainingofaclassiﬁer manceandprovidesinformationaboutregionswher eclassesoverlapor individualmisclassiﬁeddatapointsarelocated itisimportanttorealizethattheintrinsicdimensionalityoffeaturevectors rilycoincidewiththenominaldimensi sionality reductio n leftpanel schema onaldata thespecialcase ofalinearsubspa ceorhyperplanethatcontainsthedatapoints instance asetofvectors ξµ p orcloseto onal linearsubspa ce tfeature vector notethatthepopulartermcurseofdimensionality isavoidedhere itisnotobviousthatanominallyhighdimensi onisnecessa mentalfortheperformanceofmachinelearningmethods b superstitionbringsbadluck wecandistinguishtwoessen tiallydiﬀerentconcept onal represen inonefamilyofapproaches eachoriginaldata pointisrepresen position oftherepresen tativesaredirectlydeterminedbymeansofoptimizingasuitable costfunct ionwhichisbasedontheaimof approximately borhoodrelations ortheoveralltopologyoftheoriginal funct tantexample md belowandbrieﬂy mentionafewotherpopularmethods inthesecondmajorframework anexplicitmappingisdeterminedwhich projectionisoptimizedaccordingtoaspeciﬁccriterionwhichis methodsarelessﬂexibledue formoftheactualmapping theyoﬀerthepossibilityto project mostpopularexampleofthe basicconcept pca whichis discussed gandunsupervised learning leftpanel scatterplotof md sionalembedding distancemeasuredn ξ distanceinndimensi ons buttheconsiderationofanymeaningfuldissimilarity asdµν ξµ ξν inthe following anumberofmethodsaimatﬁndinganembeddingofthedatapointsin onaleuclideanvectorspacethatpreservesrelationsbetweenthe individualfeaturevectorsintheoriginalspace goal couldbetoapproximatelyreproduce themsel f rankstructure orassociatedprobabilitydensi tie sionalscaling onalscaling md tatives n andtheirrelations m p e weevaluatethedissimilaritiesof thecorrespondingvectors p measurecouldbebasedonanyreasonablevectornorm witheuclideanmetric ingeneral measuresdnanddmneed ion classicalmdsisalsoknownasprinc ipalcoordinate sanalys cussed sionality reductio n tooptimizeinmdsisthequadraticdeviation p µ µ dµν allcoordinatesyµ jareconsidered degreesoffreedo mthatcanbeobtainedby minimizationofe local minimumof correspondstoanarrangemen tofppointsinm dimensi onswhichreﬂect m obviously wecanalsonotexpecttoﬁndauniqueminimumofe theactual tialrandomness inthetrainingprocess simpleexamplebasedoneuclideandistanceinbothspaces onal featurevectorsξµ leftpanel displayarelativelysmallvarianceincomponent onalrepresen imizingthequadraticdeviation coeﬃcient couldbeobtainedinordertoevaluatethequalityofthemdsresult notethatthequalityoftheembeddingisinvariantunder simultaneous thetermmdsisfrequentlymeanttoimplytheuseofeuclideandistances inrnandrm andminimizingthequadraticdeviation avarietyofdistancemeasures dnanddmcouldbeconsidered ciﬁccostfunct ionscanbechosenwhichput forinstance diﬀerentempha si onsmallorlargedistances agoodoverviewofmdsrelatedmethodscanbe obtainedfrom mph bbh discusses choicesinauniﬁedframework monmapping locallinearembedding lle andlaplacian eigenmaps neigh borhoodembedding anotherpopularfamilyofmethodsisoftenused forthevisualinspectionof sne asintroduced byhintonandroweis thedataischaracterizedinterms thatwaslatersuggestedbyvandermaatenandhinton divergence asameasure similaritybetweentheassumed densi tiesin optimizationcanbedoneusing gandunsupervised learning approximationandprojection umap hasbecomepopular featureistheassumpt ionthatdataisdistributedin orcloseto aparticular manifoldalongwhichdistances arecomput ed embeddingmethodsareverypopular embedding wereferthereadertotheoriginalliteratureandreviewslike bbh bbk methodsisthat generally speaking connect visedlearning regressionorclassiﬁcation oneoptionistoaddthem tothesetandthenrecomput theextended data onecantryto obtainanexplicitmappingfunct ionψthatapproximatelyrealizesψ andcanbeappliedtonoveldatapoints seeforinstance foradeep learningbasedapproach featureselection aratherdirectwayofreduci ngthedimensi onalityofinput dataistoselect asubset offeatures neglectingallothers itappearstobeanaturalideato simplytryalldiﬀerentcombinationsoffeaturesandselectthebestpossible subset thenumberofsubset swithkfeaturesselected fromndimensi andgrowsveryrapidlywithkandn anoverviewofbasicfeatureselectionmethodscanbefoundin mainstrategiesfortheselectionoffeaturesubset shavebeen considered intheliterature ervised approach erties indep enden correlation betweendiﬀerentfeaturescouldbeexploitedinordertodiscardredunda nt featuresinagivendataset supervisedﬁlteringtakesintoaccountthelabelinformationinthedata forinstance inthecontextofregression inclassiﬁcationproblems mutualinformationorcrossentropybetween criteriaareappliedinaunivariatefashion bearstheriskofmissingtherelevanceofcombinationsoffeatureswhich wouldberevealedinanappropriatemultivariateanalysis inthewrapperapproach andrelated meth od featuresintermsoftheperformanceoftrainedregressorsorclassiﬁers eachcandidatesetoffeatures trainingandpotentiallyvalidationschemes havetobeperformed whichcanresultinconsiderablecomput ational odsisthat inprinciple favorablecombinationsoffeaturescanbefound obviously andremovesingleonesfromtheset ineverystepselectingtheonethat couldaddfeaturestotheset followingagreedy strategytoimpro vethe qualityoftheclassiﬁcationorregressionineverystep inembedded methodstheselectionofasubset offeaturesisanintegral partoftrainingaspeciﬁctypeofmodel apopularexamplewouldbethetrainingofdecisiontreesinarandom forest theactualfeaturesetiscompiledwhilethesystemis trained whilethiscanbemoreeﬃcientthanthewrapperapproach itis tiallyunivariateevaluationoffeatures andrelatedprojectionmethods avarietyofmethodsderivesanexplicitmappingoftheform ψ ξ mappingψisparameterizedandobtained inadatadrivenprocess unlikemdsorotherembeddingmethods particularlyimportantformethodsofsupervisedlearning wher ewecanderive ψfromthetrainingdataandapplythesamemappingtonoveldatainthe workingphase inprinciple wecanobtainameaningfulprojectionbyparameterizingthe funct ionψinasuitablewayandthenoptimizingitsparametersaccordingto anappropriatecostfunct inthecontextofembedding intheprevioussectioncouldbeempl oyedfortheidentiﬁcationofanexplicit mappingaswell wediscuss onlymethodswhichempl oylinearprojectionsofthedata mostmethods includingprincipalandindep enden tcomponentanalysiscan beextended stance intermsofshallowordeep neuralautoencoders asdiscussed insec seeforinstance ences therei gandunsupervised learning principalcomp onentanalysis duetotheirsimplicityandintuitivenature linearmappingsareofparticular interestandpracticalrelevance forexample principalcomponentanalysis pca isoneofthemostimportantandfrequentlyused explicitmappings inthecontextofdataanalysis unsup ervisedlearning pca isempl dimensi onalrepresen toidentifysupp osedly linearcombinationsoffeaturesthathardlyvaryovertheobserveddata method pca ground inpassing forconvenience wewillassume throughoutthefollowingthatthedataset used fortheidentiﬁcationofthemappingψiscentered thishasbeenachievedbyacenteringoftheform thesametransformation usingtheempi ricalmeaniniphastobeperformed onnovelinput vector ψcanbeapplied optimizationproblemwithrespecttotheempi ricalvariancealongw yµ itisplausibletoassume thatthecorrespondingsolutionoflargestvariance thedirectioninwhichmostoftheinformationaboutξiscontained infact thisisrigorouslytrueunder theassumpt ionthatallfeaturesfollowanormal theempi iξµ j wecanrewritetheobjectivefunct ion eq asaquadraticform j jξµ tebydeﬁni tion negativebutotherwi seunbounded wecanassume thatchasordered value thecomplemen ixcwith elemen jξµ jξν j deﬁned andrelated meth od itisstraightforwardto showthatthemaximumofevarisachievedifwistheeigenvalueofcwiththe incaseofdegenerateleadingeigenvalues wehaveto onaleigenspa weproceed sequence ofdirections weidentifythemleadingprincipalcomponentsofip withtheleadingeigenvectorsofc wk m yµ k rightpanel thevectorsmarked canserveascoordinateaxesdeﬁni onalsubspa ceoflargest variationinthedataset yµ yµ yµ m associatedlinearsubspa cedisplaysthelargest variance ionsofgaussianity onalξµ equivalently onecanshowthat forﬁxeddimensi onalitym pca canbe interpretedasalinearautoencoder realizingadimensi onality reduci ξµ kuk realizesthesmallestquadraticreconstructionerror um inthecompactform gener ically wewillapplypca onaldatasets inparticular ifthenumberofexamplesislowerthanthenominaldimensi gandunsupervised learning obviouslythesubspa dimensi onalinthiscase asreﬂect themaximum ponentsareused theassumpt ionthattheleadingeigenvectorscarry mostinformation themappingistruncatedatrelativelysmallm neglecting variationsalongminoreigendirectionsasnoise powerfultoolsexistthatcanbeused todeterminetheleadingeigenvectors ofthesymmet ric numer icalmethodsforthe moregeneralsingularvaluedecomposition svd arepreferred svdreduces toeigenvaluedecompositionfordiagonalizablematrices butitis claimed tobenumer icallymorestable whitening pca canalsobeused formed coordinate onthelevelofsecond orderstatistics thedataappearstotallyisotropicwithanidentitycovariance isnostructureleftinthedata whiteningcanbeuseful analysingpropertiesofthedatawhichconcern higherorderstatisticsasinindep enden tcomponentanalysis pca byhebbianlearning itisinstructivetostudyaverysimplenumericalprocedur etocomput correspondstothepowermethodorvonmisesiteration willsee itrelatestohebbianlearningandcanbeextended tothecomput ation ofseveralleadingeigenvectorsfromaneuralnetworkperspective consideraninitialvectorw whichcanbeexpanded vectorsuiofc w repeatedmultiplication fromtheleft w t t wher andrelated meth od assumi theweightsw t willbedominatedby theargumen t canbeextended onalsubspa ceofleadingeigenvaluesandthe wecanobtainamodiﬁediterationbyreplacingcbytheshiftedmatrix withη sameeigenvectorsascandordered eigenvalue corresponding iterationreads w t w t ξµ wher eyµ t cw iyµ insteadofcomput ingthesum pineachiterationstep wecanupdatethevectorwinsinglestepsoftheform w τ τ ξµ wher ew istheinitialvectorandtheindex whichcorrespondstotherepeatedpresentationofalldata notethattheupdate canbeinterpretedashebbianlearningina derivedasthegradientbasedmaximizationofcostfunct inpractice itmakessense tonormalizetheweightvectoraftereachupdate stepinordertoavoidnumer w τ τ ξτ τ withyµ τ ngthatthepreviousw wasalready normalized wenotethat τ yµ τ inthesamelimit andwetherefo reobtain oja srule w τ yµ τ yµ τ fromeq isreminiscen tofthediﬀerence jacobiiterations forlinear system seesec italso resem bles therelation betweenbatchandstochastic gradien tdescen t gandunsupervised learning heretermso iteration isknownasoja s ruleaftererkkioja weightdecaythatrealizesanapproximatenormalizationofwforsmalllearning rate furtherprincipalcomponents inprinciple onecouldapplythepowermethodalsoforthesecondandall retically wecouldavoidcontributions bystartingfromaw comput ingum similarly onecouldconsiderthemodiﬁedmatrices k bothideasareatrisktofailin practice asnumer icalinaccuracieswillalwaysintroduce ifnotcorrectedfor itismorepromisingtoiterateasetofvectors wj m poseappropriateconditionsaftereachstep hmidt orthonormalization twocloselyrelatedextensionsofoja srulearebasedonthisconcept presenthere onlythesinglestepvariantsandreferthereadertotheoriginal literature proof seealso foramoreelaboratediscussi oja ssubspacealgorithmandsanger srule initializearandomwm m atdiscretetimestepτ µofthecurrentexample sequentialpresentation τ m accordingtooja ssubspacealgorithm wm τ m τ k τ wk oraccordingtosanger srule wm τ m τ k τ wk wher eyµ k τ τ andrelated meth od theﬁrsttermsinthebrackets ofeqs correspondtothefamiliar remainingtermscanbemotivatedas notethatinoja ssubspa cealgorithmthesumineq isoverallindices m whileinsanger srule m onecanshowthatsanger salgorithmyieldstheordered principal component τ inoja ssubspa ce algorithmthevectors wk m ofthesamesubspa donotnecessa rilybecomeidenticalwithukanddo notdisplaythesameorderwithrespecttothepartialvariances ofthedata kandyµ infact theupdates canalsobederivedas singleexample gradientmaximizationofcostfunct ion complemen tedbypenaltytermsof yµ kyµ l independentcomp onentanalysis asdiscussed pca identiﬁesdirectionsinwhichuncorrelatedprojections displaythelargestvariances inindep enden tcomponentanalysis ica goalistoﬁndorthogonaldirectionswhichareindependentinabroadersense forinstance used forblindsourceseparation toidentifyindep enden tsourcesinamixedsignal based onhebbianlearninghavealsobeensuggestedforindep enden tcomponent analysis typically theproblemisaddressed byconsideringaproxyforstatistical indep endence twopopularconcept sare mutualinformation projectionsareidentiﬁedwhichcarryaslittleinformationabout mutualinformationcanbedeterminedastherelativeentropy ix y x y y x y px x py y withthejointdensi typx tie yidentiﬁesindependentdirectionswandvin featurespace b deviationfromgaussianity orthogonaldirectionsinwhichtheprojectionsappearleastgaussianare expectedtobemostinterestingandpotentiallyrelevantforthetaskat hand woulddisplay forinstance veryskewedormultimodal gandunsupervised learning leftpanel left panel centerpanel correspondingtoanormaldensi ty rightpanel respectively asoneexamplestrategyfor b wediscuss themaximization solutevalue deﬁned wher e thedatais centeredandwhitened inaﬁrststep eq appearingisotropicwithunit varianceandzero eq simpliﬁesto thekurtosisasdeﬁned iszerofornormaldensi inabsolutevalue canserve tywithkurtosis leftpanel whichappearsmorebumpythanagaussian centerpanel andadensi tywith kurtosis ussiandensi tie okurtosis theidentiﬁcationandexplorationofstructuresinagivendatasetcanconstitute averyuseful methodsofunsup ervisedlearning areappliedbeforetheactualclassiﬁcationorregressionscheme isimplemen ted purposecanbetoobtaininsightintothecomplexityoftheproblemand intothediﬃcul forexample thedatasetcontains clustersorgroupsofsimilarfeaturevectors thisknowledge couldbeused inthedesignofaspeciﬁcclassiﬁer similarly densi tyestimation techniquescanbeappliedtoobtainknowledgeaboutthegeneralstatistical propertiesofthedata theadditiv omitted thekurtosis ofagaussian densit yisobviously ingandvecto rquantization afteraverybriefdiscussi onofelemen od wepresenttwoprominentandrelatedmethodsofunsup ysis vectorquantization vq bycompetitivelearningandgaussianmixture model gmm fordensi tyestimation basicclusterin gmeth od arebasedon suitabledistancemeasures infeaturespacelikethefamiliareuclideanmetricsin ca cb betweentwoclustersisderivedfromthepairwised xa xb oftheirindividual elemen ationof clusterdistances areknownas d ca cb d xa xb wher etheclosestpairoffeaturevectorsdeterminesd ca cb e d ca cb xa xb thenumberof d ca cb d xa xb correspondstothelargestpairwisedistanceofvectorsincaandcb andsimilarmeasures canbeused proacheslike bottomup everydatapointisinitiallyconsidered twoclusterscaandcb whicharetheclosestaccordingtosomecriterion d ca cb aremerg edintoonecluster topdown iouscriteriacanbeconsidered intheclusterselectionandtoguidethe actualdivision nsorsimilarmethods hierarchicalclusteringdoesnotrequire apredeﬁned theprocedur egeneratesa t splitsofthedatasetintoanincrea conﬁgurationcanbechosenoncethedendr gandunsupervised learning comp etitiv elearningforvectorquantization onepossibleaimofunsup ervisedlearningistherepresen tationofapotentially largesetoffeaturevectorsbyafewtypicalrepresen termvector quantization vq shouldreﬂect thefrequency toreduce storageneeds ortorevealstructuressuchasclustersinthedata inthefollowingwepresentabasicscheme forunsup zationbycompetitivelearning nically itresem blesthemethodsof supervised learningvectorquantization discussed oystheconcept ofcompetitivelearning unsup ervisedvqisappliedtounlabeledfeaturevectorsanditsaimis thefaithfulrepresen tationofdatasets notanactualclassiﬁcationorregression task likemostunsup ervisedlearningtechniques vqisalsoguidedbyacost funct funct ionmeasures jargon input vectorscanbeinterpretedasstimuliwhichactivatetheneuronsorunits asweightvectors exemplarsorexpectedstimuli averypopularapproachtovqisbasedonthecrispassignmen tofanydata pointtotheclosestprototype ﬁxeddistancemeasure werestrictthediscussi ontotheuseofsimplesquared euclideandistanced x y alizationsto alternativemeasures acorresponding suitablecostfunct error wk µ wk µ wk µ wj k wher hvqaccum tances ofallfeaturevectorsfromtheirclosest sentedbytheprototypes standardcompetitivevqcorrespondstothestochasticgradientdescen t basedminimizationofhvq intuitiveandcouldbeobtainedonpurelyheuri sticgrounds ateachdiscrete timestep asinglerandomlyselectedfeaturevectorxµisdrawnwithequal thecurrentlyclosestprototypewk µ determinedaccordingtoeq ingandvecto rquantization ofsec onlythewinner isadapted inunsup ervisedvqthe winningprototypeisalwaysmovedclosertotheconsidered input vector updatedoesnotdependonadditionalinformationsuchasthelabelsinlearning vectorquantization competitivelearning vectorquant ization selectasinglefeaturevectorxµrandomly wk µ withd wk µ xµ d wj xµ wk µ µ t t µ t termcompetitivelearninghasbeencoinedforthisandseveralrelated trainingschemes asprototypescompeteforupdates algorithmdescr wta scheme extensionstotheupdateofseveralprototypesateverytimestep havebeenconsidered alsointhecontextofunsup ervisedvectorquantization asinanystochasticgradientdescen t convergence oftheprototypevectors hastobeguaranteedbyempl enden toradaptivelearningrate η t whichslowlyapproacheszerointhecourseoftraining seethediscussi s nsalgorithm prescription itconsiders allavailabledataineachupdateofthesystem andaltersallprototypesatatime lloyd salgorithm p initialize n repeatthefollowingsteps assigneverydatapointxµtothenearestprototypewk µ wk µ d wj xµ b comput eupdatedprototypes center wjasthemeanof alldatapointswhichwereassignedtowjin k µ k µ gandunsupervised learning practicalissuesandextensionsofvq quantizationerrorhvqcanbeinterpretedasaqualitycriterionwhen thisisonlymeaningful hvqwilldecrease withincrea placingoneprototypeoneachindividualdata akeydiﬃcul nsalgorithmisthatthe objectivefunct otherproblems thiscanleadtoastrongdependence ofthetrainingoutcome placingaprototype inanempt yregionoffeaturespacecanpreventitfromeverbeingidentiﬁed asthewinner foranyofthedatapoints thusleavingitunchangedinawta trainingprocess thetermdeadunithasbeencoinedforsuchaprototype forcompetitivelearning havebeensuggestedwhich helptoovercomethisproblembyupdatingnotonlythewinner instead totypesarerankedaccordingtotheirproximitytothepresentedfeaturevector gener ally themagnitudeoftheupdateofaprototypeisadecreasingfunct ion calledneuralgasalgorithm whichempl oysrelatively largenumbersofprototypesrepresen tingthedensi tyofdatainfeaturespace theideahasalsobeenextended inthecontextofsupervisedlearning som dient onal latentspace thewinningprototype asdeﬁned inconventionalvq butneighborsofthewinner inthegridarealso gy frequently vectorquantizationisconfused orevenidentiﬁedwithclusteringas represen tpronounced thequantization anduseful ifthere tivesituations wher eprototypesrepresen tsimple onaldatawith displaysasingle elongatedclusterrepresen tedby theminimizationofhvqdoesnotcorrespondtotheidentiﬁcationofasetof b showsanidealizedcaseofapproximatelyspher icalclusters eachofthetwoclusterscanberepresen c ingandvecto rquantization b c d vectorquantization represen onaldata pointsbyprototypes schema tic assmall red dot prototypepositionscorrespondingtominimalhvqare nelsarereferredtointhetextofsec clustersappeartobeelongatedalongoneoftheaxesinfeaturespace totypesinthespacebetweentheapparentclusters wher ehardlyanyexamples areobserved panel c illustratesthefactthattheoutcomeofvqcanbe highlysensi tivetocoordinatetransformations d separatedclusteris fewdatapointscontributelargedistances theirtotalcontributiondonothaveasigniﬁcantinﬂuence onthepositionofthe prototypeswhen minimizinghvq thenumberofprototypesorclusters asdiscussed insec unsup ervisedlearningincludingclusteringandvector quantizationfrequentlylacksimpleperformancemeasures canbeformulatedmathema ticallyintermsofhvq butthecostfunct ionis plexity gandunsupervised learning illustrationofthe elbowmethod thequantization error persample asobtained fromtheirisﬂowerdatasetwith asafunct meansalgorithm methodsofclusteringandtherelatedcriteria inabsence ofadditionalinformationlikedomainknowledgeprovidinga ground truth itisimpossibletoinfer tersorprototypesfromthedatasetpalone inthissense clusteringand problem ultimatelytheirevaluationdepends ontheuser s preference andsubjectivequalitycriteria method nsalgorithm displaysthequantizationerrorhvqasobtainedintheirisﬂowerdataset quantizationismuchhigher whilefork signiﬁcantreduct ion elbowshapethatsuggests ofcourse theinsightdoesnotcomeasasurpr readyconsidered samplesfromthreediﬀerentclassescorrespondingtoamoreorlesspronounced clusterstructure eralpracticalsituations theshapeofthecorrespondingcurvesandtheelbowis frequentlymuchlessconclusive severalrelatedandalternativemethodshavebeensuggested someofwhich arebasedonmorerigorousstatisticalargumen nsorothermethods ontheactual qualitycriterionandthevisual representation theelbowcould estima tion aratherfunda mentalapproachtoobtaininginsightintothepropertiesofa ξµ p tythatcouldhave providecomprehensi veoverviewsofrelevantmethodsandtheoretical background severalbasicconcept scanbeappliedindensi ricmethods precisely aparticularfunct ionalformofthedensi tyisassumed optimizationofitsparameterscanbeformulatedasanunsup ervisedlearning processbasedontheobservedfeaturevectorsinp aspeciﬁcfunct ionalform apriori butaimtoinferadescr iptivedensi tydirectlyfromthedata nottobeconfused withkernelfunct ionsinthecontextofsvmandrelated method herewefocusontheimportantfamilyofmixturemodels somebasicideas behindparametricdensi tyestimationandthenpresenttheparticularpopular exampleofgaussianmixturemodels gmm parametric densityestimation intheparametricapproachwemakeexplicitassumpt ionsaboutaprobability densi enden tly reassume appropriateparametricformand likelihoodoftheobserveddataas l θ andℓ θ l θ ty p onkofθaswellasthetypeofparametersdependonthe actualstructureoftheconsidered model theoptimizationofℓ θ similartothediscussi ingapriordensi typo θ andderivetheresultingmaximumaposteriori map bayesianestimationtechniquescanbeappliedtoobtaina probabilisticdescr iptionofthemodelparameters thesuggestiv ename ametr icmetho dsmayverywellcompr iseparameter s gandunsupervised learning proachandconsideritsapplicationtoaspeciﬁctypeofmodeldensi tiesinthe nextsection gaussianmixturemodels asinanymachinelearningproblem modelselectionisakeydiﬃcul tyalso inthecontextofdensi assumed modeldensi tyshouldbe appropriatetorepresen verypowerful itcanbediﬃcul ttodeﬁne suitablemodelswhichallowforan analyticaloreﬃcientnumer concept theconsiderationofadaptivemodelswhicharedeﬁned asmixturesofspeciﬁc basisfunct tiesp constituteaparticularpopularand p σm wm p σm wm sume thatthecontributingdensi tiesareisotropicwithcovariancematrices onalgaussiandensi tiesareof coursepossible themodelparametersθin m quantifythecontributionoftheindividualgaussiansandhavetosatisfythe condition givenaparticularrealizationofthemodelweassignafeaturevectorξto oneofthecontributinggaussianswithprobability σm wm σk wk k gously wedeﬁne thequantitiesqµ mby forthespeciﬁcmodeldensi ty read ℓ θ k estima tion itisstraightforwardtoworkoutthenecessa ryconditionsforamaximumof canbewritteninthesuggestiveform qµ ξµ qµ m withtheassignmen tprobabilitiesqµ mdeﬁned ineq derivationof explicitly intuitivenaturaliterationmethod todeﬁne algorithm gaussianmixturemodel isotropicgaussiancontributions σm m e wm m t qµ m t t ξµ m qµ m t t pm m t notethatherewm butthetermsqµ m t areevaluatedbyinsertingthepreviouswm t intoeq tailisnotessen tialfortheiterationtowork itisveryinterestingtonotethat em durefortheoptimizationofthe likelihood methodologicalframeworkhasbeenformalizedbydempst generalproblems involvingincompletedata mationproblemwecaninterprettheassignmen tprobabilitiesqµmasunknown pproachcanbe foundintextbookslike algorithmintermsofa onaldatapoints leftpanel gandunsupervised learning illustrationofdensi tyestimationbyadaptationofagaussian onaldatapointsdrawnfroma multimodaldensi initialdensi tyrepresen tedbyamixtureof themodeldensi epsaccordingto amixtureofsixgaussiansisadaptedtothedatafollowingtheiteration thedensi tyisapproximatedvery tingonly thisdoesnotappeartoconstituteaproblem bycoincidinggaussianswith equalmeansandvariances orbyeliminatingredunda ntcontributionsbyhaving actualrealizationoftheestimateddensi tyin termsofthemodelparameterswilldependontheinitializationinthisexample itisveryinstructivetoconsiderthealgorithm inanextremely thatthevariances nentsofthegmm thesum isdominatedbythelargestsumma nd thetermwiththesmallest whichisexponentiallylargerthanall otherones consequentlyweobtain lim inthislimit datapointsareassignedinacrispwaytotheclosest wmaccordingtoeuclideandistance inthegmm algorithm updatesofthevariances becomeobsoleteandthepmaresimplycomput edas thefractionofcrispassignmen tstowm t whilethenewwm areobtained asthemeansofthedatapointscurrentlyassignedtowm t gvaluesandimpu tation tech niques intheconsidered limit thegmm algorithmbecomes identicalwiththe nsprocedur epresentedin fortheobservationmadeinsec sticlearningprocedur e canbeinterpretedasspecialcasesorlimitsofmoregeneralstatisticalmodelling scheme manyrealworlddatasetsarecompro misedbymissingvalues offeaturevectorsthathavenotbeenobserved registeredorcommunicated duetoavarietyofmoreorlesscomplex reason foracomprehensi vediscussi quitefrequently theuseofacertainsprea dsheet basedsoftwaretoolleadsto unwantedmissingness orotherartefactsandmakesitdiﬃcul arenotexplicitlydiscussed butalwaysshouldbetakenintoconsideration asapotentialsourceoferror intheliterature thefollowingrathercoarsecategorizationofmissingness canbefound examplesaretaken missingcompletelyatrandom missingness isconsidered tobecompletelyatrandomiftheabsence presence missingness anaccidentallydamaged bloodsampleinamedicalstudywouldresultinmissingtheobservation ofaparticularfeature missingatrandom inthiscase thefactthatafeatureismissingmightbepredictablefrom otherinformation similartotheonepresentedin personmaymissaniqtestbecausetheyareillonthedaythattheiq theperson butdoesnotdependonthepotentialoutcomeofthetest thefeaturevalueitself missingnotatrandom ifmissingness isspeciﬁcallyrelatedtothefeaturethatismissing somewha tvaguetermnotatrandomisused forinstance apersonmight haveavoidedadrugtestresultinginamissingobservation becausethey afeaturethatcannotberegisteredwhenev eritexceeds anallowedrange ofvalues tprocess thedistinctionofthese typesofmissingness itcanbediﬃcul t ifnotimpossible toinfer theunder lyingreasonfor http gandunsupervised learning missingness randmartypes aresometimesreferredtoasignorable arecertainlytheleastdiﬃcul ttohandle moresystematicformsofmissingness asinmnar wouldrequiresophisticated modellingtechniquestotakethem methodsdiscussed inthefollowingaremostappropriatefordataaﬀectedby mca rormartypemissingness approacheswithoutexplicitimputation dependingonthefrequency andnatureofmissingfeatures simplestrategies canbeappliedwhichignoreoreliminatethemissingness beforehandorasan integralpartofthetrainingprocess themoststraightforwardideatohandlemissingnessistoincludeonlycomplete featurevectorsintheanalysisandomitallothers thisisappealingsinceit disregardinga subset ofsamplesmightintroduce bias inanycase simpledeletionisonly wouldhavetoreject allincompletedatainvalidation similar alsoproblematic strategyistoomitfeaturesentirelyiftheyappearto bemissinginasigniﬁcantfractionoftheavailablesamples gness insomemachinelearningframeworksitispossibletorestricttheirapplication comput ationandrankingofpairwisedistances inasubset concept couldbeapplied forinstance ilarly inlvqandotherprototypebasedsystems thedistances ofanincompletetestortrainingsamplefromallprototypescanbecomput ed andcompared excludingthemissingcomponents fortheidentiﬁcationofthe closestprototype canhandlemissingness andnoise withoutexplicitimput probabilisticrandomforest imputationbasedonavailabledata themostwidelyused approachtohandlemissingvaluesisimput ation replacemen tofmissingfeaturesbymoreorlesssophisticatedestimatesbased te astraightforwardbutnaiveideaistoreplaceamissingvaluein say gvaluesandimpu tation tech niques ξjbythecorrespondingmeanormedi aninthedatasetascomput edfromall instance howeverintroduce orenhanceabias inimbalanced datasets asaseemi nglymoresophisticatedchoice semeanormedi anofξj couldbeused forimput wouldbeunavailableforunlabeledfeaturevectorsinthetestorworkingphase nn approachpartlysolvestheabove mentionedproblemandconstitutesapopulartoolforimput incompletefeaturevectorξwithmissingvalueξj onecandeterminethenearest neighbor s thevalueofξjinthenearestneighboror anappropriateestimatebasedontheknearestneighborscanbeimput ed ifafeatureξjcanbeexpectedtobecorrelatedwithotherfeaturesξkwith imput ationcanbebasedonaregressionscheme fromtheavailable datawecaninfer thisdependency andobtainapredictionforthemissingvalue termcolddeckimput ratedatasetisused forimput ingmissingvaluesinthetrainingdataorwhen inhotdecksettings available training datasetitselfisused fordeterminingtheimput edvalues methodsdiscussed canbeempl oyedtoestimatethedensi tyofdata offeatureξj theresultinggmm canbeused generaterandomvaluesfortheimput ationofmissingvaluesaccordingly frequently morethanoneimput ationvalueisgeneratedpermissingfeature gener ativeapproachesorrandomizedversionsofregressionbasedimput ation canbeused toprovidemultipleversionsoftheimput training andvalidationcanbeperformed onanumberofversionsinordertoobtain reliableperformanceestimates asanexample mouse hasrecentlybecomepopular e aﬁrststepallmissingvaluesarereplacedbysimplemeanormedi tation withtheexceptionofoneselectedfeaturedimensi say regressionbasedimput ationisused toreplacethemissingvaluesofξkinthe uentlythemissingvaluesofadiﬀerentfeatureξj gandunsupervised learning imput edbyregressionandtheentireprocedur tures andnaiveestimates havebeenreplacedbyregressionbasedimput ation theimput ationofallmissingvaluescanbedonewithdiﬀerentselectionsofthe initialfeatureξkandbyvaryingtheorderoffeatures thusobtainingseveral versionsoftheimput eddataset augmentation herewediscuss theproblemsthatarisewhen balanced inthesense thattheprevalences oftheindividualclasses arevery tiesoccur multimodaldistributionsofthetargetvariable buthere reference consult forinstance validatingagiven classiﬁer seethediscussi itisoftennecessa rytotakethe tation ofcertainclassesinthetrainingdatacanleadtoverypoorperformancewith trainingsetdoesnot necessa rilyreﬂect theprevalences wecanexpectintheworkingphase weightedcostfunctions assume pjofthoserepresen virtuallyallobjective funct ionswehaveconsidered fortrainingareofthegeneralform e w theinﬂuence ofthediﬀerentclassesbyconsideringtheweightedcostfunct ion ebal w wher ethepartialsums pjcontainonlyexamplesfromclass c toptimizationofeballeadsto updatesthatareequivalenttodescen tintheoriginale eq undersampling anotherstraightforwardwaytocompensateforclassimbalances formtheactualtrainingonsetswithbalanced ing augmen tation beachievedbyrandomlyselectingthesamenumberofexamplesfromeach thetrainingsetsizewillbelimitedtoatmostmin p pc mplingcanbelimitedtotheactualtrainingset whilevalidationandtestsetscanremainunbalanced buttheevaluationcriteria shouldberobust againstimbalance likethebacorothermeasuresdiscussed therandomselectioncouldbedoneonce disregardingtheremainingdata thisstrategywouldnotmakeuseofallavailableinformationand bearstheriskofselectingatypicalcases instead therandomunder sampling shouldbeperformed repeatedly whichenablesthecomput ationofaveraged performancemeasures oversampling onecanalsoaimatincrea singtheeﬀect iveinﬂuence oftheunder represen ted ideasaredescr ibedinthefollowing randomoversampling inthissimpleapproach domlyselectedexamplesintheunder represen tedclasses itcanberealizedby randomselectionofexampleswithreplacemen forlossfunct ionsofthegeneralform theeﬀect ofincludingcopied examplesinthetrainingdataisverysimilartoweightingtheclassesasineq inthecorrectedcostfunct ioneveryexamplefromagivenclass hasthesameweight inpractice randomoversamplingintheunder represen dersa mplingoftheoverrepresen tedonesareoftencombined generativeoversampling insteadofusingexactcopiesofavailableexamples onecanaimatgenerating syntheticdatawhichreﬂect thestatisticalpropertiesoftheminorityclasses simplestideawouldbetoaugmentthetrainingsetbynoisycopiesof formsasuitabledensi tyestimationanduses theresultingmodelforgenerating additionalsamples synth eticminorityoversamplingtechnique smo te cbh lowsasimilar yetsimplerconcept basicscheme issumma rizedinthe gandunsupervised learning smote syntheticminorit yoversampling technique euclideandistance ing thetwoselectedfeaturevectors assignittothesameclass severalmodiﬁcationsandextensionscanbeconsidered forexample boridentiﬁcationandconstructionofthesyntheticdatapointcouldbebasedon diﬀerentdistancemeasures foranexampleofapplyinggeodesic smote practicalissues suitabilityoftheapproachesdiscussed insec ontheavailabilityofexampledatafromallclassesandonthedetailsofthe problemathand boththeweightingofclassesinthecostfunct ionandunder samplingare samplingisonlyfeasiblefordatasetswithareasonablylargenumberof ifthatisnotthecase applicationofmachinelearningisquestionableanyway gener ativeoversamplingcannotreallycreatefunda tion sincetheaugmentedfeaturevectorsmoreorlessfaithfullyreﬂect theoversamplingand subseq uenttrainingbearstheriskofoverﬁttingtotheavailableminorityclass generativeoversamplingintroduces problem ofmodeland parameterselection teorthe choiceofamodeldensi tytobeginwith imbalanced tocompensatetheimbalanceshouldbe appliedwithcareandtheyshouldalwaysbeevaluatedinapropervalidation process dataaugmen tation inpracticalapplications evenbalanced tional onecanempl oyallmethodsdiscussed foroversamplingintheprevioussection concept ing augmen tation isdiscussed brieﬂyin withadditionalreferences imagedataaugmentationfordeeplearningisprovidedin obviously afewcasesinwhichaugmentation appearsjustiﬁablecanbeidentiﬁed inspeciﬁcapplications generativemodelsoftheexpecteddatainthe workingphasemaybeavailable togeneratesurrogatedatafortrainingand thatthecharacteristicsofanovelinstrumen t scopeinastronomy inaddition keypropertiesof theobjectsofinterestareknown artiﬁcialdatacanbegeneratedbymeans ofsimulationsoftheobservationprocessandsubseq uentlyused trainaclassiﬁerorregressionsystem rmbj forjustonerecent systemstrainedonsurrogatedatashouldeventually bevalidatedandtestedinarealworldsetting robustness noisecanbeincrea plemen thisstrategycanbeviewedasaregularizationtechnique obviouslythenoisemustnotbetoostrongandshouldreﬂect theexpected levelinrealdata analogousstrategiescanbeappliedinordertoachieverobustness originaldataisoftensubjected tosystematicvariations suchasrotation scaling orskewingofobjects aproperlydesignedtrainingsetwillresultinaclassiﬁerorregression systemthatdisplaysthedesiredinvariances itisessen tialthattheappliedvariationsaresuitableforthetaskathand willnotbeaﬀectedbyarbitrary rotationsintheimageplane asallorientationsshouldoccur inthedata handwr ittencharacterrecognitionisinsensi tive torotationsofthesymbolsonlyinaverysmallrangeofangles inmanycases datasetaugmentationappearstobeacheapbuteﬃcientway toincrea tivesshouldalwaysbeconsidered inparticular invariance couldbeimposed throughpreprocessing gantly theycanbeachievedbyappropriatenetworkdesignandmodiﬁedloss funct ionswhichrealizeinvariantfunct ion wbj foranexampleand gandunsupervised learningconcludingquote everybodyrightnow theylookatthecurrenttechnology andtheythink ok andtheydon trealizehowarbitraryit andthere snoreasonwhyweshouldn thingelse optimization ther eisnothingobjectiveaboutobjectivefunct ion elland herewesumma rizesomeessen tialmathema ticalconcept valuedfunct onalargumen particular weconsiderlocalextrema alsodiscuss basicaspectsofconstrainedoptimization notethatwerefrainfromprovidingtheprecise yetusuallymild rem forinstance weassume implicitlythatallconsidered funct ionsare continuousanddiﬀerentiable thatsecondderivativesaresymmet ric morerigorouspresentationswereferthereadertothemathema ticalliterature sionaltaylorexpansion ionoftheform f x ifthevalueofthefunct ionf xo anditsderivativesareknowninsomepointxo onaltaylorexpansiontoobtaintheapproximation f xo xo xo weassume issmall ﬁrsttermmerel ycorrespondstothesimpleestimate f x xo closetothereference secondtermtakesintoaccount ation realworldillustrationofextrema ons elevationz x y maximum minimum saddlepointsorextended http theﬁrst partial derivativesofthefunct ionwithrespecttothecoordinatesx withtheformalvector weobtainthegradientoffinxo xo xo thethirdtermin involvesthe onalhessematrixorhessian h xo ofsecondderivativeswithelemen t hij xo xo xo xo hj mation implicitly thataquadraticapproximationis suitablefortheextrema andsaddlepoints localextrema andsaddlepoints weconsiderunconstrainedproblemsoftheform minimize x x weassume thatxcanbechosenanywher wedo nothavetoconsiderminimaattheborderofallowedregions mum itonlyimpliesthatthefunct ionlocallyincrea seswhenev ermovingaway fromtheminimum inthissense aglobalminimumisalsoalocalminimum necessaryandsuﬃcientconditions anobvious necessa ryconditionforthepresence ofalocalminimumoffinthe thisfollowsimmedi atelyfromthetaylorexpansion uptoﬁrstorder withη f x f assumi ngthat issatisﬁed f fromeq weobtainthesuﬃci entcondition inthepresence smallstepsaway increa singthefunct rdercorrections o canbeneglected te ditionthatallitseigenvaluesarepositive d constructanorthonormalsetofeigenvectors ui d ation notealsothataneigenvalueρirelatestothecurvature secondderivative ofthefunct γ γ γui γui bycomparisonwiththeconventional mensi onaltaylorexpansionwith vanishinglinearterm ifthehessianisindeﬁni te itdisplaysalocalmaximum whileinothers mensi onal localminimum semi requiremorecarefulconsiderations ﬂatregion alongui ng example unsolvablesystems oflinearequations forp nthesystemisoverdeterminedand ingeneral inconsistent thenotationischosentoresem blethemachinelearning settingsthatweareconsideringthroughout wher thesystemaregivenbyadatasetofthefamiliarform usingthematrixandvectornotationintroduced wedeﬁne noweq isconvenientlywrittenas χw thesolutionofthe minedandcannotbesatisﬁed exactly wecanresorttoapproximativesolutions anaturalandpopularchoiceistominimizethecorrespondingsum ofsquared error sse inthesense ofalinearregression esse w andsaddlepoints wher ethelasttermisindep enden asin byconsidering theﬁrstorder necessa ryconditions insec interms oftheleftpseudo inverse theconditionthat assumpt ionthat assumpt ionthattheequations p cannotbesatisﬁed multaneously iξµ notethatforthequadraticesseataylorexpansionuptosecondorderwould ofcourse suﬃci entsecondordercondition correspondstopositivedeﬁni te te χu inordertohavepositivedeﬁni forp n cannothavefullrankandis boundtohavezeroeigenvalues correspondingly thesystem hasmany onofconstrainedoptimization inthenextsection strictpositivedeﬁni teness existsandcorrespondsto alocalminimum whichisalsoaglobalminimumsinceunrestrictedquadratic costscannotdisplayother localminima insec thecaseofsingular withλ whichcorrespondstoasimpleformofregularization alternatively anddiscussed y notsuﬃcien inthedata setcanstillyieldsingular ation constrainedoptimization equalityconstraints frequently oneencountersoptimizationproblemsoftheform minimize x subjecttonequalityconstraints gi x n wher ionsgideﬁne additionalconditionsunder whichf x hastobeminimized theconstraints gi x n mentsx asaconsequence solutionsoftheproblem donotnecessa respondtolocalminimaofthe unrestricted objectivefunct ionfdiscussed onecantransform theproblemtoanunconstrainedoneandproceed asbefore formally constraintsgivenintheformofequationsasin canbedealt withsystematicallybyintroducinglagrangemultipliers n thelagrangefunct ionorlagrangian x λi x x satisfytheﬁrstordernecessa ryconditions wher eweusetheshorthand secondsetofconditionsmerel ycorrespondstotheoriginalconstraints gi x suﬃci entconditionsforthepresence lateinthepresence ofconstraints foradetaileddiscussi notethattheextended onalhessianofthelagrange funct ionlisindeﬁni te ingeneral condition canoftenbeexploitedinordertoeliminatesomeof thevariablesor infact themultipliersandtosimplifythestructureofthe optimizationproblemsigniﬁcantly insec theadalineproblem sadaptivelinearneuron ther e linearequality constraint p bymakinguseoftheﬁrstorderconditions whilethelagrangemultipliersplay correspondstoanunconstrainedmaximizationofthemodiﬁedcostfunct ion ation example edlinearequations werevisitthesetoflinearequationsconsidered ifitrepresen tsp thesystem foranygivensolutionwof wecanconstructa continuum ofsolutions constraint examp le minimal norm solutionoflinearequations minimize weobtainthelagrangefunct ion w theﬁrstordernecessa ryconditions whilethesecondconditionisobvious obtainthemodiﬁedcostfunct ion notethathere thatwehaveencountered weassume thatp nandthat reget immedi ately thismotivatesthedeﬁni ation remark auniﬁedtreatment inauniﬁedway lim lim leftpseudo inverse rightdeﬁned left correspondstoalimitoftheregularizationterm thatweintroduced heuri inequalityconstraints concept oflagrangemultipliershasbeenextended tothepresence equalityconstraintsinoptimizationproblemsoftheform minimize x subjecttonconstraints gi x n werefrainfromdiscussi ityconstraints detailswereferthereaderto ple wecouldintroduce pairsofinequalityconstraintsgi x x simultaneouslyinordertoincludeequalityconstraints eﬀect ively formally thecorrespondinglagrangefunct ion x λi n x x ryconditionsforasolutionof ucker kt theo remofoptimizationtheory theyread ion onlyinequalityconstraints stationarity gi n constraint n complemen tarity ation wher eweusetheshorthand theﬁrstcondition correspondstothestationarityofthelagrangian simplyrepresen tstheoriginal constraint timum individualmultipliersλi withgi x whichfollowsfromthecomplemen taritycondition thecontrary ifgi x theconstraintdoesnothave tobeenforcedexplicitlyandistermed inactive moredetailedinterpretationsanddiscussi onsofthektconditionscanbe foundintheliterature seeforinstance example optimalstabilityintheperceptron asanimportantapplicationofthekttheoremweexploitthenecessa tions intheproblemofmaximumstabilityfortheperceptron minimize p thektconditionsarebasedonthelagrangian w whichisthesameasfortheadalineproblemwithequalityconstraintsasgiven ineq t andobtainfrom thefollowingsetofnecessa ryconditions ion optimalstability t embeddingstrengthsλµ linearseparability ativemultipliers complemen tarity ation asoutlinedinsec theﬁrstconditionshowsthatthelagrangemultipliers weightscanbeinterpretedasto resultfromiterativehebbianlearningandcan infact beeliminatedfromthe optimizationproblem thewolfedualforconvexproblems concept ofdualityplaysanimportantroleintheanalysisandsolutionof optimizationproblems theideaistypicallytoderiveanalternativeformulation ofagivenproblem whichistheneasiertohandlenumer forquitegeneralproblems whichisdiscussed insomedetailin forinstance lemswhichareoftheform withtheadditionalrequiremen tsthat x n isconvex b theobjectivefunct ionf x isaconvexfunct iononik inparticular condition issatisﬁed ifallfunct ionsgi x areconvexoreven thateverylocalsolutionoftheproblemisalsoaglobalsolution rathermildassumpt λn solvesthe followingproblem wolfe dual ofaconvexproblemoftheform max imize x x withthelagrangianlasdeﬁned ineq thesolutionsatisﬁes f notethatwhileintheoriginalproblem fisminimizedwithrespectto x themaximizationrefers toλandxunder foramoredetaileddiscussi onofthissubtletyandthewolfedualofmore generalproblems example quadraticoptimiz ationunderlinearconstraints frequently x toeliminatetheoriginal variablesx resultinginasimpliﬁedoptimizationproblem asanexample consideraquadraticproblemwithlinearinequalityconstraints whichinvolves asymmet thefunctions fandgishould becontinuously diﬀerentiable tional regular ityassumption isdiscussed section optimiz ation examp le minimize ﬁnes fortheapplicationofdualityarepresentedin correspondingwolfedual read examp le wolfedualofthequadraticproblem max imize wecanstillsimplifytheproblem andobtain examp le simpliﬁcationofthewolfedual max imize notethatthisspeciﬁcexamplehastheexactsamemathema ticalstructureas theproblemofoptimalstabilitydiscussed fortheproblemofoptimal stability gradientbasedoptimization thegradientisthebasisoratleastanimportantcomponentofmanypractical optimizationtechniques gradientdescentpersiststobeoneofthemostpopular andmostsuccessful methodforthetrainingofneuralnetworksandmoregeneral machinelearningsetups wediscuss thereasonsforthisperhapssomewha t surpr gradientanddirectionalderivative gradientasdeﬁned ineq ation taylorexpansion thecorrespondingdirectionalderivativeinxocanbe writtenas lim x xo ei xo iondecreases whileawith xo inagivenpointxothemagnitudeofthedirectionalderivativequantiﬁes howrapidlythefunct ionincrea xo weseeimmedi atelythatweobtainthedirectionsof steepestascentfor xo steepestdescen tfor xo stationarityfor xo gradientisalwaysperpendiculartotheleveldirectionsalongwhichfis locallyconstant gradientdescent thepropertiesofthegradientcanbeused forthenumer icalminimizationofa funct ionf x weconsideraniteration oftheform x t x t ifthestepsizeη entlysmall thetaylorexpansionoffinx t dominatedbythelineartermandwehave f x x t x t f x t ineverystepofthesequence x t consequentlyfcanonlydecreaseandthegradientdescen t proachalocalminimum analogously x t inordertoapproachalocalmaximum itisimportantthatthestepsizeηissmallenoughinordertoguarantee validityofeq andensur econvergence thisstatemen tmorepreciseinthefollowing theroleofthestepsize wecaninvestigatethebehaviorofgradientdescen tingreaterdetailbyassumi ng thatx t f x t x t x t x t x t optimiz ation t theupdate atsteptofthedescen tsatisﬁes approximately andapproximatedthegradientaccordingto wecanexpandtheinitial deviationas ately tui thecorrespondingsquaredmagnitudereads t wher ewehaveused lim sincealleigenvaluesarepositiveandtherefo asymptoticbehaviorofδtisdominatedbythelargesteigenvalueofthehessian ρn theconditionforconvergence oftheiterationto thelocalminimumisthat ρmax η ηmax convergence isguaranteed oncetheiterationissuﬃci entlyclosetotheminimum wenotethat thefactor forη iterationwillapproachtheminimumsmoothly η ηmax theorientationofδtﬂipsineverygradientstepandx t displaysanoscillatory finally forη ηmax threediﬀerentregimesareillustratedand summa itisimportanttorealizethattheaboveresultisvalidonlyinthevicinity ofagiven localoptimum itdoesnotenableustomakestatemen tsconcerning ation factthatconvergence canbeachievedwithaconstant ovalue thepracticaluseful ness ofthis insightislimited thepropertiesofthehessiancanbeverydiﬀerentforevery localminimumandtheyareobviouslynotavailableinadvance toalarge extent theinitializationx ofagradientprocedur ewilldeterminewhichof potentiallymany localminimawillbeapproached closetoaspeciﬁcminimum thelocalhessianh x t canbeseen asan toselectasuitablestepsize moresophisticated useh x t oranapproximationthereo fexplicitlyintheupdate seeforinstance moreinvolvedtechniqueshere andrestrictthediscussi arefrequentlyusedinthecontextofmachinelearning thegradientundercoordinatetransforma tions afrequentlyignoredpropertyofthegradientisthatthedirectionofsteepest descen coordinatetransformations inthecontextoftheadalinealgorithmweseeinsec descen tforthecostfunct ionofeq inthespaceofembeddingstrengths analogouslytoindicatewhichsetofvariablesthegradientrefers interpretingtheobjectiveasafunct ionofzweobtainwiththechainrule assumi ngthataisinvertible transformationofthegradientisdiﬀerentfromthatof ordinary tor forabriefdiscussi directionofsteepestdescen tin thetransformed coordinatesdoesnotcoincidewiththenaiveprojectionofthe originalgradient ingeneral ther efore weshouldbeawarethatagradientdescen tprocedur efoundfor onecoordinatesystemdoesnotnecessa rilyfollowthesteepestdescen thisseems tocastsomedoubtonthedistinguished roleofgradient descen reassuringly thatdirectionsofdescen tneed notbe thesteepestinordertobeuseful innumer icalminimizationprocedur e asgradien tdescen function t foramoredetaileddiscussi point correspondingmethodofnaturalgradientdescen tcanbe inthecontextofmachinelearning foradiscussi onandfurther reference variantsofgradientdescent theprevioussectionpresentsanddiscusses plain classical gradientdescen tfor theminimizationofcostfunct ionsinunrestrictedoptimizationproblems thebasicideaofgradientdescen tcanofcoursebemodiﬁedandadaptedto thespeciﬁcpropertiesandneeds discus anumberofvariantswhichareparticularlyrelevantinmachinelearning problem thisisthecase forinstance forthepopularstochasticgradient descentanditsmodiﬁcationsandextensions coordinatedescent quitegenerallyweconsidertheminimizationofacontinuouslydiﬀerentiable funct ionf x onalargumen xd t marksthedirectionofthesteepestdescen tina givenpointx t t inthefollowing fromapracticalpointofviewitisoftenadvantageoustoresorttosome directionofdescen ta t whichisnotnecessa rilythesteepestand therefo onlyhastosatisfy t acorrespondingiterativeprocedur eoftheformx t t beused toapproacha local minimumoff providedasuitablestepsizeηis chosen aparticularlysimpleexampleisthesequentialperformanceofstepsalong oneofthecoordinateaxeswitha t t caneasilysatisfyeq bysetting t t ei t t tei t t theiterationstepchangesxonlyinonecomponent t minedbythecorrespondingpartialderivativeoff uentialcoordinatedescen tarecursionoftheform t modd generatesacyclicsequence oftheformi t ation essen tially thestructureofthealgorithmisthesameasforaconventional gradientdescen tstepwhen ineachcomponentwemakeuseofthepreviouslyupdatedcoordinates whileinconventionalgradientdescen tthevaluesfromthepreviousloopare used resulting short isasimple sometimessurpri recentreviewofthisclassicalapproachcanbefoundin onecouldalsoconsidermodiﬁcationswithrandomizedselectionoftheupdated coordinate constrainedproblems andprojectedgradients ingeneral thesolutionofconstrainedproblemsbymeansofgradientdescen t techniquesrequiresconsiderablereﬁnemen oneimportantapproachinthiscontextisgradientprojection sume unconstrainedgradientdescen t t thestepisaccept edandx t wiseonedeterminesx asaprojectionintodasx straints thecomput ationoftheprojectedgradientcanbecostly inthecaseofsimplerconstraints tie e theiterationproceeds byunconstrainedgradientdescen torascentuntiloneor severalconstraintswouldbeviolatedand therefo ily activeinequalitiesaretreatedasequalityconstraintsinthefollowingsteps whichmovealongthecorrespondingplanes stochasticgradientdescent aspeciﬁcmodiﬁcationofgradientdescen tmaybeappliedwhen tioncanbewrittenasasum overanumberofindividualfunct ionsasin f x x x x wher ethesecondidentifyfollowsfromthelinearityofthegradientoperator inmachinelearning veryoftenthetrainingprocess isguidedbyacost funct ionwhichcanbewrittenasasum supervisedlearning ittypicallycorrespondstoanerrormeasure evaluatedwithrespecttotheindividualexamplesinthetrainingset coordinate ascentcanbeformulated formaximization analogously t acorrespondingcostfunct ioncanbewrittenintheform e w w wher eeµ w ξµ quantiﬁesthecontributionofanindividualexampledatatothetotalcosts quitegenerally thevectorwismeanttoconcatenatealldegreesoffreedomin thetrainedsystem lnetwork obviouslythisisoftheform withvariableswandindividualfunct ion eµ w discussi oninthefollowingrefers tothismachinelearningsetup andnotation butcarriesovertomoregeneralproblemsofthetype ofcourse thenumer icalminimizationofecouldbeaimed atbyapplying standardgradientdescen tasforanyother moregeneralobjectivefunct analogytoeq thebasicformofupdateswouldbe w t w t wenote thatcostsoftheform canbeinterpretedasanempi icalaverage µ whichcorrespondstodrawing e w w accordingly gradient w w w thissuggeststoapproximatethefullgradientofebycomput ingarestricted empi ricalmeanoverarandomsubset weconsideronerandomlyselected singleexampleinanindividualtrainingstep µ t p w t t w t wher batchgradientdescen asingleupdatestepis ingeneral comput tionallycheaperthanthefullgradientdescen t forwhichasum overall exampleshastobeperformed individualupdatestepsfollowarough stochasticapproximationofthetrue gradientofe thetermstochasticgradientdescen t sgd hasbeencoined fortheiterativeprocedur wecouldactuallydraw ment arandomexamplefromdindep enden datesareorganizedinepochs p andpresentingtheentiredinthisorderbeforemovingontothe ation onaverageovertherandomselectionprocess ansgdupdateisguidedby therefo wecanexpectthatthecost funct singletrainingstepsmayactuallyincrea setheobjectivefunct iontemporarily letusnowstudythebehaviorofsgd ngthatw t w t t p selectionprocess t inthissense thesgdtrainingbecomesstationaryinalocalminimumofeand itisimportanttorealizethat generally inanindividualupdatestep exactlysatisﬁed thiscanbeseen fromtheaveragemagnitudeoftheupdate step eµ forinstance aquadraticerrormeasure isempl oyed simultaneously represen tingaperfectsolutionwithe ingeneral theadaptivequantitiesw t willpersist toﬂuctuateinthevicinityofalocalminimum discussed robbinsandmonro seealso shownthatconvergence canbeachievedwithatimedependen tlearningrate schedulewithlim t theconditions lim t ii lim t intuitively theﬁrstcondition t hastodecreasefastenoughin ordertoachieveastationaryconﬁguration ii implies thatthedecreaseisslowenoughsothattheentiresearchspacecanbeexplored eﬃcientlywithoutstoppingtheiterationtooearly simpleschedul eswhichreduce t realizationofsuchadecreaseisoftheform t b ecalculationofagradient sophisticatedschemes plicitlytimedependen t canbebasedon estimated secondorderderivativesorontheobservedvariance ofthegradientoverseveralupdatesteps forinstance learningrateadaptation provide moredetailsandfurtherreferences forabriefdiscussi wher e alsoothermodiﬁcationsofsgdareintroduced examplecalculationofagradient inordertoexempl ifythecomput rwardnetwork weconsiderhere onalinput k hidden unitsandasingleoutput σ ξ w j weightvec torsw j assumed tobeadaptive whileforsimplicitylocalthresholdsarenotconsidered outputactivationh istakentobe potentially diﬀerentfrom thehidden unitactivationsg ξµ τµ p weconsiderthefamiliarquadratic deviation σ ξµ notethatonlyσdepends ontheweights thetargetvaluesτareﬁxedandgiven weomittheindex µandwriteσinshortforσ ξ firstwecomput depends onvkandweobtain w j shorthand δg w k ofcourse h x γx x γx weight w m n thesameshorthandδasdeﬁned aboveweobtain m m w m ξn ation thatcontainstheweightvectorw m w m m w m m arejustnumbersastheycorrespondto wasintroduced k m w m ξ wher m thekadditionalequations forthegradientofthefullcostfunct ionwehavetoperformsums foraparticularsingle example weget σ w j shorthand w k m w m ξµ inanetworkwithmorelayers thechainrulehastobeappliedseveraltimesand responseσisdeterminedbypropagatinganinput towardstheoutput dientiscomput edbypropagatingthedeviation backwardsthroughthe isthebasicconcept behindthefamousbackpropagationoferrorforeﬃcient ion rwardneuralnetworks hubblediagram dichotomy dichotomy ii iii ion ion rio alizationerrorasafunct n ortvectors linearlyseparabledata ortvectors softmargin machine iclayerednetwork listoffigures ion ons ionapproximation tnearaminimum tnearaminimum ion ion der vqsystemanddatavisualization ﬁttingandoverﬁtting tpheno meno n tativetrainingdata roc onmatrix onalmanifold onalscaling onalextrema andsaddlepoints adaline parallelupdates adaline sequentialupdates adatron sequentialupdates adatronwitherrors sequentialupdates batchgradientdescen t basicform competitivelearning vectorquantization crossvalidation gaussianmixturemodel gener alizedlearningvectorquantization glvq gener alizedmatrixlearningvectorquantization gml vq gener iciterativeperceptronupdates embeddingstrenghts gener iciterativeperceptronupdates weight grandmo kerneladatron sequentialupdates learningvectorquantization lloyd oja ssubspa optimalbraindamage obd optimalbrainsurgery ob sanger syntheticminorityoversamplingtechnique smo te adalineadaptivelinearelemen t adaptivelinearneuron adatronadaptiveperceptron algorithm auc areaunder thecurve aurocareaunder thereceiveroperatingcharacteristicscurve bacbalanced accuracy cm committeemachine cnnconvolutionalneuralnetwork codcoeﬃcientofdetermination elm extremelearningmachine em ximization algorithm fp fn falsepositives falsenegatives count fpr fnrfalsepositiverate falsenegativerate glvqgeneralizedlearningvectorquantization gmlvqgeneralizedmatrixrelevancelearningvectorquantization gmm gaussianmixturemodel ly ly ica indep enden tcomponentanalysis iqrinterquartilerange kl divergence kt ucker kttheorem ktpoint lm leastmeansquares lvqlearningvectorquantization mae meanabsoluteerror mar missingatrandom map maximumaposteriori probability mcar misingcompetelyatrndom mnar missingnotatrandom onalscaling mouse multipleimput ationbychainedequations msemeansquarederror npcnearestprototypeclassiﬁer classiﬁcation obdoptimalbraindamage obsoptimalbrainsurgeon odeordinarydiﬀerentialequation pmparitymachine principalcomponentanalysis pct perceptronconvergence theorem ll precprecision pspperceptronstorageproblem rbfradialbasisfunct ion recrecall relu rectiﬁedlinearunit rocreceiveroperatingcharacteristics roiregionofinterest rslvqrobust softlearningvectorquantization senssensi tivity spec speciﬁcity sgdstochasticgradientdescen t smotesyntheticminorityoversamplingtechnique snestochasticneighborhoodembedding ssesum ofsquarederrors svm supp ortvectormachine tp tntruepositives truenegatives count tpr tnrtruepositiverate truenegativerate umap uniformmanifoldapproximationandprojection onenk vqvectorquantization withrespectto y reference bibtexsourceﬁleisavailableuponrequest fromtheauthororat mostonlinesourcespointtothepublisher sﬁnalversions someofwhichmight epossible linkstoaccessi blepreprintversions areprovidedasanalternative theauthorcannotguaranteethecorrectness ofthelinksandisnotliablefor potentialcopyrightinfringemen tsonthecorrespondingwebsites http http foundationsofthepotentialfunctionmethodinpatternrecognition tomationandremo tecontrol http mathematicaland http ter ama http ama http teemachine computationaltostatisticalgapsin mationprocessingsystems asso http hopeorhype http roski editor nationalworkshoponmachinelearninginsystemsbiology bibl iography ofproceedingsofmachinelearningresearch http stuart tionbychainedequations whatisitandhowdoesitwork online http strainedoptimization nnand editor neurodynami c workshop arnoldsommer feldinstitut page http sifyingscotchwhisky dialbasisfunctionnetworkwithrelev leysen editor proceeding oftheeuropeansymp http apracticaloverview http umma gorini editor geometryatwork page ciationofamer http dimen http online http oftilin gmicroarraydata bylearningvectorquantizationandrelev editor dataengineeringandautomatedlearning ideal turenotesincomputerscience online http nn analysis intranslationalresearch integrationofexpertknowledgeand interpretablemodels editor peansymp osiumonartiﬁcialneuralnetworks page http http iography r variational relevancelearning http schmidt andthe topicgroupinitialdataanalysis ofthestra simp lerulesforinitialdataanalysis ploscomputationalbiology http http stateofneuralnetworkpruning editor proceedingsofmachinelearningandsystems http ityoflvqalgorithms http learningtheory colt press ny preprint http dioinverse tutorialreview http http tionarityofmatrixrelev internationaljoint conferenceonneuralnetworks http whatdoesit estimateandhowwelldoesitdoit journaloftheamericanstatistical association http editor lecturenotesincomputerscience berlin http bibl iography http editor alcob rithmsforcomputationalbiology lncs http line http forduniversitypress new york ny book http id http mationscienceandstatistics heidelberg germany http id line http editor summer school http rchdirectionsininterpretablemachine editor onartiﬁcialneuralnetworks http http ityreduction ofthesevent hacm edgediscoveryanddatamining kdd new york ny http iography wal kantan ziegler editor advancesinneuralinformationprocessingsystems ciates http online http pdf chine online http burg editor incomputerscience http http editor advancedlecturesonmachinelearning tiﬁcialintelligence lnai berlin http anewtypologydesignofperformancemetricsto measureerrorsinmachinelearningregressionalgorithms plinaryjournalofinformation knowledge andmanagement http nn edrankmatrixlearning discriminativedimen duction chotomies editor http dimens ionreductionand http bibl iography http editor ralinformationprocessingsystems http chawla hall http cc callyadaptingkernelsinsupportvectormachines vancesinneuralinformationprocessingsystems http cgb ginanalysis ofthelvqalgorithm editor advanc ingsystems cambridge http http arbib editor dataminingandknowledgediscoveryhandbook boston ma googlebooks http id abeuc circuittheory http demic press http lian http ingrulethatperformsindependentcomponentanalysis bmc http iography tinuoussquashingfunctionintheoutputareuniversalapproxima tor http trans electroniccomputers http ropagation theory tectures http line http versitypress cambridge uk http press cambridge http networklearningbyexponentiallinearunits elus n editor learningrepresentations http cyb http raydavies single onlineavailableathttps roth faisal learning cambridgeuniversitypress http machinelearning icml new york online http duda ley new http bibl iography enhancement byweightingofexamples iversität würzburg http incompletedataviatheem methodological http inhansonmoody editor advance inneuralinformationprocessingsystems http physicalreviewletters http icml http tionaryenvironment online http http usrebaaaqbaj ing cambridgeuniversitypress cambridge uk book http id improvemen t http preprint http http toswish comparing deeplearningactivationfunctionsacrossnlp cessing brussels http iography analysis http nalysis gorithm afastandsimplelearningprocedureforsupportvector icml gankaufmann sanfrancisco ca http tsintaxonomicproblems http chitecture editor mationprocessingsystems volume http http thekushner http amethodforconstructingand http ofpredictivelearningandfunction approxima ofnato asiseriesf compu tersystems science springer berlin http foramechanismofpatternrecognitionunaﬀectedbyshiftinposition http ahierarchicalneuralnetworkcapableof http eartheoryanditsapplications ieice http space editor theeuropeansymo siumonartiﬁcalneuralnetworks http bibl iography ieee http editor telligenceandstatistics ingresearch fortlauderdale fl http interpretablemodelscapableofhandlingsystematicmissin cambridge ma http osium onartiﬁcialneuralnetworks compu tationalintelligenceandmachine learning http anintroductiontovariableandfeature http http tion http pdf http gls al patternclassiﬁcationwithmissin gdata areview neuralcomput applic http http http iography gofdimen sionality mathematical http press http http gw analysisandrecognition dar http http http uppersaddleriver nj usa http neuralinformationprocessingsystems bridge ma http hebb theorganizationofbeha vior http theoryandalgorithms mitpress cambridge http trans http ofthecognitivesciencesociety http editor intelligenceandartiﬁcialintelligence aninterdisciplinarydebate heidelberg http sionality bibl iography editor caldatabasemanagement berlin http reading ma usa http algorithm http withemer gent http http http editor advancesinneuralinformation processingsystems http gareaundertheroccurve machinelearning http murray wolf markiperceptronoperator smanual buﬀalo ny http hinton http neuralgaswith generalsimila online http internationalconferenceonneural network http iography asimp legeneralisationoftheareaunderthe roccurveformultipleclassclassiﬁcationproblems machinelearning http htf sofstatistical new york ny usa http pnas http http deep online http ancelearningvector http editor network esann http hw wiesel receptiveﬁeldsofsingleneuronsinthe cat http gdeepintorectiﬁers ieee internationalconferenceoncomputervision iccv page http zhu siew extreme chine theoryandapplications neurocomputing http offeatureselection mationandcommu nicationtechnology ic mipro http muscle andsynap new online http bibl iography statisticalaccuracymeasuresforclassiﬁcationbasedonlearningvector editor osium http http kls sionaldatacanbezeroornegativeduetothe http ityinneuralnetworks http http jos informationprocessinginsingle new york ny chapter http san http editor mitpress cambridge http berlin online http kramer centelli lellearningalgorithms forneuralnetworks inproceedings cessingsystems nip cambridge ma usa online http learninginfeedforwardneuralnetworksforregressionproblems neural network ieee http http iography n bard http n http nder learningvectorquantization andvisualization wso http n solla age editor mationprocessingsystems volume http thesis http erenceon knowledgediscoveryanddatamining kdd new york ny http http http atedatawithmissin http voircomputingapproachesto http transactionson http llps lin feedforwardnetworkswithanonpolynomialactivationfunction canapproxima teanyfunction http bibl iography http ionalityreduction http http onthegeneralised line http new online http learning http http online http processin http andcrust aproofofthe pizzaconjectureandothertastyresu http ofthedevelopmentofmultidimen seriesd thestatistician http andtheirconnection containingpapersofamathematical http http umap imationandprojectionfordimen http iography micchelli approxima tionbysuperposition http http andtemathematikund http perceptrons anintroductionto compu tationalgeometry mitpress cambridge ma usa http mpcb weinberger editor page http dimen sionalityreduction acomparativereview http island winterschoolofastrophysics bigdataanalysisinastronomy instituodeastroﬁsicadecanarias http nn online http pdf eid sirat propagationlearningalgorithm ininternationaljointconference onneuralnetworks new online http http structurediscovery thesis http largecommit http bibl iography http oflearningvectorquantization preprint http neuralnetworksresea rchcentre helsin organizingmaps som andlearningvectorquantization lvq otaniemi http editor vancesinneuralinformationprocessingsystems http http madison wi ipress preprint http wolf datascience andwhymachinesmust http julien http nw ofgam posiumonartiﬁcialneuralnetworks computationalintelligenceand machinelearning line http nw classiﬁcation amachinelearninganalysis ofgam acataloguedata http http http iography http principalcomponents andsubspaces http z ityoftheoptimalperceptrontogeneralise http http ollb toshareornottoshare http anempiricalstudy http ervonvenkisdimen http layeredneuralnetworks relu http http mann editor bielefelduniversity http nn http basedweightsaliencies γobd editor advancesinneuralinformationprocessing system http bibl iography line http sofcausalinference cambridge ma http crosoftresea http summa ry regionsofdeepfeedforwardnetworkswithpiecewise linearactivations http tsonlearningbyback ence http online http inmontavon orr editor neuralnetworks tricksofthe trade uterscience springer http http machinelearningpioneersays ieee spectrum http n http modelselection andalgorithmselection http http http iography rhw art hinton http rks ofdropconnectinlearningvectorquantizationnetworksforevaluation http http cambridge http http new york ny usa http id berlin http theinternationaljointconferenceonneuralnetworks page http preprint http aprobabilisticmodelforinformation http falo ny http uk online http sionalityreductionbylocal http whyshoulditrustyou bibl iography new york ny http l http http cornelluniversityoperations resea http bridgeuniversitypress cambridge uk online http ieee transactionsoncomputers http sammo http preprint http nn ieee http areview online http furrer zürich http machinelearning http inneuralinformationprocessing system page online http thesis http iography anoverview neural http edestatesofamerica pnas http tee http criminativedimen ninthinternationaljointconferenceonartiﬁcialintelligence http hinton asimp lewaytopreventneuralnetworks http basedneuralnetworklayers http http http regularization andsearchingfor http asystematicanalysis ofperformance http http tionfunctionsisuniversalapproxima http cambridge http bibl iography http antization learningwithregularizersin multilayerneuralnetworks http supportvector machine regularization optimization andbeyo cambridge ma usa http ysisasakerneleigenvalueproblem neuralcomp http ry cambridge uk http onent analysis http cambridgepress wellesley http taﬀ http ofreservoircomputing theory application andimplemen editor proceedingsoftheeuropeansymp http http http editor vancesinneuralinformationprocessingsystems page http iography ofconvergenceingeneralized sson e editor tionalconferenceonartiﬁcialneuralnetworks icann http frameworkfornonlineardimen http http line http calsociety statisticalmethodology http resentations iclr http http onlinelearningwithensembles physicalreview http nn nn sionofdeep learningarchitectures ingvectorquantization wso press http http vkhb nn http ation nn tweida bibl iography application icml new york http line http w editor onartiﬁcialneuralnetworks esann http editor peansymp osiumonartiﬁcialneuralnetworks page http download nn nn martínguerrero editor ingvectorquantization clusteringanddatavisualization http vw s toolboxforgmlvq http http editor advancesinneuralinformationprocessingsystems ranasso ciates http webresource http ndregression newtoolsforpredictionandanalysisin http http wesco nconvent http iography adaline neuronusingchemical tor http http http lueproblem http osium onswitchingcircuittheoryandlogicaldesign page http phdthesis http http perceptron madaline http mae overtherootmeansquareerror rmse inassessin gaverage http learning http http watkin line http http http bibl iography er editor enceonmachinelearning ingresearch atlanta georgia online http algorithmsand http id youtubechannelpseu rchfromthe s s http line http http s partitionsofspacebyhyperplanes http biehl shallow deepa biased introduction neural network old school machine learning shallow deep collection lecture note offer accessible introduction neural network machine learning general clear beginning note able cover rapidly changing growing field entirety focus lie classical machine learning technique bias classification regression learning paradigm recent development instance deep learning addressed briefly touched argues having solid knowledge foundation field essential especially want explore world machine learning ambition go application software package data set shallow deep place emphasis fundamental concept theoretical background involves delving history neural network foundation recent development note aim demystify machine learning neural network losing appreciation impressive power michael biehl associate professor computer science bernoulli institute mathematics computer science artificial intelligence university groningen joined intelligent system group hold honorary professorship machine learning center system modelling quantitative biomedicine university birmingham uk research focus modelling theoretical understanding neural network machine learning general development efficient training algorithm interpretable transparent system topic particular variety interdisciplinary collaboration concern practical application machine learning biomedical domain astronomy area michael biehl